[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html",
    "href": "posts/distributed-training-from-scratch/index.html",
    "title": "Distributed Training From Scratch",
    "section": "",
    "text": "Assume that you are preparing for a marathon. If you plot your performance versus amount of training hours you put in, the relationship is non-linear, performance gains begin to diminish after a certain point (as we will hit the human capacity).\n\n\n\nThis concept holds true for LLMs as well. For example, in the LLaMA family (shown below), as the model size increases, the performance improves but the training time increases. The size of the circle represents the size of the model. And here we can see that as the model size increases, the performance improves but the training time increases.\n\n\n\nBut then it requires more training. The number of GPU hours required for training are really huge. If you see the y-axis, it is measured in millions of GPU hours. So how are we supposed to do it in one lifetime if we were to do it on one GPU? It is not feasible.\nSo, we need to focus on how we can efficiently exploit multiple GPUs using different forms of parallelism. And in this blog, we will look at how to do that and what are the different forms of parallelism available and how we can implement them from scratch using PyTorch and later on we will use Ray to scale the training."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#introduction",
    "href": "posts/distributed-training-from-scratch/index.html#introduction",
    "title": "Distributed Training From Scratch",
    "section": "",
    "text": "Assume that you are preparing for a marathon. If you plot your performance versus amount of training hours you put in, the relationship is non-linear, performance gains begin to diminish after a certain point (as we will hit the human capacity).\n\n\n\nThis concept holds true for LLMs as well. For example, in the LLaMA family (shown below), as the model size increases, the performance improves but the training time increases. The size of the circle represents the size of the model. And here we can see that as the model size increases, the performance improves but the training time increases.\n\n\n\nBut then it requires more training. The number of GPU hours required for training are really huge. If you see the y-axis, it is measured in millions of GPU hours. So how are we supposed to do it in one lifetime if we were to do it on one GPU? It is not feasible.\nSo, we need to focus on how we can efficiently exploit multiple GPUs using different forms of parallelism. And in this blog, we will look at how to do that and what are the different forms of parallelism available and how we can implement them from scratch using PyTorch and later on we will use Ray to scale the training."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#deep-learning-training-basics",
    "href": "posts/distributed-training-from-scratch/index.html#deep-learning-training-basics",
    "title": "Distributed Training From Scratch",
    "section": "Deep Learning Training Basics",
    "text": "Deep Learning Training Basics\nBefore diving into scaling, let’s quickly review the standard model training loop, such as a simple Multi-Layer Perceptron (MLP):\n\nPseudo Code for the Training Loop\n\n\n1  model = MLP().to(device)\n2  optimizer = Adam(model.parameters())\n3  criterion = CrossEntropyLoss()\n4  data_loader = DataLoader(dataset)\n5  \n6  for epoch in range(num_epochs):\n7      model.train()  \n8      for inputs, targets in data_loader:\n9          # 1. Move batch to GPU\n10         inputs, targets = inputs.to(device), targets.to(device)\n11         \n12         # 2. Clear gradients\n13         optimizer.zero_grad()\n14         \n15         # 3. Forward pass\n16         outputs = model(inputs)\n17         loss = criterion(outputs, targets)\n18         \n19         # 4. Backpropagation\n20         loss.backward()\n21         \n22         # 5. Optimization\n23         optimizer.step()\n\n\n\nIn a typical training loop, after defining the model, the optimizer, the loss function, and the data loader, the training loop trains the model by performing the following steps for each epoch:\n\nIterate over the data in mini-batches (line 6-8).\nMove each batch of data to the GPU (line 9-10).\nZero out any previous gradients (line 12-13).\nPerform a forward pass to calculate the model outputs and loss (line 15-17).\nCompute gradients through backpropagation (line 18-19).\nUpdate the model parameters using the optimizer (line 20-21).\n\n\n\n\nThis pattern is the core of most deep learning training routines."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#bottlenecks-in-single-gpu-training",
    "href": "posts/distributed-training-from-scratch/index.html#bottlenecks-in-single-gpu-training",
    "title": "Distributed Training From Scratch",
    "section": "Bottlenecks in Single-GPU Training",
    "text": "Bottlenecks in Single-GPU Training\nWhen training deep learning models on a single GPU, high-bandwidth memory (HBM) is utilized by four main types of data:\n\nModel Parameters (\\(\\Phi\\)):\nThe weights being learned during training.\nParameter Gradients (\\(\\nabla \\Phi\\)):\nThe gradients computed during backpropagation, required for parameter updates.\nOptimizer States (\\(\\Phi_{\\text{optim}}\\)):\nAuxiliary variables needed by the optimization algorithm, such as momentum and variance estimates (e.g., in Adam).\nActivations (\\(\\mathcal{M}_{\\text{act}}\\)):\nThe intermediate outputs from each neural network layer required to compute gradients during the backward pass.\n\nOf these, the first three (Parameters, Gradients, and Optimizer States) are considered static components. They collectively define the minimum “static” memory footprint determined by the model architecture itself.\nThe fourth component, Activations, is dynamic and its memory footprint depends on the input size (such as batch size and sequence length). Thus, activations often become the main bottleneck in large-scale training.\n\nStatic Memory\nAnd when we are training if you look at the training loop again, until step optimizer.step(), we need to keep everything in the memory. And after optimizer.step(), we can discard the activations and the gradients. And we can keep the model parameters and the optimizer states in the memory.\nIf \\(\\Psi\\) is the total number of parameters in the model, the total static memory required (\\(\\mathcal{M}_{static}\\)) using the Adam optimizer is a fixed amount: \\(16\\Psi\\) bytes.\n\n\n\n\n\n\n\n\n\nComponent\nPrecision\nSize (\\(\\Psi\\) Bytes)\nRationale\n\n\n\n\nModel Parameters\nBF32 (4 bytes)\n\\(4\\Psi\\)\nUsed for forward and backward passes\n\n\nParameter Gradients\nBF32 (4 bytes)\n\\(4\\Psi\\)\nUsed in backpropagation\n\n\nOptimizer States (Adam)\nFP32 (4+4 bytes)\n\\(8\\Psi\\)\nStores 1st and 2nd moment estimates (\\(4\\Psi\\) each)\n\n\nTotal Static Memory\n\n\\(16\\Psi\\)\nThe absolute floor for static storage\n\n\n\n\n\n\n\n\n\nNoteWhy Adam Optimizer Uses \\(4+4\\) Bytes per Parameter\n\n\n\nAdam maintains two additional FP32 (4-byte) tensors per parameter: the first moment (mean of gradients, \\(m\\)) and the second moment (uncentered variance, \\(v\\)). Thus, for each parameter, Adam stores \\(4\\) bytes for \\(m\\) and \\(4\\) bytes for \\(v\\), totaling \\(8\\Psi\\) bytes.\n\n\nAnd when it comes to training a model, its all about how smartly we can manage this memory footprint. In modern LLM training, mixed precision is employed, typically using BF16 (2 bytes) for fast computation while maintaining a full-precision FP32 (4 bytes) copy of weights and optimizer states for numerical stability.\n\n\n\n\n\n\nTipMixed Precision Training\n\n\n\nMixed Precision Training accelerates deep learning and reduces memory use by combining 16-bit (BF16/FP16) and 32-bit (FP32) floating-point operations.\n\nHow it works: Forward and backward passes use low-precision (e.g., BF16) for parameters and activations, while a full-precision FP32 master copy of weights and optimizer states is kept for numerical stability.\nWhy it matters: Enables training of larger models or larger batches within the same hardware footprint. Mixed precision is now standard in large-scale model training.\n\n\n\nSo, with mixed precision, the breakdown is as follows:\n\n\n\n\n\n\n\n\n\nComponent\nPrecision\nSize (\\(\\Psi\\) Bytes)\nRationale\n\n\n\n\nModel Parameters\nBF16 (2 bytes)\n\\(2\\Psi\\)\nUsed for forward and backward passes\n\n\nParameter Gradients\nBF16 (2 bytes)\n\\(2\\Psi\\)\nUsed in backpropagation\n\n\nMaster Weights\nFP32 (4 bytes)\n\\(4\\Psi\\)\nFull precision copy for the update step\n\n\nOptimizer States (Adam)\nFP32 (4+4 bytes)\n\\(8\\Psi\\)\nStores 1st and 2nd moment estimates (\\(4\\Psi\\) each)\n\n\nTotal Static Memory (with Mixed Precision)\n\n\\(16\\Psi\\)\nThe absolute floor for static storage\n\n\n\nYou might notice that the total static memory remains \\(16\\Psi\\) bytes. So what is the advantage of mixed precision training?\nThe key benefits of mixed precision are:\n\nIncreased Training Speed: As we are using lower-precision data types (like BF16) during forward and backward passes, computation is faster and less memory bandwidth is used.\nReduced Activation Memory: Since our model parameters and optimizer states are stored in FP32, the activations, which are stored in BF16 during training, require half the memory compared to FP32, so the dynamic (activation) memory footprint is significantly lower.\n\nWhile the absolute static memory is unchanged, mixed precision allows for faster training and greater memory efficiency, especially for storing activations, enabling larger models or batches to fit within the same hardware limits.\nAnd this calculation reveals a significant challenge: a 70 Billion parameter model requires approximately \\(70\\text{B} \\times 16 \\text{ bytes} \\approx 1120 \\text{ GB}\\) of static memory. With high-end GPUs typically offering only \\(80 \\text{ GB}\\) of memory, loading even the static state of the model becomes impossible, without considering the dynamic activations.\n\n\nDynamic Memory\nThis component is dependent on the input batch and is the primary cause of memory bottlenecks.\n\nActivations: The output of each layer. They must be stored until the backward pass to compute the gradients.\n\nFor a linear layer \\(y=Wx\\), the gradient for \\(W\\) is calculated as: \\[\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot x^T\\]\nThis requires saving the layer’s input, \\(x\\) (the activation from the previous layer).\n\nActivation Memory Equation: The total memory required for activations (\\(m_{act}\\)) in mixed precision can be estimated by the following equation: \\[\\mathcal{M}_{\\text{act}} = L \\cdot \\text{seq} \\cdot \\text{bs} \\cdot h \\cdot \\left(34 + \\frac{5 \\cdot n_{heads} \\cdot \\text{seq}}{\\text{h}}\\right)\\]\nWhere:\n\n\\(L\\): Number of layers\n\\(\\text{seq}\\): Sequence length\n\\(\\text{bs}\\): Batch size (number of samples)\n\\(h\\): Hidden dimension of the model\n\\(n_{heads}\\): Number of attention heads\n\n\n\n\n\nAs we can see, activation memory usage is not static for a given model; it scales:\n\nLinearly with the batch size (\\(\\text{bs}\\))\nQuadratically with the sequence length (\\(\\text{seq}\\))\n\nAnd this quadratic scaling with \\(\\text{seq}^2\\) (an effect stemming from the attention matrix) means the activation memory is the part that will blow up when you increase the batch size or train with longer sequences."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#batch-size-intuition",
    "href": "posts/distributed-training-from-scratch/index.html#batch-size-intuition",
    "title": "Distributed Training From Scratch",
    "section": "Batch Size Intuition",
    "text": "Batch Size Intuition\nSo, as we can see that the longer the sequence, the more activations we need and hence the more memory we need. So, even for one single sequence, the memory required is more than 50 GB. And this is a significant challenge for training large models.\n\n\n\n\n\n\nNoteGlobal Batch Size\n\n\n\nWhen we talk about batch size in LLM pre-training, we usually refer to it in terms of the number of tokens, not the number of sequences. That is, the token batch size is calculated as sequence length × number of sequences (micro-batch size)\n\n\nTypically, the global batch size used in pre-training is extremely large—usually in the millions of tokens. And in practice, we start the training with a smaller batch size and gradually increase it over the course of training.\n\nSmall Batch Size: Used at the beginning of training when the loss is high. It provides quick, noisy signals that help the model traverse the loss landscape rapidly toward the minima.\nLarge Batch Size: Used as training approaches the optima. It provides a more accurate gradient direction (a clearer signal), which reduces noise and ensures confident, stable convergence."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#memory-usage-in-transformer",
    "href": "posts/distributed-training-from-scratch/index.html#memory-usage-in-transformer",
    "title": "Distributed Training From Scratch",
    "section": "Memory usage in Transformer",
    "text": "Memory usage in Transformer\nTo get a sense of the memory usage in a Transformer, let’s take a look at the memory usage of Llama 3.1 {8B, 70B and 13B} models.\n\n\n\nFrom this graph, we can clearly see that forshort sequences (or small batch sizes), memory usage for activations is almost negligible, but from around 4K-16K tokens they start to take up a significant amount of memory (this is because of the quadratic scaling with the sequence length, which we discussed earlier), while usage for parameters, gradients, and optimizer states is roughly independent of the sequence length and batch size.\nHow can we solve this problem of activation explosion? Can we somehow avoide storing all those activations ?\n\nSolution 1: Activation Recomputation\nRecall why we need to store all those activations in the first place ? It is because we need to compute the gradients for the model parameters. So, if we can somehow avoid storing all those activations, we can save a lot of memory.\nOne effective approach is Gradient Checkpointing also known as Activation Recomputation. With this technique, we discard most of the activations during the forward pass to save memory and recompute them on the fly during the backward pass when gradients are needed.\nNormally, we’d store every hidden state between learnable operations (like feedforward layers, layer norm, etc.) to use them during the backward pass. With activation recomputation, we only store activations at specific checkpoints and recalculate everything else during backpropagation. This helps us manage memory while training large models. The process typically looks like this:\n\n\n\nBut as we know in life, there is no free lunch. Although we save memory by discarding most of the activations during the forward pass, we spend some extra compute to recompute these activations on the fly during the backward pass.\nThere are a few ways to do activation checkpointing, and each involves different memory and compute tradeoffs.\nThe most aggressive approach is called Full Activation Checkpointing, where you only store activations at the end of each layer (instead of storing every intermediate activation). This method is great for memory since you’re keeping so little, but it’s the most compute-heavy, often increasing computation time by 30–40% because you have to recompute almost everything during backpropagation.\nBut do we really need to treat every part of the model the same? By profiling, we find that the main memory culprit is the activations from the Multi-Headed Attention (MHA) layers, since they scale quadratically with sequence length.\nThis leads to a more balanced strategy: Selective Checkpointing. Here, we only skip storing activations for the heavy MHA layers and still store them for the lighter MLP layers. The payoff is impressive: up to 70% memory savings for only about 2.7% extra computation.\n\n\n\n\n\n\nNoteActivation Checkpointing on Llama 3.1 8B model\n\n\n\nAs you can see the graph bellow, on an 8-billion parameter model with a batch size of 1 and sequence length 4096, activation memory without any checkpointing can hit 97 GB, which is enough to break most GPUs. With selective activation checkpointing, that drops to 17 GB. And with full checkpointing, at the extreme, memory usage can go down to just 1 GB!\n\n\n\n\n\nNow that we’ve learned about recomputation, we can tame the activation memory usage we saw in the previous graphs!\nHowever, activations still have a linear dependence on the batch size, so as we move to larger batch sizes this might become an issue again. So, what can we do to increase the batch size ? And to takle this we have the next trick in our box - gradient accumulation, lets discuss that next.\n\n\nSolution 2: Gradient Accumulation\nGradient Accumulation is a technique that allows us to accumulate gradients over multiple micro-batches before performing a single global optimization step. This is particularly useful when we have a large batch size and we want to avoid running out of memory.\nThe general idea is to split the batch into smaller micro-batches (let’s say 3) and process them one by one. We compute the gradients for each micro-batch and accumulate them (we do not do optimizer.step() after each micro-batch). And after processing all the micro-batches, we perform a single global optimization step.\n\n\n\nLet’s take an example of a simple linear regression model:\nLet’s use a different analogy: predicting the score of a student on a test based on two factors—the number of hours studied (\\(x_1\\)) and the number of hours slept the night before (\\(x_2\\)). We assume a simple linear relationship between these inputs and the output score:\n\\[\n\\text{score}_{pred} = x_1 w_1 + x_2 w_2 + b\n\\]\nOur aim is to use stochastic gradient descent to determine the best values for \\(w_1\\), \\(w_2\\), and \\(b\\) such that the mean squared error (MSE) between the actual score (\\(\\text{score}_{target}\\)) and the predicted score (\\(\\text{score}_{pred}\\)) is minimized:\n\\[\n\\underset{w_1, w_2, b}{\\mathrm{argmin}} \\; (\\text{score}_{pred} - \\text{score}_{target})^2\n\\]\nWithout gradient accumulation, we would update the parameters after each batch of student’s data.\n\nWithout Gradient Accumulation\n\n\n1  def train_no_accumulate(params: ModelParameters, \n2                         num_epochs: int = 10, \n3                         learning_rate: float = 1e-3):\n4      for epoch in range(1, num_epochs + 1):\n5          for (x1, x2), y_target in training_data:\n6  \n7              # Calculate the output of the model\n8              z1 = x1 * params.w1\n9              z2 = x2 * params.w2\n10             y_pred = z1 + z2 + params.b\n11             loss = (y_pred - y_target) ** 2\n12 \n13             # Calculate the gradients of the loss w.r.t. the parameters\n14             loss.backward()\n15 \n16             # Update the parameters (at each iteration)\n17             with torch.no_grad():\n18                 # Equivalent to calling optimizer.step()\n19                 params.w1 -= learning_rate * params.w1.grad\n20                 params.w2 -= learning_rate * params.w2.grad\n21                 params.b  -= learning_rate * params.b.grad\n22 \n23                 # Reset the gradients to zero\n24                 # Equivalent to calling optimizer.zero_grad()\n25                 params.w1.grad.zero_()\n26                 params.w2.grad.zero_()\n27                 params.b.grad.zero_()\n\n\n\nWith gradient accumulation, instead of updating the parameters after each batch of data, we accumulate gradients across several micro-batches (micro_batch_size = 3) and then update all at once. This allows us to train with larger effective batch sizes even if memory is limited.\n\nWith Gradient Accumulation\n\n\n1  def train_accumulate(params: ModelParameters, \n2                       num_epochs: int = 10, \n3                       learning_rate: float = 1e-3, \n4                       micro_batch_size: int = 3):\n5  \n6      for epoch in range(1, num_epochs + 1):\n7          for index, ((x1, x2), y_target) in enumerate(training_data):\n8  \n9              # Calculate the output of the model\n10             z1 = x1 * params.w1\n11             z2 = x2 * params.w2\n12             y_pred = z1 + z2 + params.b\n13             loss = (y_pred - y_target) ** 2\n14 \n15             # Accumulate gradients\n16             loss.backward()\n17 \n18             # If we have processed 3 micro-batches OR reached the end of the dataset\n19             if (index + 1) % micro_batch_size == 0 or index == len(training_data) - 1:\n20                 with torch.no_grad():\n21                     # Equivalent to optimizer.step()\n22                     params.w1 -= learning_rate * params.w1.grad\n23                     params.w2 -= learning_rate * params.w2.grad\n24                     params.b  -= learning_rate * params.b.grad\n25 \n26                     # Reset the gradients = optimizer.zero_grad()\n27                     params.w1.grad.zero_()\n28                     params.w2.grad.zero_()\n29                     params.b.grad.zero_()\n\n\n\nGradient accumulation allows us to reduce activation memory, which grows linearly with batch size, by processing smaller micro-batches sequentially. This reduces stored activations and gradients since only one micro-batch’s worth of activations needs to be kept in memory at a time, which helps reduce the overall activation memory footprint.\nAgain there is no free lunch. As gradient accumulation requires multiple consecutive forward/backward passes per optimization step, it increases the compute overhead and slows down training. But it allows us to train with larger effective batch sizes even if memory is limited.\nSo far, we’ve seen how techniques like gradient checkpointing and gradient accumulation help deal with the memory blowup issue caused by activations: the dynamic part of memory usage. Both allow us to fit larger models or batches on a single GPU, but mostly by working sequentially and slowing down training. However, these don’t address the static memory required for parameters, gradients, and optimizer states, nor do they fully utilize available hardware (assume we have more than one GPU).\nTo tackle this, we can scale training across multiple GPUs using Data Parallelism. By splitting micro-batches and processing them simultaneously on several GPUs, we address both memory and compute bottlenecks, and that is what we will discuss in the next section."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#scaling-with-multiple-gpus-data-parallelism-dp",
    "href": "posts/distributed-training-from-scratch/index.html#scaling-with-multiple-gpus-data-parallelism-dp",
    "title": "Distributed Training From Scratch",
    "section": "Scaling with Multiple GPUs: Data Parallelism (DP)",
    "text": "Scaling with Multiple GPUs: Data Parallelism (DP)\nRecall that in Gradient Accumulation, we were processing micro-batches (MBS) sequentially. Since these micro-batches are independent of each other, we can process them parallelly on different GPUs. Something like this, if you see carefully now we are processing the micro-batches in parallel on different GPUs, w.r.t what we did in Gradient Accumulation where we were processing the micro-batches sequentially on a single GPU:\n\n\n\n\nThe Data Parallel Setup\nIn a Data Parallel setup, we distribute the data across multiple GPUs, while maintaining a full, redundant replica of the model parameters, gradients, and optimizer states on each GPU.\n\nReplication: We maintain a full, redundant replica of the model parameters (\\(\\Phi\\)), gradients (\\(\\nabla \\Phi\\)), and optimizer states (\\(\\Phi_{\\text{optim}}\\)) on each GPU.\n\n\n\n\n\nParallel Processing: Each GPU processes a unique micro-batch simultaneously. This involves same operations, different data.\n\n\n\n\n\nLocal Computation: Each GPU performs its forward pass and backward pass locally and independently, resulting in a local gradient (\\(\\nabla \\Phi_i\\)).\n\n\n\n\nIf you look carefully, we can perform the forward pass and the backward pass in parallel on different GPUs. But we cannot perform the optimizer step and update the parameters independently on different GPUs. If we do that, we will end up training N different models on N different GPUs which is not what we want.\nSo, after the backward pass, we need to somehow synchronize the gradients across the GPUs. And this is done by the All-Reduce primitive.\n\n\nGradient Synchronization: The All-Reduce Primitive\nBefore we dive into the All-Reduce operation, it’s important to note that NVIDIA provides a rich set of communication primitives as part of its distributed training ecosystem (such as NCCL). These primitives simplify and accelerate multi-GPU (and multi-node) communication, enabling efficient synchronization and sharding operations required for large-scale training.\n\n\n\n\n\n\nNoteCommunication Primitives in Distributed Training\n\n\n\nAll-Reduce is just one such primitive—used specifically for synchronizing gradients across GPUs at the end of each backward pass in standard Data Parallel training. However, there are several other primitives (like All-Gather, Reduce-Scatter, Broadcast, etc.) designed for different patterns of communication and parallelism. We will encounter and discuss these additional primitives as we explore more advanced parallelization techniques (e.g., ZeRO, model sharding, tensor parallelism) later in the series.\n\n\nFor now, let’s look at All-Reduce in detail, since this is exactly what we need for synchronizing the gradients during Data Parallel training.\nSince each GPU computes a gradient based only on its local micro-batch, we must add them to get the global gradient before performing the optimization step. The required communication operation is the All-Reduce primitive:\n\nInput: Different tensors (the local gradients \\(\\nabla \\Phi_1, \\nabla \\Phi_2, \\dots\\)) on each GPU.\nOperation: A reduction operation (usually summation, \\(F\\)) is applied to all tensors.\nOutput: The result of the reduction (the global gradient \\(\\sum \\nabla \\Phi_i\\)) is made available on all GPUs.\n\n\n\n\nOnce every node receives the global gradient, it performs the optimizer.step() operation independently, ensuring all model copies remain in sync. These collective operations are defined in the torch.distributed API.\nHere I’ve a machine with 4 T4 GPUs.\nray@ip-10-0-69-225:code$ nvidia-smi -L\nGPU 0: Tesla T4 (UUID: GPU-31a1b562-c769-c7f1-ede1-48847cec8d53)\nGPU 1: Tesla T4 (UUID: GPU-1beaf204-f6f7-182d-67f8-aee6c58128df)\nGPU 2: Tesla T4 (UUID: GPU-934ca246-df7e-2c7f-4bdd-b07859e46b2d)\nGPU 3: Tesla T4 (UUID: GPU-141171cb-db62-b770-97ff-955f8c7f2265)\nNow let’s create a simple example to demonstrate the All-Reduce operation by creating 4 tensors on each GPU and performing the All-Reduce operation on them.\n\nAll-Reduce Example\n\n\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_all_reduce():\n    tensor = torch.tensor([dist.get_rank() + 1] * 3, dtype=torch.float32).cuda()\n    print(f\"Before all_reduce on rank {dist.get_rank()}: {tensor}\")\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    print(f\"After all_reduce on rank {dist.get_rank()}: {tensor}\")\n\n# Initialize the process group and set the device, create a tensor on each GPU and perform the All-Reduce operation on them.\ninit_process()\nexample_all_reduce()\n\n\n\nWe can run this code on 4 GPUs using the following command:\ntorchrun --nproc_per_node=4 dist_all_reduce.py\nWe will get the following output:\nBefore all_reduce on rank 3: tensor([4., 4., 4.], device='cuda:3')\nBefore all_reduce on rank 0: tensor([1., 1., 1.], device='cuda:0')\nBefore all_reduce on rank 2: tensor([3., 3., 3.], device='cuda:2')\nBefore all_reduce on rank 1: tensor([2., 2., 2.], device='cuda:1')\n\nAfter all_reduce on rank 3: tensor([10., 10., 10.], device='cuda:3') \nAfter all_reduce on rank 0: tensor([10., 10., 10.], device='cuda:0') \nAfter all_reduce on rank 2: tensor([10., 10., 10.], device='cuda:2') \nAfter all_reduce on rank 1: tensor([10., 10., 10.], device='cuda:1') \n\n\nOverlapping Communication and Computation\nIn a naive DP implementation, the GPUs sit idle during the communication phase, as the All-Reduce operation begins only after all gradients are computed in the backward pass. This is inefficient.\n\n\n\nTo eliminate this idle time, we overlap the communication and computation.\n\nMethod: As soon as the gradient for a specific layer is computed during the backward pass (e.g., \\(\\nabla L_2\\)), we immediately trigger the All-Reduce for that gradient in the background.\nRationale: The computation of the next layer’s gradient (\\(\\nabla L_1\\)) is independent of the communication of the previous layer’s gradient (\\(\\nabla L_2\\)).\nImplementation: This technique is implemented via hooks in PyTorch (like post_accumulate_grad_hook), allowing the next computation step to proceed while the communication step runs concurrently, significantly improving throughput. It attach an all-reduce hook function to each parameter that requires gradients. So, now it communicates more frequently but in smaller packets.\n\n\nOverlapping Communication and Computation\n\n\ndef register_backward_hook(self, hook):\n    \"\"\"\n    Registers a backward hook for all parameters of the model that \n    require gradients.\n    \"\"\"\n    for p in self.module.parameters():\n        if p.requires_grad is True:\n            p.register_post_accumulate_grad_hook(hook)\n\n\n\nBefore while communication was happening, we were waiting for all the gradients to be computed in the backward pass.\n\n\n\nBut now, we are overlapping the communication and computation. So, we are not waiting for all the gradients to be computed in the backward pass. We are computing the gradients for the next layer while the communication for the previous layer is happening.\n\n\n\nWe can infact do better and communicate more efficiently by grouping the gradients into larger buckets and performing the All-Reduce operation on them. Its like packing items into boxes before shipping (have you seen at times while placing an order on Amazon, they offer us to pack multiple items into a single box to save on shipping costs ?, well thats what we are doing here but with gradients).\nWith this we can significantly reduce the communication overhead and speed up the computation operations."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#the-limitations-of-simple-data-parallelism-dp",
    "href": "posts/distributed-training-from-scratch/index.html#the-limitations-of-simple-data-parallelism-dp",
    "title": "Distributed Training From Scratch",
    "section": "The Limitations of Simple Data Parallelism (DP)",
    "text": "The Limitations of Simple Data Parallelism (DP)\nNow, that we have seen how to scale out the training with multiple GPUs using Data Parallelism, we can ask ourselves a question - is this scaling lossless ?\nThe answer is no. There’s a communication overhead associated with the Data Parallelism. And as the degree of data parallelism increases, there’s a noticeable drop in tokens per second per GPU (throughput). Although we are overlapping the communication and computation, we are still waiting for the gradients to be computed in the backward pass.\nMore importantly, all these discussions about Data Parallelism (DP) so far have assumed that the entire model can fit on a single GPU. But what if the model is too large (e.g. GPT-3 with 175B parameters) to fit in the memory of a single GPU (NVIDIA A100 with 80GB of memory)?\n\n\n\nAs model sizes grow, it becomes common that a single accelerator (GPU in our case) cannot contain all model parameters, optimizer states, and gradients. Therefore, we need to find additional ways to scale training beyond simple DP, which can allow us to train models that don’t fit on a single GPU.\nAnd that is what we will discuss in the next section - ZeRO (Zero Redundancy Optimizer)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Distributed Training From Scratch\n\n\nA comprehensive guide to understanding and implementing distributed training for deep learning models from first principles\n\n\n\n\n\nNov 15, 2025\n\n\nSuman Debnath\n\n\n\n\n\nNo matching items"
  }
]

This lecture series is on **Efficient LLMs**, and in **Part I**, we are looking at how to scale pre-training or training of language models on a cluster of GPUs with huge sequences and a lot of, you know, a big batch size, right?

---

## üîÅ Recap: Scaling LLM Training

Let's quickly recap what we covered yesterday. I would appreciate your help in recapping what we covered.

* **Marathon to LLMs:** We started with training for a marathon and then we quickly shifted to training large language models. We saw that, you know, training time in both is‚Äîif you have more training time, then your performance is going to be good. Training time for measuring training time for language models is measured in terms of number of **GPU Hours**. Right? And we started with the question of how to train **big models on big data**.
* **GPU Memory Components:** To answer that, we saw that, you know, we started with training on one GPU and we started looking at what's there in the GPU when we are training. There are primarily four things: **model parameters**, **gradients** of those parameters, **optimizer states**, and **activations**. The first three are **static** (or size is independent of the batch size), and **activations** will depend upon the batch size.
* **Batch Size Intuition:** We took a small detour to understand whether we need a bigger batch size or a smaller batch size. And we came up with the intuition that, you know, in the beginning of the training, **a smaller batch size** is okay because it is quick. It gives us, you know, it's a noisy signal but at least a signal in the right direction. We need to be quick. We should start with a smaller batch size, and towards the end, we need a stable signal‚Äîwe should have bigger batch size so that we can get a **clearer** signal.
    * A quick analogy for this could be that, you know, let's say you're going from Delhi to **Kanyakumari**. When you're starting your journey from Delhi to **Kanyakumari**, in the beginning, as long as you're moving south, you're okay, right? You're okay taking noisy steps as long as they're in the right direction. Right? And small batch sizes will give you quick, noisy steps in the right direction. But as you approach closer to **Kanyakumari**, you would want to move in the right direction. Your steps should be in the right direction. Right? So you should compute your gradients using bigger batch sizes.
    * What does that mean for language models? We saw that for one of the models recently, they started with **4 million tokens** in the beginning, and which increased, which they kept on doubling, and it increased till I think **128 million tokens** towards the end, right? So that was a quick detour on whether we need a small batch size or a big batch size.
* **Memory Footprint Breakdown:** To fully understand, you know, what's going on within the training, we looked at‚Äîwe dived deeper into‚Äîwhen we looked at what is the size of these model parameters, gradients, optimizer states, and activations. We will do a quick recap of that here.

---

## üìê Static vs. Activation Memory

### **Static Memory (Weights, Gradients, Optimizer States)**

These three things they always remain in the GPU. Right? So their dimension is‚Äîwe saw that it's $H$ (is the hidden dimension that we have). $H \times H$ because most of the weight matrices are $H \times H$ or $H \times 4H$. So it's $H \times H$ plus some bias term $H$. Right? We had **three** of these (for Q, K, V) and **one** of these (for the output projection) in the attention layer. **We estimate the total parameters in a layer.** Right. Okay. And this will remain‚Äîthis is irrespective of the batch size that you have, right, that you're using for computing your gradients. If you have $L$ layers, you will have $L \times \text{this}$ plus **token embeddings**.

### **Activation Memory (Input-Dependent)**

Whereas activation memory will depend upon the batch size. So the way it will look like is you have a batch size, let's say $B$. In each batch you have a sequence, which is a sentence, right? A sequence of, let's say, length $S$. Right? And each token here will have an $H$-dimensional embedding. So this is $B \times S \times H$. Right. So this is how an activation block will look like. Right.

We counted these. We have **many** of these in a layer at different points. Right? And there is one special operation which is **attention** where we need to compute **the** probability of a token of a query on all the keys. Right? And every other token acts as a key. Right?

* For the attention layer, we have this **sequence length $\times$ sequence length** ($S \times S$). Right? This is the memory that we need for activation in the attention layer.
* If we have $N$ number of heads, so it's going to be $N_{\text{heads}} \times S^2$, right? And we will have one for each element of the batch. So **Batch $\times$ Number of Heads $\times$ Sequence Length Squared** ($B \times N_{\text{heads}} \times S^2$), right?
* In one layer, we have **a few** of this.

So this gives you an idea of, you know, what is the difference between the static weights, which are not dependent of on the input‚Äîthese things‚Äî**which scale with $H^2$ or linearly with $L$**, and activation memory, which is really huge, right, which depends on the batch size as well, right?

### **Taming the Beast: Activation Reduction**

Then we looked at because this is the biggest culprit, we want to see how can we reduce the activation memory, right?

1.  **Activation Recomputation (Gradient Checkpointing):** The first tool that we have there is activation recomputation. That means **do not store** these activations; rather, recompute them.
    * We saw that the **attention matrix** is the biggest culprit ($S^2$ term). So we do not store the activations corresponding to **Multi-Headed Attention (MHA)**. We rather recompute them when we are doing backward pass. Right?
    * We save $\mathbf{70\%}$ of the activation memory at the cost of **2 to 4% extra compute**. Right? If we checkpoint everything, if we recompute everything and save only at the end of the layer, we will have 30 to 40% increase in the computation time, but activation memory will reduce further, right?
2.  **Gradient Accumulation (Increasing Batch Size):** Then we saw, you know, how to increase the batch size. And for that, the next tool in our hand was **Gradient Accumulation** where we sliced these activations across this dimension (dimension zero, the batch dimension), right?
    * We take some **micro-batches (MBS)**, we call this as a micro batch. If we cannot fit the activation memory for the entire batch in the memory, we split this cuboid by this dimension zero, which is the dimension of batch size, right, and run only a micro batch in one go.
    * Compute the gradients. **Do not do `optimizer.step()`**. Save those gradients and have another micro batch. Do its forward pass, backward pass, accumulate the gradients, and hence the name **Gradient Accumulation**. Right?
    * If you are using any tools for training, for example, **Hugging Face Trainer** or **SFT Trainer**, there you can‚Äîwhen you are initializing your trainer class‚Äîyou can specify this parameter. There's a parameter called **`gradient_accumulation_steps`**. You can specify it there, right?

---

## ‚ö° Scaling with Multiple GPUs: Data Parallelism (DP)

We then asked the question: **Can we parallelize? Can we speed up if we have more GPUs?** All of this was on one GPU, and we are now asking the question: Can we parallelize if we have more GPUs?

Many of you came up with the idea that these micro batches are independent of each other. What we can do is that we can have these micro batches computed on different GPUs.

* In gradient accumulation, we were processing micro batches sequentially. To speed up, we said that we observed that they are independent of each other, and we said that, you know, instead of computing processing them sequentially, we process them **parallelly on three different GPUs**, right?
* We do this by keeping a **replica** of the model, gradients, and optimizer on each GPU. Right? **Same operations, different data**. Right? Exactly same operations, but they are being operated on different data. That will give us different activations and different gradients. Right?
* Once we have these gradients computed on three different GPUs (in this case), we need to **add them to get the global gradient** before we do **`optimizer.step()`**. Right? And that introduces, you know, **communication** between the GPUs, right? That is where we stopped yesterday: how to communicate gradients across GPUs.

### **The All-Reduce Primitive**

That was the question where we stopped yesterday. Right?

For communication, now PyTorch also provides various primitives, right? I will introduce those primitives as and when we encounter them, right?

* Here, we have to take the gradients from three different GPUs, let's say A, B, and C. These are $\nabla W$ computed on the micro batch $X_1$, $\nabla W$ computed on $X_2$, and $\nabla W$ computed on $X_3$, right?
* To compute the overall gradient, I need to **add them**, right? And after adding them, I need to send them to all the nodes because all the nodes are doing the same operations. All the nodes have their own copy of model weights, optimizer states, and they need to update them separately. This is what will keep the models in sync with each other, right? Because their update steps are also same, right?
* This primitive is called **All-Reduce**. The function $F$ here is an addition, and then $X$ in this case is $A+B+C$.

These are called **Collective Operations** and also **communication primitives**. They are defined in the **`torch.distributed`** API.

#### **All-Reduce Example**

We looked at a small code snippet to illustrate All-Reduce.

* We set up the communication backend using `dist.init_process_group`, which assigns a **rank** to each GPU (0, 1, 2, etc.).
* A tensor is created on each GPU based on its rank: Rank 0 gets a vector of **ones**, Rank 1 gets a vector of **twos**, and Rank 2 gets a vector of **threes**.
* When we run `dist.all_reduce(tensor, op=dist.ReduceOp.SUM)`, the result is a vector of **sixes** on **all three GPUs**. (i.e., $1+2+3=6$ on all ranks).
    * **Before All-Reduce:** Three different tensors on three different devices.
    * **After All-Reduce:** The same tensor (the sum) on all the machines.

### **Overlapping Communication and Computation**

Let's look at the computation and the communication timeline for this.

* In the forward pass, we go through Layer 0, Layer 1, and Layer 2.
* Then we do the backward pass, which computes the gradients on Layer 2, Layer 1, and Layer 0. This happens on both the GPUs simultaneously on different data points (same operations, different data points, different GPUs).
* Once we have computed **all** the gradients in the backward pass, we need to communicate them. **That's when communication will kick in.**
* While this communication is happening, the GPUs are sitting **idle**‚Äîthey can't do anything.

**Issue:** GPUs are idle during the communication phase.

**Solution: Overlap Communication and Computation**

Instead of communicating all the gradients together, as soon as we have computed the gradient for a layer, we can send it across for communication. This is because computation of a gradient for a layer is independent of the communication of the gradient for a previously computed layer.

* As soon as the gradient for the last module (Layer 2) is computed, we can actually start the communication for this **in the background**.
* This means our **computation and communication are overlapping**. We don't have to wait for the entire backward pass to complete. As soon as we have the gradients for a specific layer/module, we start the communication for that segment.
* **Implementation:** This is implemented via **hooks** (specifically `post_accumulate_grad_hook` in PyTorch) which triggers the communication function as soon as the gradient for a parameter is accumulated.

**Bucketing Gradients:** Communicating at a parameter level means a lot of small packets. We can optimize this by **bucketing the gradients**, which means waiting for the entire layer, for example, to finish its backward pass, and then sending all of its gradients together.

### **Combining DP and Gradient Accumulation**

Since micro-batches within gradient accumulation also use the same model weights, we can naturally combine **Data Parallelism (DP)** with **Gradient Accumulation (GA)**.

* **Global Batch Size:** $B_{\text{Global}} = B_{\text{Micro}} \times N_{\text{GA}} \times N_{\text{DP}}$ ($N_{\text{DP}}$ is the degree of data parallelism, i.e., number of GPUs).
* **Example:** For a target global batch size of 1024 sequences (4 million tokens total):
    1.  Max out **Micro-Batch Size** ($B_{\text{Micro}}$) based on the memory of one GPU (e.g., $B_{\text{Micro}}=2$).
    2.  Set **Data Parallelism** ($N_{\text{DP}}$) based on available resources (e.g., $N_{\text{DP}}=128$).
    3.  Set **Gradient Accumulation Steps** ($N_{\text{GA}}$) for the remaining factor: $1024 = 2 \times N_{\text{GA}} \times 128 \implies N_{\text{GA}} = 4$.

---

## üö´ Hitting the Wall: Limitations of DP

**Question: Is the scaling lossless?** Will $N$ GPUs give $N$ times the throughput?

* **Answer:** No. There's a communication overhead. As the degree of data parallelism increases, there's a noticeable drop in **tokens per second per GPU** (throughput).

**Assumption Check for Vanilla DP:** The primary assumption of vanilla DP is that the **entire model fits on one GPU**.
* For an 8 billion parameter model, the memory footprint (parameters + gradients + optimizer states) is $\approx \mathbf{100\, \text{GB}}$, which is above the standard $\mathbf{80\, \text{GB}}$ GPU limit.

**Solution: Sharding the Static Memory (Zero Redundancy Optimizer)**

Since the model (weights, gradients, and optimizer states) does not fit, we must shard the static components across the GPUs. This is the idea behind **ZeRO (Zero Redundancy Optimizer)**.

| Strategy | Sharded Component | Total Memory (Excluding Activations) | Communication Primitive(s) | Communication Cost (Total Z) |
| :---: | :---: | :---: | :---: | :---: |
| **DP (Vanilla)** | Activations (Data) | $16Z$ | **All-Reduce** (Gradients) | $1Z$ |
| **ZeRO-1** | Optimizer States ($8Z$) | $8Z + 8Z/N_{\text{DP}}$ | **All-Reduce** $\rightarrow$ **Reduce-Scatter** (Gradients); **All-Gather** (Parameters) | $2Z$ |
| **ZeRO-2** | Optimizer States ($8Z$) + **Gradients ($4Z$)** | $4Z + 8Z/N_{\text{DP}}$ | **Reduce-Scatter** (Gradients); **All-Gather** (Parameters) | $2Z$ |
| **ZeRO-3** | Optimizer States, Gradients, **Parameters ($4Z$)** | $\approx 4Z/N_{\text{DP}}$ | **All-Gather** (Parameters); **Reduce-Scatter** (Gradients) | $3Z$ |

* $Z$ is the total number of parameters. Memory is computed in FP32 equivalent units (bytes).
* $N_{\text{DP}}$ is the number of data parallel ranks (GPUs).

### **Detailed Look at ZeRO-2**

In ZeRO-2, we shard the optimizer states and the gradients.

1.  **Forward Pass:** All GPUs maintain a full copy of the model weights and do the forward pass on their micro-batches.
2.  **Backward Pass:** Each GPU computes the full gradient ($\nabla W$) for all parameters.
3.  **Reduce-Scatter:** Instead of All-Reduce, we use **Reduce-Scatter** on the gradients. This operation:
    * **Reduces (sums/averages)** the gradients across all GPUs.
    * **Scatters** the result, so that each GPU receives and keeps only the portion of the gradient corresponding to its sharded parameters/optimizer states.
    * We use a transient buffer to compute the full gradient, but we only **persist** the $1/N_{\text{DP}}$ slice.
4.  **Optimizer Step:** Each GPU updates only its $1/N_{\text{DP}}$ slice of the optimizer states and model parameters.
5.  **All-Gather:** Since the updated weights are now sharded, an **All-Gather** operation is needed after the optimization step to reconstruct the full, updated model on all GPUs before the next forward pass starts.

By sharding gradients in ZeRO-2, we eliminate the need to reserve persistent memory for all $4Z$ bytes of gradients, which were redundant in DP.

### **Detailed Look at ZeRO-3 (Full Sharding)**

ZeRO-3 shards all memory components, including the model parameters, bringing the memory requirement down to its minimum, allowing very large models to fit.

* **Issue:** If the model parameters are sharded, a GPU only holds $1/N_{\text{DP}}$ of the weights. It cannot do a forward pass through a full layer.
* **Solution: Overlapped All-Gather (Prefetching):** In the forward pass, when computing Layer $L$, the GPU prefetches (All-Gathers) the parameters needed for Layer $L+1$.
    * Fetch Layer $L$'s parameters $\rightarrow$ Forward pass through Layer $L$ $\rightarrow$ **Flush** Layer $L$'s parameters (once no longer needed for the current forward pass).
    * This communication overhead is $3Z$: All-Gather for forward, All-Gather for backward, and Reduce-Scatter for gradient update.
* **Result:** ZeRO-3 can fit an 8 billion parameter model with a sequence length of 16K within the 80 GB limit. ****

---

## ‚è≠Ô∏è What's Next: Tensor Parallelism

We have focused on **Data Parallelism (DP)**, which parallelizes across the **batch dimension** (dimension 0: $B \times S \times H$). This assumes that the activations for at least **one sequence** ($S \times H$) should fit into the memory.

**Question:** Can we somehow shard these activations as well?

* If we shard the activations **for a single sequence**, it means we are using the **same data** across different GPUs.
* This will bring us to **Tensor Parallelism**, where we shard the weights and activations within a layer (across the $S$ or $H$ dimensions) so that a single sequence can be processed across multiple GPUs.

We will start with sharding the **MLP Block** (specifically the $H \to 4H$ and $4H \to H$ linear layers) next time.

Would you like me to elaborate on the **Reduce-Scatter** or **All-Gather** collective operations?
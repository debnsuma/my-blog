Okay. So let's get started. And uh let's start with a very small story, **an** anecdote. Uh let us say you guys are training for **a** marathon, right? And uh if I ask you to plot a graph between what would be your performance versus how much training, you know, training hours you put in for training, what would that graph look like? So here on the y-axis, we have the training time and on the x-axis, we have the performance. So here, what would what do you expect what would be the graph? Right. So something, right, something like this. **It's expected to be a non-linear relationship where performance gains diminish after a certain point.** And then, of course, there are a lot of other parameters. For example, who is training, right? If **Usain** Bolt is training, then it's going to be different for him versus if you and I are training, right? Our capacity will also be a factor there, right?

So the same story holds for language models, and here we are looking at **the LLaMA** family of models, right? The size of the circle represents the size of the model. And here, we can see that as the model size increases, its performance is improving. **A** bigger model is giving me better performance, right? But then it requires more training as well. The number of GPU hours required for training are really huge. And and and focus on the units on the y-axis. What does it say? Million GPU hours, right? So how are we supposed to do it in one lifetime if we were to do it on one GPU? Is it **feasible**? It's not. It's not feasible, right? And that is what the first part of the talk will focus on. We will see how to efficiently train these LLMs or pre-train these LLMs, how to exploit multiple GPUs that we have for training, right? So we will look at different forms of parallelism: data parallelism, tensor parallelism, and so and so forth.

-----

### **üìà Efficient Implementation and Design**

Then, uh let's look at another graph here. Here on the x-axis, we have again the performance on some uh leaderboard, and on the y-axis, we have the speed, as in uh at at what speed are we able to do inference on the model. Right? We are measuring it in terms of **throughput**, which is how many tokens the model is able to spit out per second. Right? So here also, we see that uh smaller models have lower performance and very high throughput, right? Uh but but we see some interesting points here. For example, if you look at these two models, right, MPT 7 billion and **LLaMA** 1 7 billion. Similar size, similar performance, right? They're roughly **performing at the same level**, right? They lie here, but they have very different throughput. So how is it possible? How, why is this more efficient in terms of **throughput** than this? Right? So that brings us to the second part of the talk, which is **Efficient Implementation**. Right? Here, we will look at **FlashAttention** or **PagedAttention**. Right? So these are implementing the same algorithm in a more efficient way and, if required, going to the GPU kernel and and and writing specific kernels for specific operations. For example, that's what **FlashAttention** does, right?

Okay. Now again, coming back to the same chart, let's look at these two data points, right? Again, similar performance, similar sizes, but one has better throughput than the other. So one is **a Mixture of Experts (MoE)** model, 8 cross 7, roughly 56 billion parameters. The other is **LLaMA** 1 33 billion. Right? So how come similar size models again have so different throughput? Right? Here, we will look at the third aspect which can give us better throughput and better efficiency, which is **Better Design**. Right? So we are changing‚Äîwe are we are uh designing a different algorithm or a better algorithm. In the previous one, we were writing the same algorithm efficiently. Here, we are saying that we will come up with different architectures or better architectures which will give us more efficiency. Right? So in particular, we may look at **Grouped-Query Attention (GQA)** or **Mixture of Expert (MoE)** models and if time permits, **we'll cover other architectures.**

-----

### **üó∫Ô∏è Lecture Series Overview**

Okay. So so this is the broad overview of what we are going to cover in the next uh five lectures. We will look at, you know, how to scale training, where we will look at different forms of parallelism. And then we will look at **efficient implementation** leading to better throughput and better performance, better scaling. And then we will look at **efficient design** or efficient architectures for better throughput and better scaling and better performance as well, right?

Okay. So for the first part, we will religiously follow this tutorial. It's an amazing tutorial on **Hugging Face**. You will get the link. It's called the **"Ultra-scale Playbook"**, right? We'll we'll try to follow it as much as possible.

-----

### **üß† Deep Learning Training Basics**

Okay. So let's start from the basics, right? So how many of you have ever trained a neural model or a deep learning model? Forget about LLM, right? Okay. So what are the broad steps when you train a neural model? So what is the first thing that you do during training? Yeah, that's that's pre-processing of the data. Let's say you have clean data, and you have written the class as well. Let's say we are training a simple **MLP** for now, right? My model class is an MLP class, right, which is an **NN.Module** in PyTorch, right? We're following PyTorch implementation, right? So you will have **NN.Linear** layer, right? So X or whatever, whatever you want to define, right? Then you can have a function one, function two, which could be **ReLU**, right? And then you can have another non-linear layer. Let's say we have a simple MLP. We have defined that, and we will have the corresponding **`forward`** function, right? We would define the corresponding `forward` function of this class, right, with some input $x$.

Right. So once we have written the code, when we are training the training loop, what would the training loop look like? Good. Very good. So:

```python
for batch in data_loader:
    # 1. Move batch to GPU
    batch = batch.cuda()

    # 2. Forward pass and Loss
    loss = model(batch) 
    
    # 3. Backpropagation
    loss.backward()
    
    # 4. Optimization
    optimizer.step()
    # 5. Clear gradients (often done implicitly or at the start of the loop)
    optimizer.zero_grad() 
```

Good. Very good. So `batch in data_loader`. Yes. Then **move the batch to the GPU**. Okay. Good. So `batch = batch.cuda()`. This puts it on the GPU, right? Very good. Then assuming that yeah, so we have model class, model. Let's call it `model`, right? We pass it through this. What will we get? Let's assume we get loss, right? Let let's say the model is computing the loss, or it can give you the activations or the probabilities in the end. Then you can compute the loss either way, right? You can do that in the form. Okay, very good. Then we do `loss.backward()`. Then **we do `optimizer.step()`**. Okay, cool. So so `loss.backward` is right, right. We do this. Okay. Then we'll have something called `optimizer` will do `optimizer.step()`. Amazing.

-----

### **üíæ GPU Memory Footprint**

So now let's look at what happens in the GPU memory when we do this, right? Uh what all is eating up the GPU memory? Right? So this is the illustration here. **You have multiple layers/heads in a model.** We are doing a forward pass as you have described, right? Then when you do the forward pass, right, what will what, you know, so so what will be there in the memory?

1.  **Input Data (`X`):** You have already put your batch, your data input $x$.
2.  **Model Weights (Parameters):** Your model weights‚Äîthat is also on the GPU. All of these layers, their parameters are going to be on the GPU.
3.  **Activations:** Each layer will give you some output. So input is $x$ here. You will have $L_1(x)$, right? This will also be on the GPU. We call these the output of these layers as **activations**, right?

So why do we need these activations? Why are we keeping them in the memory? **So we can pass it to the next layer and then clear the memory?** Right? So for example, when we are here in first, second, third, fourth layer, right? Output of the third layer is going to be $L_3(x)$. Do I need to keep this in the memory? Right. So we will need it for **backpropagation**. Right. Right. So where do we need it for backpropagation? Yeah. So even if you have a linear layer, let's say $y = Wx$, right? What is the derivative in the backpropagation? We have to compute the derivative of $W$, right? So derivative of $W$. What is the derivative of $W$?

So let's say when we are backpropagating, you will have the derivative of loss with respect to $y$. When you're backpropagating, you will get that as input, right? So what would be derivative of $W$ with respect to the loss? So it's going to be $\frac{\partial L}{\partial W}$. $\frac{\partial L}{\partial W}$ is $\frac{\partial L}{\partial y} \cdot x^T$.

**The mathematical expression for the gradient of the loss $L$ with respect to the weight matrix $W$ in a linear layer $y=Wx$ is:**
$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y} \cdot x^T$$

That is the **formula**. I mean, you can check the shape, right? So this is $10 \times 1$. This is $1 \times 100$. So the derivative will turn out to be a $10 \times 100$ matrix. That's what we need. We need the derivative of each of the element of $W$, right? I will not prove this here. Right? So to compute the gradient in backpropagation, we have to get the gradient of $W$, and therefore, we need to save $x$. What is the input of that layer? Right? These are called activations, and therefore, we need to save them in the memory until we have done the entire backward step and **optimizer step**. Right. So we need to save the activations. Model parameters are there, and then you have activations, right? I can include this $x$ within the activations because $x$ is the input to the first layer. Right? The batch is input to the first layer itself. Right? So I can I can put that in activations.

Okay. What what else would be in the memory when you **compute the gradients**? So activations are stored till here, right? We have computed the activations here. When you do `loss.backward()`, you're computing the gradients, right? So just like every weight $W$ was there, we'll have corresponding $dW$ in the memory, right? So we will have the **gradients**, right? And then finally, what happens within the optimizer? For example, **Adam**. I'm assuming that you are aware of what **Adam** is. So what all it needs to store in an **Adam** optimizer? **First moment and second moment.** Correct. So optimizer also needs memory because you need to update the first and second order moment for every parameter, right? For every $W$, you will have that, right? So you will have **optimizer state**, right? And **it is a 2x factor because of the first moment and second moment.** Right? So all of this remains in the memory till you do `optim.step()`, right?

Even after `optim.step()`, what all will get cleared from the memory? What all will remain in the memory? Parameters will remain. Yes, **gradients will be cleared**. Yeah, we may not need gradients, but we can reserve **space**. Yeah, we may not need; we can flush out gradients because we have used them, right? Optimizer states, do we persist them? Sure. That will also remain, right? And because we have to allocate space again and again for gradients, PyTorch does some preparatory work and reserves some persistent memory or cache for that because it will need that. It knows that it is going to need that. Right? So it will keep space for the gradients as well. Right? On the other hand, activations are input dependent. They will get cleared, right? And the next batch will come and we'll fill the activations again. Right?

So good. So we have already gone through through this. Right? So there's a forward pass and then we do backward pass. So this is the space, let's say, for the model parameters. This is the space for the gradients. We need to keep all of them. And we do the optimization step where we have two units for first order moment and second order moment. And then we update the gradients. We get the updated parameter. We update the parameters and get the updated parameters, right? So this is what we have covered. We need **model weights**, **model gradients**, **optimizer states**, and **activations** which are needed to compute the gradient, right?

And in this example, if this matrix, this first layer is let's say the weights are $W$ and the second layer weights are $M$, right? Uh model weights are going to be $W$ and $M$. Model gradients are going to be $dW$ and $dM$, which we will save. Optimizer states we'll save separately, and activations are going to be these, right? The hidden vector that we will get when we pass this input $x$ to the $L_i$, right?

Okay. So this is **memory profile** of **the** first four training steps of a very small **LLaMA** model. Right. As you can see, model parameters are always there. This this is model parameters, right? When we are doing the first pass, first step, activation memory is **peaking** up. It peaks till here, right? Notice that in the first step, it stays there for long. This is because in the first step, PyTorch is doing some preparatory work, reserving memory for, you know, what what it expects would come in the future. Then, as the backward pass starts, now memory for the gradient‚Äîthis orange part‚Äîis getting increased. Activation memory is simultaneously reducing, right? And then, after the first step, you see that the optimizer state has also come into play, right? So, so this is the optimizer memory, which is almost double of the model parameters, right?

Okay. So, as we can see here, that memory for even one sequence, right, is roughly 50 GB‚Äîis more than 50 GB, right? I think this is for one sequence of 4,196 or 4,096 tokens, right? But when we do a forward pass, we do it with a batch‚Äîa bigger batch, right, which contains not just one sequence but multiple sequences, right?

-----

### **üìè Batch Size and Memory Scaling**

So typically what is what is a typical batch size which we use for pre-training of LLMs? Any guesses? What do you think? **Two.** Okay. So, so a clarification: so typically when we talk about batch size in pre-training, we talk about it in terms of number of tokens, not number of sequences. So, right. So, sequence length into, you know, **micro-batch size** or our batch size would give me token batch size. This is the formula, right? Uh let's say we have 1,024 sequence length tokens **per** sequence, right? And we have **400** sequences. Then our batch size is going to be $1,024 \times 400$ tokens, whatever that comes out to, right? Roughly 4 million‚Äîno, 4 million. Yeah, right? So so we talk in terms of tokens, number of tokens. So typically what is the batch size we use in pre-training? I think. **It's usually very large, in the millions of tokens.**

Okay. So what is the advantage of bigger batch size versus smaller batch size? **Bigger batch size gives a more accurate gradient direction, which reduces the noise and helps convergence.** Right. If it takes if it takes more time, will it take more time to converge or will it take less time to converge in terms of overall clock time? Right. So correct. Right. So so typically what we do is, as you said, that smaller batch size, it will be fast to do the forward pass. Bigger batch size, it will be very slow to do the forward pass. Right. So typically what we do is that in the beginning of the training, when loss is very high, we take a smaller batch size because we want quick signals, right? Right? So we quickly want to move through the loss landscape. And as we are approaching the optima or or the minima, now we want to take small steps, or we want to take steps‚Äîwe want to be more confident about our about the steps that we are taking, right? We want good signal. If you talk in terms of signal-to-noise ratio, initially we are okay with noisy signal because you want to move. When we are very far away from **optima**, you want to move very quickly, and therefore, smaller batch size works better. And as we move ahead in the training, we prefer larger batch size, right?

So people have fit a power law curve to what is the optimal batch size. So they define something called **critical batch size**, which tells me, you know, at a particular loss level, what batch size should I use that would give me fastest convergence, right? They fit a power law curve, and in one of the papers, for example, they start with 16 million tokens, right? **They** compare it with two sequences that someone was proposing, right? In the beginning, they double it at 16, when the training has seen 69 billion tokens. Then they double it again at 690 billion tokens, and then again at 4.7 trillion tokens, right? So we start with smaller batch size and keep on increasing that. And the order of the small batch size here is 16 million tokens, right?

So what should we do? I mean, this is this is for roughly 4,000 tokens, 50 GB memory, right? So how should we fit bigger batches? How should we train? Right. Okay. So before we do that, let's, I mean, to answer that question, we need to carefully look at what exactly is there in the memory and what all can we reduce and improve.

-----

### **üî¢ Calculating Static Memory (Weights, Gradients, Optimizer States)**

Okay. Okay. So let's quickly look at how much‚Äîlet's compute how much memory weights, gradients, and optimizer states take, right? So this is the static kind of static memory, right? The other‚Äîthe last part is activation memory, which is dependent on the input. This will always remain on the GPU, right? So this is, I mean, for reference, I have one layer of transformer, and we will compute the total memory which is required for this layer, right? So let's say our vocabulary size is $V$ and embedding size is $H$, right? So $V$ is the vocabulary size and $H$ is the hidden dimension.

So how much would token embeddings take? How many token embeddings are there? We have $V$ tokens. Each token is taking $H$ dimensions. So we have $H \times V$. Right? So these are these many parameters, and depending on whether we have four bytes or two bytes for one parameter, we will have the memory.

Okay? Now let's talk about **Multi-Headed Attention (MHA)**. Uh what all goes in **Multi-Headed Attention**? What is the first thing that happens? **Query (Q), Key (K), Value (V) projections.** Yeah, we have Q, K, V, right? So these matrices, and they are of size $H^2$, right? $H \times H$, right? I mean, even if you have multiple heads, we ensure that the output is $H$ dimensional only. Plus $H$, if we have the corresponding bias, right? Times 3, right? **So $3(H^2 + H)$ for the Q, K, V projections.** And then once we have the output from each head, we concatenate them and pass it through another projection layer, right? So that is again 1 **MLP** which is $H^2 + H$. Sorry, which is $H^2$. **No, it is** $H^2 + H$. **It's an $H \times H$ projection, which is $H^2$ parameters, plus $H$ for the bias.** Right? $H$ is the dimension of the input and $H$ is the dimension of the output, right?

Uh then we have **Layer Norm**. What does the layer norm do? **It normalizes the input across the hidden dimension.** Right? So if we have a vector $x$ of $H$ dimension, what will it what will **it** do on this? Normalize it. And does it have any parameters? **Yes, it has two trainable parameters: $\gamma$ (scale) and $\beta$ (shift).** Okay, so it there are two parameters because it not only normalizes it, it defines where should the final value be, right? So it has its own $\mu$ plus $\sigma \times (x - \text{Expected Value of } x)$ or divided by, let's say, $x^2$ expected value of $x^2$, depending on how you normalize. You may have, you may or may not have this, but $\mu$ and $\sigma$ are the corresponding parameters. It is normalizing and then putting it in some other space with mean $\mu$ and $\sigma$, right? So these are the two parameters, right? So we will have $2H$ for each dimension, right?

Okay. And what about **MLP**? So **MLP** $H$ grows to $4H$ and then comes back to $H$, right? So we have $H \times 4H + 4H$. Right? These are the weights of the first layer. **Then the second layer is $4H \times H + H$.** Not into two, because here the bias is going to be $H$ only. Here the bias was $4H$, right? So it's going to be $4H^2 + H$ **(for the second linear layer)**. Right? So here we have $4H$ and we have $H$. **Wait, the first layer is $H \times 4H$ weights and $4H$ bias. The second is $4H \times H$ weights and $H$ bias.** Okay. And then again, layer norm, which is going to be $2H$, right?

So it will turn out to be this. So this is the memory that is required for weights, gradients, and optimizer states. It's going to be proportional to this. So we have counted the number of parameters that are there in in an **MLP** layer. So this $L \times \text{this}$ ($L$ is the number of layers) and $H \times V$ we have token embeddings initially only. And then in the after the $L$ layers, we have a layer norm‚Äîan extra layer norm. This $2H$ corresponds to that, right?

And with these parameters, if we are storing in **FP32**, which is the classical way of storing, right, which requires four bytes per parameter, we will require $4N$ memory (four bytes for parameters), $4N$ for gradients, and $8N$ for optimizer first and second order moment, right? Total $16N$ bytes roughly, right?

And we do computations in **mixed precision** as well. So in mixed precision training, we keep the parameters and the gradients, right, in low precision. So all the computations are done in low precision. All the activations are computed in low precision. Once we have computed the gradients and we have to update the optimizer state using those gradients, we do that in high precision because that is accumulating over time. We want high precision there, right? So, so for optimizer, we keep it same, $8N$. For parameter and gradients for doing computation, we reduce it by half. And in addition, we keep a high-precision copy of the weights as well, right? Because weights are also getting kind of updated over time, and we want them in high precision, right? So we have a copy of the weights, we call that **master weights**‚Äîthat is in $4N$. And this also totals $16N$.

So why are we doing this? Faster computation. Very good. And so so we saw that, right? Uh these this memory will always be there, right? It's something else which comes and **goes** which is dependent on the input. This memory PyTorch reserves after **the** first pass, right? What is the last component of the memory other than these which which was taking up most of the space? **Activations**, right? That was taking the most of the space, right? And if we are doing computation in low precision, we will have activations in low precision, and that is why we are using mixed precision: activation faster computation‚Äîvery good‚Äîsecond is we save on activation memory, right?

Okay. Sorry. This one. Yeah. Okay. So let's say we are using **Adam** optimizer, right? Let's say we have only one matrix $W$ that we are updating, simplified, right? **Adam requires first moment ($m_t$) and second moment ($v_t$).** Yeah. **Adam** requires first moment and second moment, right? So it requires it keeps a running or exponentially weighted average of the gradients at every step $t$, right? It keeps on accumulating that in memory, right? And then it also has $v_t$ where it keeps on accumulating **the** square of the gradients, right? This is used to update the memory, right? So we have two parameters for each, **two** memory slots for each parameter, and so therefore, it's $8N$. Right. Clear. Okay.

Cool. So roughly, you know, you can multiply it with 16 to give to get the amount of memory that you will require on GPU. It's a rough **estimate**. So 1 billion will require roughly 16 GB memory, right?

-----

### **‚ö†Ô∏è Activation Memory Bottleneck**

Okay. So now here we looked at memory for weights, gradient, and optimizer states. And let's look at the the biggest beast, which is memory for activations, right? Right. So what is‚Äîso yeah, so here we have the transformer layer on the left, and for illustration, I have this the the attention block here, right? So let's look at how much memory will the activations require, right? So we have Q, K, V. All of these will have some input, right? That will require some activation, right? So that will be batch size times sequence length. This is the number of tokens that we have, right? Times $H$, right? This is the same input. Yeah, this is the token embedding **vector** that is input to the QKV matrices, right? Make sense?

Okay. Then we when we are doing self-attention, we take the $Q$ and $K$ and we multiply them, $Q K^T$. That will also require some memory, right? And then we do **Softmax**. Right. **Softmax** will also require some memory. How much memory will **Softmax** require? What do we do in **Softmax**? So for each sequence, right, for each sequence we have, let's say, 100 tokens. Each token will attend on itself on on all other tokens, right? So it's going to be sequence square, right? Right? So we need probability. What is the probability or or the attention weight for the last token with respect to all the previous **tokens**, let's say, right? Those are‚Äîwe have a probability distribution there. And then we have this for for all the tokens. So it's roughly sequence of the order of sequence square, right? And we have some number of heads here, right? And then we have batch size as well. So for for each sequence, we have sequence square times number of heads plus batch size. So this is of the order of this.

So everywhere else, it's of the order of batch size, sequence length, and $H$. And in the attention block within **Softmax**, because we are attending, each token is attending on some other token, we have of the order of sequence square, right? So let's skip this part and let's look at the final formula. You can you can do this on your own. This is simple, right?

So what we get is that for the attention block, the amount of activation memory that we'll require‚Äîthe important thing to note here is it's it's **sequence square**, right? And everywhere else, it's going to be of the order of **sequence length times batch size times $H$**. These are the number of tokens that we have, and it is linear with respect to that, right? Batch size $\times$ sequence is the number of tokens that we have, and $H$ is the dimension of each token, right?

So, okay, we **can** see that it scales linearly. Sorry for for this mess up here. The final formula here you can see, you know, some factor times sequence length $\times$ batch size $\times$ hidden dimension plus sequence square. This is of the order of sequence square. And this is the thing that we may have to tame later on, right? So to get a sense of, you know, how it looks like, look at the scale on the y-axis here, right? Even for an 8 billion model, if you want to train with longer sequences, because **sequence length squared** becomes the bottleneck, it goes up to up to 1,500 GB, right?

So how should we handle these situations, right?

-----

### **‚úÖ Solution 1: Gradient Checkpointing (Activation Recomputing)**

Okay. So any guesses what can we do if activation is taking a lot of memory? Can we somehow avoid storing those activations? So recall, why do we need these activations? Where do we need this? **For backpropagation.** Right? So we have a simple layer $y=Wx$, right? I get $X_1$ and I'm storing the input $X$, and this is the activation that I'm storing, right? And for the next layer again, you know, whatever is the output of this, let's say $X_0$, this is $X_1$. We are storing this, and and each layer has multi-headed attention and **MLP**, and we are storing activations for all of them in between. Right, we saw what what all we are saving, right?

So is there a way where we can avoid saving the internal activations? **Yes.** Well, if you do **Dropout**, then you will have to store the **Dropout mask** as well for backpropagation, and then, you know, after that, when you're backpropagating, you have to go through that as well, right? **Afterwards.** **You could store them on the CPU memory.** So you are saying that we do not‚Äîwe don't store it in the GPU, we store it in the CPU. That's what you're recommending, right?

Okay. Okay, let's try to build up the solution here. Let's say I this is one layer. It has a multi-headed attention and then **MLP**, right? All of these have their internal internal activations as well. Within them as well, there is an activation, right? And then **MLP** will also have its own activation input‚Äîits input, right? So this is $X_0$, and this is $X_1$. Now, we saw that, you know, the number of activations for **Multi-Headed Attention** they were of the order of **sequence squared**, right? And here, this was of the order of sequence $\times$ batch ($S \times B \times H$). This is sequence squared $\times$ batch $\times$ number of heads, right? This was sequence‚Äîeverything else is sequence $\times$ batch $\times H$ this order, right?

If, let's say, I don't‚ÄîI do not save this, and I don't save this. I I do not store these activations, right? I store $X_0$, which is the input to this layer. I store $X_1$, which is the input to the next layer in between. I'm not storing anything. Does that give you a hint? **We need to recompute them during the backward pass.**

What will I need? So in between, there are multiple steps, and I need the input to all of these steps, right? Each layer, let's say, has $F_1, F_2, F_3$ functions, right? And these $F_1, F_2, F_3$ these are **Multi-Headed Attention**, **MLP**, **Layer Norm**, and all of those, right? Uh for backpropagation, for example, through this layer, which is computing $F_2$, I need what is the input to the $F_2$, right? This‚ÄîI'm I'm zooming this. I have this block which is computing $F_2$. To compute the gradient for this, I need what is the input to $F_2$, and that is $F_1$ of $X_1$. $X_1$ is the input here, right? Right? So $X_1$ I'm storing, right? For $F_2$, I need $F_1(X_1)$, right? To to compute the gradients. So instead of saving this, can I compute it again?

I'm storing something in between, some **checkpoints**, right? So, as I said, after every layer, I'm storing something, right? And I go through the layer again to compute the internal activations, right? So for that, we need how much memory? We need we need memory **for** activations for a single layer, not for all the layers. But what is the‚Äîthere's no free lunch, right? What is the cost we are paying? **Time/computation cost.** Right? But at least this is making it feasible, right? We saw that it was not feasible earlier because memory was growing up. We have, let's say, only 80 GB per GPU, right?

So this is called **Activation Recomputing** or **Gradient Checkpointing**, right? What we are doing‚Äîso this is the usual flow. And what we do is that we save these checkpoints. We save these activations, and internally for the activations within the layer, we do a small forward pass during the backward pass, right?

So now, you know, there are different ways of doing it. If you are storing it, if you are **storing** only at the end of each layer, that is called **Full Activation Checkpointing**, right? It saves most of the memory because you're storing very less at the end of every layer, but it is the most expensive. Your computation cost will go by 30 to 40%, right?

But through the profiling of the activations, we know that what takes most of the space? Which activations? Activations of which layer? Which which part? Sorry. Yeah. So that was **Multi-Headed Attention (MHA)**, right, which was of the order of **sequence squared**, right?

So we can do **Selective Checkpointing** where we do not store checkpoints for the **Multi-Headed Attention**. We save the checkpoints‚Äîthe activations for **MLP**. So that gives us a reduction of up to 70% memory reduction and only 2.7% extra computational cost, right? So this is the first tool that we have used to make it work, right? So before this, it was not working; it was breaking. This at least, you know, there is a scope that it will work even though for an 8 billion model, batch size one with, you know, so the activation memory with, let's say, for 4,096 without any **recomputation** it's 97 GB. With selective activation, we are able to reduce it to 17 GB, right? And with full activations, because we are not storing anything, right, we are storing only at the end of every layer (16, whatever, 16 or 28 layers that are there), so it reduces to one GB, right?

So right. So this is what we are able to gain by a very simple trick of **gradient recomputation** and and by default in most of the libraries, typically you would have this as **on** unless you are training a very small model. Yeah. Yeah. Yeah. Yeah. Right.

Okay. So let's look at a 1 billion model. So here, notice that the model itself is not fitting within the memory. Right. It's it's roughly 100 GB. Right. The the the non-negotiable weights, gradients, and optimizer states. So we can't we can't do anything here. Let's look at a 1 billion model. And here again, even with 8,192 sequence length, right, earlier look at the activation, it was blowing up even with 4,096 it was blowing up, right? With selective activations, at least we are able to tame it. And and now proceed with one batch size or right. One batch size is taking up 25 GB memory. If you have till 80 GB, we can still now increase the batch size, right?

-----

### **üîÑ Solution 2: Gradient Accumulation**

Okay. Now that we are able to, you know, start the engine, at least do one forward pass, backward pass with one batch size, what can we do to increase the batch size? So my objective is that I want to accumulate the gradients for, let's say, 100 sequences of length 4,096, right? But I can't. Right? Let's say with two or four batch size, my memory gets hit. Let's focus on **the** 1 billion model only. Now, how can I compute the gradients for 100 sequences? What should I do? And and remember here that it's the activation memory which is becoming the culprit. These we are good for now, right? This is roughly 16 GB for a 1 billion model. Right?

So again, four sequences‚Äî**micro-batch size** equal to four‚Äîit fits in 80 GB, but I want my batch size to be 100. I want sequence length to be 100. How should I go about it? Can I do that first of all? Is it feasible? Can I do that? Can I get gradient from 100 sequences? **Yes.** **By processing them one by one/sequentially and adding the gradients.** Very good. This is precisely the answer I was looking for.

So what what he's suggesting is that different batches are independent of each other. I do not need activations from one batch for the second for computing the gradients of the second batch, right? So I can do them sequentially. I can take one micro batch. So this is called a **micro batch** now, right? So four becomes my micro batch, **MBS** **(Micro Batch Size)**, and this becomes my **BS (Full Batch Size)**, right? So for 100 batch size, if we want to compute gradients from 100 sequences, I compute gradients from four sequences at a time, right? Keep **storing** the gradients. Recall that we have gradients for each parameter, right? I just keep on storing the gradients. I get $\Delta W_1$ from the first micro batch. I save it. I do forward pass and backward pass again. Then the gradients will get accumulated. I will get‚Äîsorry, I'll get $\Delta W_2$ from the second micro batch, right? Up till, in this case, 25 micro batches, right? I can compute the gradient independently for each sequence. Does that make sense? Right. Very good. Very good.

So if you use batch size of four and do 25 steps, right? So let's say I start with $\theta_k$. After four batch size, I will change it to $\theta_1$ if I do `optimizer.step()`. I compute the gradients, I compute the‚Äîupdate the optimizer states, I update the parameters, right? Then so this first batch will come here. Let's let's call it $x_1$. Then second batch comes, $x_2$, and I compute $M(\theta_1, x_2)$ to get the loss, right? And then I update it to $\theta_2$. Here I computed $M(\theta_k, x_1)$.

Remember, if I compute the gradients from a smaller number of sequences, they are noisy. And now we are in a regime where we want to increase the batch size, right? So what I want is I want the **overall** gradient. So this will give me gradients, let's say, from uh uh let's use this notation: $\Delta\theta_0$ essentially means gradient at $\theta_k$ computed from $x_1$, right? And I will have $\theta_k + \alpha \times \Delta\theta_k(x_1)$. This is how I will update, right? This will be my $\theta_1$ roughly, right?

So here, now, can you answer your question: what is the difference? **The parameters are updated only once after all micro-batches.** Yes. Exactly. Exactly. So essentially, you know, what is happening is that you are starting from $\theta_k$. If you have, if you are doing with only four samples or four sequences at a time, you will move to $\theta_1$, and then you will move to $\theta_2$, move to $\theta_3$. These are from $x_1, x_2, x_3$. But what I want instead is that I want to move to some $\theta_1'$, and I want to compute the loss from all of these at $\theta_k$. That is the difference, right?

But we don't have to store **all the activations**. Right? We do not‚Äîyou're right. This is right and wrong both. We do not have memory to store the activations for 100, right? We may have memory for storing the gradients of the 100, even though we don't need to do that because we need to just add the gradients. Right? Gradients are additive. **The total gradient** if I have to compute gradient over $X_1$ and $X_2$, I compute gradient over $X_1$ and $X_2$ and add, right?

So I can accumulate. I will have one big memory allocated for the gradients. I compute, right? So let's see, uh here I do a forward pass and a backward pass, right? Compute the gradients and store it here. This is the memory I have for‚Äîthis is the memory that I have for gradients, right, in the GPU. I get another micro batch. I do forward and backward and add to the same set of gradients, right? And then again, I do a forward and backward pass. I again store in the same gradient. So I'm accumulating the gradients here, right?

So that is what is called **Gradient Accumulation** and update the optimization step once. What you were suggesting earlier, it was that you will have an optimization step here, and then optimization step here, and then an optimization step here. These will be noisy gradients computed from a much smaller batch, right?

-----

### **üöÄ Solution 3: Data Parallelism**

Okay. So we have looked at two ways of handling the memory blowup issue. One was **Gradient Checkpointing**‚Äî70% reduction with only 2‚Äì3% extra computation cost. Second was, you know, if you're not able to fit all in one time, you know, do **Gradient Accumulation**‚Äîrun them sequentially. At least you will get two **loops**, right?

Okay. Now this was all on one GPU, right? If I tell you that we have three GPUs, then, you know, can you do something better? **Yes, we can use Data Parallelism to process micro-batches simultaneously.** That's just something you could give the batches to the so that it not can‚Äîlike, how we are three **people here**?

So let's say I have three GPUs: GPU 0, GPU 1, and GPU 2, right? Let's say I keep‚ÄîI have the optimizer states. I have the model parameters there, right? So this is the optim state, and these are the master weights that I have, right? Model master weights. Let's say in the beginning, they are all in sync. They are same. All of the value of all of these is same, right? We have an exact replica of these, right?

So as both of you correctly pointed out, that these micro batches are independent of each other. I can do a forward pass. Take the first micro batch, do the forward pass here, right? And then the backward pass and have my gradients, right? And then here also, I can do the forward pass, backward pass, compute my gradients, and compute my gradients here. They are independent of each other. Let us assume for now that my my batch size is three times the micro batch size now, right? So I have these gradients: **Gradient 1 ($G_1$), Gradient 2 ($G_2$), Gradient 3 ($G_3$)**, right? I need to add them, right? So that that means we need to communicate these gradients with each other between the machines, right?

So you can have a space because, see, this is happening on the GPU memory. So I'm not sure how will you keep a common space. Yes, I understand you're talking about shared memory, and that typically happens in **a single-node setup**. Just keep one in GPU, and then when you **compute $G_2$ and $G_3$,** just add it to **$G_1$.**

Right. So you're saying that we compute it there in a transient, some small memory, and directly send it to **the master GPU**. Yeah, we can do that. But let's now for now, let's simplify things here, right? Let's say because we know that, you know, we saw that we have space for optimizer, model, and gradients. We can say that the problem was with the activations, right? And earlier, we were doing it sequentially on one GPU because of the activation memory space that activations were taking. We are saying that, okay, let's have these activations on three different GPUs for three different micro-batches and compute the gradients. Let's assume for now for simplicity, we'll get to that as well. Let's assume that we have the space for gradients. These $G_1, G_2,$ and $G_3$‚Äîthese are the gradients computed of the same parameters computed using different data points, right? So we are doing **the** exactly same computation on each on each GPU but on different data points, right?

And we need to now communicate the gradients on all the machines, right? So what we need on each machine we need $(G_1 + G_2 + G_3)/3$, right? This is the‚Äîthis is the gradient, right? And we need this on on all machines. Once we have this, then we have the gradients, and then we can update the optimizer which will update the model weights, and then again, after the end of the cycle, everything is in sync with each other, right?

So now the question here is, how are we communicating the gradients with each other? Right? So right, so we have gone through all of this. Yeah, so instead of processing smaller micro-batches sequentially, we process them parallelly on GPU 0, GPU 1, GPU 2. We keep a replica of all three parameters, gradients, and optim states for simplicity now, right? Activations and gradients are computed locally and independently on each GPU. We communicate the gradients across GPUs and then independently update the optimizer states and parameters on each GPU. So now all the, you know, all the GPUs are in sync with each other with respect to the state of the training. Right?

Now the question is, how do we communicate between the gradients? Right? So think about it, and we will start from here tomorrow.
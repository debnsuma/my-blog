{"entries":[],"headings":["introduction","deep-learning-training-basics","bottlenecks-in-single-gpu-training","static-memory","dynamic-memory","batch-size-intuition","memory-usage-in-transformer","solution-1-activation-recomputation","solution-2-gradient-accumulation","scaling-with-multiple-gpus-data-parallelism-dp","the-data-parallel-setup","gradient-synchronization-the-all-reduce-primitive","overlapping-communication-and-computation","the-limitations-of-simple-data-parallelism-dp","zero-zero-redundancy-optimizer","zero-1-sharding-optimizer-states","zero-2-sharding-gradients","zero-3-sharding-parameters","introduction-to-ray---a-unified-ai-compute-engine","ray-core-primitives","an-example-for-stateless-tasks-tasks","going-further-for-stateful-tasks-actors","ray-for-different-ai-workloads","distributed-training-with-ray-train-and-pytorch","single-gpu-pytorch-training-on-cifar-10","dataloader-function","training-function","training-the-model","distributed-training-with-ray-train","ray-train-architecture","ray-data-ray-train-integration","setup-the-environment","distributed-training-with-ray-train-and-pytorch-fsdp","specify-cluster-scaling","data-preparation-pytorch-dataloaders","define-the-training-function","configure-run-checkpointing-and-storage","launch-training-with-torchtrainer","load-and-use-checkpoints","conclusion","references-further-resources"]}
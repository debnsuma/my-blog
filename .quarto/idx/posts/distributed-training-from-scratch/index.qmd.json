{"title":"Distributed Training From Scratch","markdown":{"yaml":{"title":"Distributed Training From Scratch","author":"Suman Debnath","date":"2025-11-30","categories":["distributed-training","deep-learning","ray","pytorch"],"image":"assets/ray.jpg","toc":true,"description":"A comprehensive guide to understanding and implementing distributed training for deep learning models from first principles"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\nDeep learning models are becoming massive, and so are the challenges of training them. I am quite new to distributed training, and at present I am trying to learn how to scale the training to multiple GPUs and multiple machines using different forms of parallelism. \n\nAnd in the process, I found that more than the training or the algorithm itself, it is about understanding how distributed systems work and how to manage resources (GPUs and CPUs) efficiently.\n\n<p align=\"center\">\n  <img src=\"assets/model_params.png\" alt=\"Model Parameters vs Training Time\" width=\"100%\"/>\n</p>\n\nAs models grow in size and complexity, the time required to train them is skyrocketing, sometimes taking days, weeks, or even months for a single epoch on one GPU. \n\n\n| Model            | Parameters<br/>(Millions) | Training Time on A100<br/>(GPU Hours) |\n|------------------|:------------------------:|:-----------------------------:|\n| **ResNet-50**    | 26                       | 31                            |\n| **ResNet-101**   | 45                       | 44                            |\n| **BERT-Base**    | 108                      | 84                            |\n| **Turing-NLG 17B** | 17,000                 | TBA                           |\n| **GPT-3 175B**   | 175,000                  | 3,100,000                     |\n\n\nIf we look at this table, we can see the dramatic growth in model parameter counts and the corresponding jump in training time requirements:\n\n- **ResNet-50 and ResNet-101** are manageable on a single GPU, but BERT-Base is already pushing the envelope.\n- **Turing-NLG 17B** and especially **GPT-3 175B** are in a completely different league, requiring immense computing power and time.\n\n::: {.callout-note}\nIf we tried to train GPT-3 on a single GPU, it would take roughly **355 years** to finish. Distributed training is not just useful, it's absolutely essential as model sizes and training time requirements soar\n:::\n\nBut why is this happening? Let's set the stage with some striking observations:\n\n- Model sizes and GPU demand are exploding.\n- Training hours required can reach into the millions for cutting-edge models.\n\nConsider the `LLaMA` family of models. The graph below shows that as you scale up model size (circle diameter), performance increases but so does the training time. Look at the y-axis: we're talking *millions* of GPU hours! Training even one of these models on a single GPU isn't just slow, it's practically impossible.\n\n<p align=\"center\">\n  <img src=\"assets/model-size-vs-performance.png\" alt=\"Model Size vs Performance\" width=\"70%\"/>\n</p>\n\nIn this blog post, we'll try to understand distributed training from first principles and how to scale the training to multiple GPUs and multiple machines using different forms of parallelism. We will also look at how to implement these techniques from scratch using `PyTorch` and later on we will use `Ray` to scale the training.\n\nAs I mentioned earlier, I am quite new to distributed training, and at present I'm still in `epoch` 1 of my journey :). \n\nBefore I get started I would like to thank few of the brilliant researchers, professors, like **Prof Tanmoy Chakraborty**, **Dr Yatin Nandwani** and **Prof. Song Han**. My mentor/teacher **Rohan Shravan**, for his exceptional teaching, guidance and coaching over the years. And also to few of my colleagues and friends like **Dipankar Ranjan Baisya**, **Chris Fregly**, **Zachary Mueller**, **Ram Mohan**, **Debanjan Saha** and **Siddhant Gupta** who have made this journey possible for me. \n\nMost of the content in this blog post is based on the work and lectures of these brilliant researchers, professors, and my mentors/teachers. You can find all the references and further resources at the end of the blog post.\n\n## Deep Learning Training Basics\n\nBefore diving into scaling, let's quickly review the standard model training loop, such as a simple **Multi-Layer Perceptron (MLP)**:\n\n::: {.panel-tabset}\n## Pseudo Code for the Training Loop\n\n```python\n1  model = MLP().to(device)\n2  optimizer = Adam(model.parameters())\n3  criterion = CrossEntropyLoss()\n4  data_loader = DataLoader(dataset)\n5  \n6  for epoch in range(num_epochs):\n7      model.train()  \n8      for inputs, targets in data_loader:\n9          # 1. Move batch to GPU\n10         inputs, targets = inputs.to(device), targets.to(device)\n11         \n12         # 2. Clear gradients\n13         optimizer.zero_grad()\n14         \n15         # 3. Forward pass\n16         outputs = model(inputs)\n17         loss = criterion(outputs, targets)\n18         \n19         # 4. Backpropagation\n20         loss.backward()\n21         \n22         # 5. Optimization\n23         optimizer.step()\n```\n:::\n\n\nIn a typical training loop, after defining the model, the optimizer, the loss function, and the data loader, the training loop trains the model by performing the following steps for each epoch: \n\n1. Iterate over the data in mini-batches (`line 6-8`).\n2. Move each batch of data to the GPU (`line 9-10`).\n3. Zero out any previous gradients (`line 12-13`).\n4. Perform a forward pass to calculate the model outputs and loss (`line 15-17`).\n5. Compute gradients through backpropagation (`line 18-19`).\n6. Update the model parameters using the optimizer (`line 20-21`).\n\n<p align=\"center\">\n  <img src=\"assets/single-gpu.png\" alt=\"Single GPU Training Loop\" width=\"70%\"/>\n</p>\n\nThis pattern is the core of most deep learning training routines.\n\n## Bottlenecks in Single-GPU Training\n\nWhen training deep learning models on a single GPU, high-bandwidth memory (HBM) is utilized by four main types of data:\n\n1. **Model Parameters** ($\\Phi$):  \n   The weights being learned during training.\n\n2. **Parameter Gradients** ($\\nabla \\Phi$):  \n   The gradients computed during backpropagation, required for parameter updates.\n\n3. **Optimizer States** ($\\Phi_{\\text{optim}}$):  \n   Auxiliary variables needed by the optimization algorithm, such as momentum and variance estimates (e.g., in Adam).\n\n4. **Activations** ($\\mathcal{M}_{\\text{act}}$):  \n   The intermediate outputs from each neural network layer required to compute gradients during the backward pass.\n\nOf these, the first three (**Parameters**, **Gradients**, and **Optimizer States**) are considered **static components**. They collectively define the minimum \"static\" memory footprint determined by the model architecture itself.\n\nThe fourth component, **Activations**, is **dynamic** and its memory footprint depends on the input size (such as batch size and sequence length). Thus, activations often become the main bottleneck in large-scale training.\n\n### Static Memory \n\nAnd when we are training if you look at the training loop again, until step `optimizer.step()`, we need to keep everything in the memory. And after `optimizer.step()`, we can discard the activations and the gradients. And we can keep the model parameters and the optimizer states in the memory.\n\nIf $\\Psi$ is the total number of parameters in the model, the total static memory required ($\\mathcal{M}_{static}$) using the Adam optimizer is a fixed amount: **$16\\Psi$ bytes**. \n\n| Component | Precision | Size ($\\Psi$ Bytes) | Rationale |\n|:---|:---|:---|:---|\n| Model Parameters | BF32 (4 bytes) | $4\\Psi$ | Used for forward and backward passes |\n| Parameter Gradients | BF32 (4 bytes) | $4\\Psi$ | Used in backpropagation |\n| Optimizer States (Adam) | FP32 (4+4 bytes) | $8\\Psi$ | Stores 1st and 2nd moment estimates ($4\\Psi$ each) |\n| **Total Static Memory** |  | **$16\\Psi$** | **The absolute floor for static storage** |\n\n::: {.callout-note title=\"Why Adam Optimizer Uses $4+4$ Bytes per Parameter\"}\nAdam maintains two additional FP32 (4-byte) tensors per parameter: the **first moment** (mean of gradients, $m$) and the **second moment** (uncentered variance, $v$). Thus, for each parameter, Adam stores $4$ bytes for $m$ and $4$ bytes for $v$, totaling $8\\Psi$ bytes.\n:::\n\nAnd when it comes to training a model, its all about how smartly we can manage this memory footprint. In modern LLM training, **mixed precision** is employed, typically using BF16 (2 bytes) for fast computation while maintaining a full-precision FP32 (4 bytes) copy of weights and optimizer states for numerical stability.\n\n::: {.callout-tip title=\"Mixed Precision Training\"}\n**Mixed Precision Training** accelerates deep learning and reduces memory use by combining 16-bit (BF16/FP16) and 32-bit (FP32) floating-point operations.\n\n- *How it works*: Forward and backward passes use low-precision (e.g., BF16) for parameters and activations, while a full-precision FP32 `master` copy of weights and optimizer states is kept for numerical stability.\n- *Why it matters*: Enables training of larger models or larger batches within the same hardware footprint. Mixed precision is now standard in large-scale model training.\n:::\n\nSo, with mixed precision, the breakdown is as follows:\n\n| Component | Precision | Size ($\\Psi$ Bytes) | Rationale |\n|:---|:---|:---|:---|\n| Model Parameters | BF16 (2 bytes) | $2\\Psi$ | Used for forward and backward passes |\n| Parameter Gradients | BF16 (2 bytes) | $2\\Psi$ | Used in backpropagation |\n| Master Weights | FP32 (4 bytes) | $4\\Psi$ | Full precision copy for the update step |\n| Optimizer States (Adam) | FP32 (4+4 bytes) | $8\\Psi$ | Stores 1st and 2nd moment estimates ($4\\Psi$ each) |\n| **Total Static Memory (with Mixed Precision)** |  | **$16\\Psi$** | **The absolute floor for static storage** |\n\nYou might notice that the total static memory remains **$16\\Psi$** bytes. So what is the advantage of mixed precision training?\n\nThe key benefits of mixed precision are:\n\n- **Increased Training Speed:** As we are using lower-precision data types (like BF16) during forward and backward passes, computation is faster and less memory bandwidth is used.\n- **Reduced Activation Memory:** Since our model parameters and optimizer states are stored in FP32, the activations, which are stored in BF16 during training, require half the memory compared to FP32, so the dynamic (activation) memory footprint is significantly lower.\n\nWhile the absolute static memory is unchanged, mixed precision allows for faster training and greater memory efficiency, especially for storing activations, enabling larger models or batches to fit within the same hardware limits.\n\nAnd this calculation reveals a significant challenge: a **70 Billion parameter model** requires approximately $70\\text{B} \\times 16 \\text{ bytes} \\approx 1120 \\text{ GB}$ of static memory. With high-end GPUs typically offering only $80 \\text{ GB}$ of memory, loading even the static state of the model becomes impossible, without considering the dynamic activations.\n\n### Dynamic Memory\n\nThis component is dependent on the input batch and is the primary cause of memory bottlenecks.\n\n* **Activations:** The output of each layer. They must be stored until the **backward pass** to compute the gradients.\n    * For a linear layer $y=Wx$, the gradient for $W$ is calculated as:\n        $$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot x^T$$\n    * This requires saving the layer's input, $x$ (the activation from the previous layer).\n\n* **Activation Memory Equation:** The total memory required for activations ($m_{act}$) in mixed precision can be estimated by the following equation:\n    $$\\mathcal{M}_{\\text{act}} = L \\cdot \\text{seq} \\cdot \\text{bs} \\cdot h \\cdot \\left(34 + \\frac{5 \\cdot n_{heads} \\cdot \\text{seq}}{\\text{h}}\\right)$$\n\n    Where:\n\n    - $L$: Number of layers\n    - $\\text{seq}$: Sequence length\n    - $\\text{bs}$: Batch size (number of samples)\n    - $h$: Hidden dimension of the model\n    - $n_{heads}$: Number of attention heads\n\n<p align=\"center\">\n  <img src=\"assets/activations.png\" alt=\"Activation Count\" width=\"70%\"/>\n</p>\n\nAs we can see, activation memory usage is **not static** for a given model; it scales:\n\n- **Linearly** with the **batch size** ($\\text{bs}$)\n- **Quadratically** with the **sequence length** ($\\text{seq}$)\n\nAnd this quadratic scaling with $\\text{seq}^2$ (an effect stemming from the attention matrix) means the activation memory is the part that will **blow up** when you increase the batch size or train with longer sequences.\n\n## Batch Size Intuition\nSo, as we can see that the longer the sequence, the more activations we need and hence the more memory we need. So, even for one single sequence, the memory required is more than 50 GB. And this is a significant challenge for training large models. \n\n::: {.callout-note title=\"Global Batch Size\"}\nWhen we talk about batch size in LLM pre-training, we usually refer to it in terms of the **number of tokens**, not the number of sequences. That is, the token batch size is calculated as **sequence length × number of sequences (micro-batch size)**\n:::\n\nTypically, the global batch size used in pre-training is extremely large—usually in the **millions of tokens**. \nAnd in practice, we start the training with a smaller batch size and gradually increase it over the course of training.\n\n* **Small Batch Size:** Used at the **beginning of training** when the loss is high. It provides **quick, noisy signals** that help the model traverse the loss landscape rapidly toward the minima.\n* **Large Batch Size:** Used as training approaches the optima. It provides a **more accurate gradient direction** (a clearer signal), which reduces noise and ensures confident, stable convergence.\n\n<p align=\"center\">\n  <img src=\"assets/batch-size.png\" alt=\"Batch Size Intuition\" width=\"100%\"/>\n</p>\n\n## Memory usage in Transformer\n\nTo get a sense of the memory usage in a Transformer, let's take a look at the memory usage of `Llama 3.1` {`8B`, `70B` and `13B`} models. \n\n<p align=\"center\">\n  <img src=\"assets/llama-3-1.png\" alt=\"Batch Size Intuition\" width=\"100%\"/>\n</p>\n\nFrom this graph, we can clearly see that forshort sequences (or small batch sizes), memory usage for activations is almost negligible, but from around 4K-16K tokens they start to take up a significant amount of memory (this is because of the quadratic scaling with the sequence length, which we discussed earlier), while usage for parameters, gradients, and optimizer states is roughly independent of the sequence length and batch size.\n\nHow can we solve this problem of `activation explosion`? Can we somehow avoide storing all those activations ? \n\n### Solution 1: Activation Recomputation\n\nRecall why we need to store all those activations in the first place ? It is because we need to compute the gradients for the model parameters. So, if we can somehow avoid storing all those activations, we can save a lot of memory.\n\nOne effective approach is **Gradient Checkpointing** also known as **Activation Recomputation**. With this technique, we discard most of the activations during the forward pass to save memory and recompute them on the fly during the backward pass when gradients are needed. \n\nNormally, we’d store every hidden state between learnable operations (like feedforward layers, layer norm, etc.) to use them during the backward pass. With activation recomputation, we only store activations at specific checkpoints and recalculate everything else during backpropagation. This helps us manage memory while training large models. The process typically looks like this:\n\n<p align=\"center\">\n  <img src=\"assets/gradient-accumulation-1.png\" alt=\"Gradient Accumulation\" width=\"70%\"/>\n</p>\n\nBut as we know in life, there is no free lunch. Although we save memory by discarding most of the activations during the forward pass, we spend some extra compute to recompute these activations on the fly during the backward pass.\n\nThere are a few ways to do activation checkpointing, and each involves different memory and compute tradeoffs. \n\nThe most aggressive approach is called **Full Activation Checkpointing**, where you only store activations at the end of each layer (instead of storing every intermediate activation). This method is great for memory since you’re keeping so little, but it’s the most compute-heavy, often increasing computation time by 30–40% because you have to recompute almost everything during backpropagation.\n\nBut do we really need to treat every part of the model the same? By profiling, we find that the main memory culprit is the activations from the **Multi-Headed Attention (MHA)** layers, since they scale quadratically with sequence length.\n\nThis leads to a more balanced strategy: **Selective Checkpointing**. Here, we only skip storing activations for the heavy MHA layers and still store them for the lighter MLP layers. The payoff is impressive: up to 70% memory savings for only about 2.7% extra computation. \n\n::: {.callout-note title=\"Activation Checkpointing on Llama 3.1 8B model\"}\nAs you can see the graph bellow, on an 8-billion parameter model with a batch size of 1 and sequence length 4096, activation memory without any checkpointing can hit 97 GB, which is enough to break most GPUs. With selective activation checkpointing, that drops to 17 GB. And with full checkpointing, at the extreme, memory usage can go down to just 1 GB!\n:::\n\n<p align=\"center\">\n  <img src=\"assets/llama-checkp.png\" alt=\"Gradient Accumulation\" width=\"100%\"/>\n</p>\n\nNow that we’ve learned about recomputation, we can tame the activation memory usage we saw in the previous graphs!\n\nHowever, activations still have a linear dependence on the batch size, so as we move to larger batch sizes this might become an issue again. So, what can we do to increase the batch size ? And to takle this we have the next trick in our box - **gradient accumulation**, lets discuss that next. \n\n\n### Solution 2: Gradient Accumulation\n\n**Gradient Accumulation** is a technique that allows us to accumulate gradients over multiple micro-batches before performing a single global optimization step. This is particularly useful when we have a large batch size and we want to avoid running out of memory.\n\nThe general idea is to split the batch into smaller micro-batches (let's say 3) and process them one by one. We compute the gradients for each micro-batch and accumulate them (we **do not do** `optimizer.step()` after each micro-batch). And after processing all the micro-batches, **we perform a single** global optimization step.\n\n<p align=\"center\">\n  <img src=\"assets/grad-acc.png\" alt=\"Gradient Accumulation\" width=\"100%\"/>\n</p>\n\nLet's take an example of a simple linear regression model:\n\nLet's use a different analogy: predicting the **score of a student on a test** based on two factors—the number of hours studied ($x_1$) and the number of hours slept the night before ($x_2$). We assume a simple linear relationship between these inputs and the output score:\n\n$$\n\\text{score}_{pred} = x_1 w_1 + x_2 w_2 + b\n$$\n\nOur aim is to use stochastic gradient descent to determine the best values for $w_1$, $w_2$, and $b$ such that the mean squared error (MSE) between the actual score ($\\text{score}_{target}$) and the predicted score ($\\text{score}_{pred}$) is minimized:\n\n$$\n\\underset{w_1, w_2, b}{\\mathrm{argmin}} \\; (\\text{score}_{pred} - \\text{score}_{target})^2\n$$\n\nWithout gradient accumulation, we would update the parameters after each batch of student's data.\n\n::: {.panel-tabset}\n\n## Without Gradient Accumulation\n\n```python\n1  def train_no_accumulate(params: ModelParameters, \n2                         num_epochs: int = 10, \n3                         learning_rate: float = 1e-3):\n4      for epoch in range(1, num_epochs + 1):\n5          for (x1, x2), y_target in training_data:\n6  \n7              # Calculate the output of the model\n8              z1 = x1 * params.w1\n9              z2 = x2 * params.w2\n10             y_pred = z1 + z2 + params.b\n11             loss = (y_pred - y_target) ** 2\n12 \n13             # Calculate the gradients of the loss w.r.t. the parameters\n14             loss.backward()\n15 \n16             # Update the parameters (at each iteration)\n17             with torch.no_grad():\n18                 # Equivalent to calling optimizer.step()\n19                 params.w1 -= learning_rate * params.w1.grad\n20                 params.w2 -= learning_rate * params.w2.grad\n21                 params.b  -= learning_rate * params.b.grad\n22 \n23                 # Reset the gradients to zero\n24                 # Equivalent to calling optimizer.zero_grad()\n25                 params.w1.grad.zero_()\n26                 params.w2.grad.zero_()\n27                 params.b.grad.zero_()\n```\n\n:::\n\nWith **gradient accumulation**, instead of updating the parameters after each batch of data, we accumulate gradients across several micro-batches (`micro_batch_size = 3`) and then update all at once. This allows us to train with larger effective batch sizes even if memory is limited.\n\n::: {.panel-tabset}\n\n## With Gradient Accumulation\n```python\n1  def train_accumulate(params: ModelParameters, \n2                       num_epochs: int = 10, \n3                       learning_rate: float = 1e-3, \n4                       micro_batch_size: int = 3):\n5  \n6      for epoch in range(1, num_epochs + 1):\n7          for index, ((x1, x2), y_target) in enumerate(training_data):\n8  \n9              # Calculate the output of the model\n10             z1 = x1 * params.w1\n11             z2 = x2 * params.w2\n12             y_pred = z1 + z2 + params.b\n13             loss = (y_pred - y_target) ** 2\n14 \n15             # Accumulate gradients\n16             loss.backward()\n17 \n18             # If we have processed 3 micro-batches OR reached the end of the dataset\n19             if (index + 1) % micro_batch_size == 0 or index == len(training_data) - 1:\n20                 with torch.no_grad():\n21                     # Equivalent to optimizer.step()\n22                     params.w1 -= learning_rate * params.w1.grad\n23                     params.w2 -= learning_rate * params.w2.grad\n24                     params.b  -= learning_rate * params.b.grad\n25 \n26                     # Reset the gradients = optimizer.zero_grad()\n27                     params.w1.grad.zero_()\n28                     params.w2.grad.zero_()\n29                     params.b.grad.zero_()\n```\n:::\n\nGradient accumulation allows us to reduce activation memory, which grows linearly with batch size, by processing smaller micro-batches sequentially. This reduces stored activations and gradients since only one micro-batch's worth of activations needs to be kept in memory at a time, which helps reduce the overall activation memory footprint.\n\nAgain there is no free lunch. As gradient accumulation requires multiple consecutive forward/backward passes per optimization step, it increases the compute overhead and slows down training. But it allows us to train with larger effective batch sizes even if memory is limited. \n\nSo far, we've seen how techniques like `gradient checkpointing` and `gradient accumulation` help deal with the memory blowup issue caused by `activations`: the dynamic part of memory usage. Both allow us to fit larger models or batches on a **single GPU**, but mostly by working sequentially and slowing down training. However, these don't address the static memory required for parameters, gradients, and optimizer states, nor do they fully utilize available hardware (assume we have more than one GPU). \n\nTo tackle this, we can scale training across multiple GPUs using **Data Parallelism**. By splitting micro-batches and processing them simultaneously on several GPUs, we address both memory and compute bottlenecks, and that is what we will discuss in the next section.\n\n## Scaling with Multiple GPUs: Data Parallelism (DP)\n\nRecall that in Gradient Accumulation, we were processing **micro-batches (MBS)** sequentially. Since these micro-batches are **independent of each other**, we can process them **parallelly on different GPUs**. Something like this, if you see carefully now we are processing the micro-batches in parallel on different GPUs, w.r.t what we did in Gradient Accumulation where we were processing the micro-batches sequentially on a single GPU:\n\n<p align=\"center\">\n  <img src=\"assets/dp.png\" alt=\"Data Parallelism\" width=\"100%\"/>\n</p>\n\n### The Data Parallel Setup\n\nIn a Data Parallel setup, we distribute the data across multiple GPUs, while maintaining a full, redundant replica of the model parameters, gradients, and optimizer states on each GPU. \n\n1.  **Replication:** We maintain a full, redundant **replica** of the model parameters ($\\Phi$), gradients ($\\nabla \\Phi$), and optimizer states ($\\Phi_{\\text{optim}}$) on **each GPU**.\n\n<p align=\"center\">\n  <img src=\"assets/dp_1.png\" alt=\"Data Parallel Setup\" width=\"100%\"/>\n</p>\n\n2.  **Parallel Processing:** Each GPU processes a unique micro-batch simultaneously. This involves **same operations, different data**.\n\n<p align=\"center\">\n  <img src=\"assets/dp_2.png\" alt=\"Data Parallel Setup\" width=\"100%\"/>\n</p>\n\n3.  **Local Computation:** Each GPU performs its forward pass and backward pass locally and independently, resulting in a local gradient ($\\nabla \\Phi_i$).\n\n<p align=\"center\">\n  <img src=\"assets/dp_3.png\" alt=\"Data Parallel Setup\" width=\"100%\"/>\n</p>\n\nIf you look carefully, we can perform the forward pass and the backward pass in parallel on different GPUs. But we cannot perform the optimizer step and update the parameters independently on different GPUs. If we do that, we will end up training N different models on N different GPUs which is not what we want. \n\nSo, after the backward pass, we need to somehow synchronize the gradients across the GPUs. And this is done by the **All-Reduce** primitive. \n\n### Gradient Synchronization: The All-Reduce Primitive\n\nBefore we dive into the All-Reduce operation, it's important to note that NVIDIA provides a rich set of **communication primitives** as part of its distributed training ecosystem such as NCCL(NVIDIA's collective communication library). These primitives simplify and accelerate multi-GPU (and multi-node) communication, enabling efficient synchronization and sharding operations required for large-scale training.\n\n::: {.callout-note title=\"Communication Primitives in Distributed Training\"}\n**All-Reduce** is just one such primitive—used specifically for synchronizing gradients across GPUs at the end of each backward pass in standard Data Parallel training. However, there are several other primitives (like **All-Gather**, **Reduce-Scatter**, **Broadcast**, etc.) designed for different patterns of communication and parallelism. We will encounter and discuss these additional primitives as we explore more advanced parallelization techniques (e.g., ZeRO, model sharding, tensor parallelism) later in the series.\n::: \n\nFor now, let's look at **All-Reduce** in detail, since this is exactly what we need for synchronizing the gradients during Data Parallel training.\n\nSince each GPU computes a gradient based only on its local micro-batch, we must **add them to get the global gradient** before performing the optimization step. The required communication operation is the **All-Reduce** primitive:\n\n* **Input:** Different tensors (the local gradients $\\nabla \\Phi_1, \\nabla \\Phi_2, \\dots$) on each GPU.\n* **Operation:** A reduction operation (usually summation, $F$) is applied to all tensors.\n* **Output:** The result of the reduction (the global gradient $\\sum \\nabla \\Phi_i$) is made available on **all** GPUs.\n\n<p align=\"center\">\n  <img src=\"assets/all_reduce.png\" alt=\"All Reduce\" width=\"70%\"/>\n</p>\n\nOnce every node receives the global gradient, it performs the **`optimizer.step()`** operation independently, ensuring all model copies remain in sync. These collective operations are defined in the **`torch.distributed`** API.\n\nHere I've a machine with 4 T4 GPUs.\n\n```bash\nray@ip-10-0-69-225:code$ nvidia-smi -L\nGPU 0: Tesla T4 (UUID: GPU-31a1b562-c769-c7f1-ede1-48847cec8d53)\nGPU 1: Tesla T4 (UUID: GPU-1beaf204-f6f7-182d-67f8-aee6c58128df)\nGPU 2: Tesla T4 (UUID: GPU-934ca246-df7e-2c7f-4bdd-b07859e46b2d)\nGPU 3: Tesla T4 (UUID: GPU-141171cb-db62-b770-97ff-955f8c7f2265)\n```\n\nNow let's create a simple example to demonstrate the **All-Reduce** operation by creating 4 tensors on each GPU and performing the **All-Reduce** operation on them.\n\n::: {.panel-tabset}\n\n## All-Reduce Example\n```python\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_all_reduce():\n    tensor = torch.tensor([dist.get_rank() + 1] * 3, dtype=torch.float32).cuda()\n    print(f\"Before all_reduce on rank {dist.get_rank()}: {tensor}\")\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    print(f\"After all_reduce on rank {dist.get_rank()}: {tensor}\")\n\n# Initialize the process group and set the device, create a tensor on each GPU and perform the All-Reduce operation on them.\ninit_process()\nexample_all_reduce()\n```\n:::\n\nWe can run this code on 4 GPUs using the following command:\n\n```bash\ntorchrun --nproc_per_node=4 dist_all_reduce.py\n```\n\nWe will get the following output:\n\n```bash\nBefore all_reduce on rank 3: tensor([4., 4., 4.], device='cuda:3')\nBefore all_reduce on rank 0: tensor([1., 1., 1.], device='cuda:0')\nBefore all_reduce on rank 2: tensor([3., 3., 3.], device='cuda:2')\nBefore all_reduce on rank 1: tensor([2., 2., 2.], device='cuda:1')\n\nAfter all_reduce on rank 3: tensor([10., 10., 10.], device='cuda:3') \nAfter all_reduce on rank 0: tensor([10., 10., 10.], device='cuda:0') \nAfter all_reduce on rank 2: tensor([10., 10., 10.], device='cuda:2') \nAfter all_reduce on rank 1: tensor([10., 10., 10.], device='cuda:1') \n```\n\n### Overlapping Communication and Computation\n\nIn a naive DP implementation, the GPUs sit **idle** during the communication phase, as the **All-Reduce** operation begins only after **all** gradients are computed in the backward pass. This is inefficient.\n\n<p align=\"center\">\n  <img src=\"assets/all_reduce.gif\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\nTo eliminate this idle time, we **overlap** the communication and computation.\n\n* **Method:** As soon as the gradient for a specific layer is computed during the backward pass (e.g., $\\nabla L_2$), we immediately trigger the **All-Reduce** for that gradient **in the background**.\n* **Rationale:** The computation of the next layer's gradient ($\\nabla L_1$) is independent of the communication of the previous layer's gradient ($\\nabla L_2$).\n* **Implementation:** This technique is implemented via **hooks** in PyTorch (like `post_accumulate_grad_hook`), allowing the next computation step to proceed while the communication step runs concurrently, significantly improving throughput. It attach an all-reduce hook function to each parameter that requires gradients. So, now it communicates more frequently but in smaller packets.\n\n::: {.panel-tabset}\n\n## Overlapping Communication and Computation\n```python\ndef register_backward_hook(self, hook):\n    \"\"\"\n    Registers a backward hook for all parameters of the model that \n    require gradients.\n    \"\"\"\n    for p in self.module.parameters():\n        if p.requires_grad is True:\n            p.register_post_accumulate_grad_hook(hook)\n```\n:::\n\nBefore while communication was happening, we were waiting for all the gradients to be computed in the backward pass. \n\n<p align=\"center\">\n  <img src=\"assets/dp_overlap1.png\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\nBut now, we are overlapping the communication and computation. So, we are not waiting for all the gradients to be computed in the backward pass. We are computing the gradients for the next layer while the communication for the previous layer is happening.\n\n<p align=\"center\">\n  <img src=\"assets/dp_overlap2.png\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\nWe can infact do better and communicate more efficiently by grouping the gradients into larger buckets and performing the All-Reduce operation on them. Its like packing items into boxes before shipping (have you seen at times while placing an order on Amazon, they offer us to pack multiple items into a single box to save on shipping costs ?, well thats what we are doing here but with gradients).\n\nWith this we can significantly reduce the communication overhead and speed up the computation operations.\n\n<p align=\"center\">\n  <img src=\"assets/dp_overlap3.png\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\n## The Limitations of Simple Data Parallelism (DP)\n\nNow, that we have seen how to scale out the training with multiple GPUs using Data Parallelism, we can ask ourselves a question - is this scaling **lossless** ? \n\nThe answer is no. There's a communication overhead associated with the Data Parallelism. And as the degree of data parallelism increases, there's a noticeable drop in tokens per second per GPU (throughput). Although we are overlapping the communication and computation, we are still waiting for the gradients to be computed in the backward pass.\n\nMore importantly, all these discussions about Data Parallelism (DP) so far have assumed that the **entire model** can fit on a single GPU. But what if the model is too large (e.g. GPT-3 with 175B parameters) to fit in the memory of a single GPU (NVIDIA A100 with 80GB of memory)? \n\n<p align=\"center\">\n  <img src=\"assets/model_hw.png\" alt=\"Model Exceeds GPU Memory\" width=\"50%\"/>\n</p>\n\nAs model sizes grow, it becomes common that a single accelerator (GPU in our case) cannot contain all model parameters, optimizer states, and gradients. Therefore, we need to find additional ways to scale training beyond simple DP, which can allow us to train models that **don't fit on a single GPU**.\n\nAnd that is what we will discuss in the next section - **ZeRO** (Zero Redundancy Optimizer).\n\n# ZeRO: Zero Redundancy Optimizer\n\nZeRO (Zero Redundancy Optimizer) is a family of techniques that addresses constrain of static memory (parameters, gradients, optimizer states) on a single GPU. With ZeRO, we can train models that don't fit on a single GPU, and it does that by sharding the static memory components across multiple GPUs.\n\nThis approach is organized into three possible optimization stages:\n\n- **ZeRO-1**: optimizer state sharding\n- **ZeRO-2**: optimizer state + gradient sharding\n- **ZeRO-3**: optimizer state + gradient + parameter sharding\n\n<p align=\"center\">\n  <img src=\"assets/zero.png\" alt=\"ZeRO\" width=\"100%\"/>\n</p>\n\nWithout even going further, you can probably guess that, with this approch we need to do a lot of communication between the GPUs. But as we have seen in the previous section, we can overlap the communication and computation to some extent. So, we can reduce the communication overhead by overlapping the communication and computation.\n\nLet's discuss each of these techniques in detail, starting with **ZeRO-1**.\n\n### ZeRO-1: Sharding Optimizer States\n\nRecall from our earlier discussion, the **static memory footprint** per GPU specifically for **mixed precision (using BF16 + FP32)**:\n\n| Component | Precision | Size ($\\Psi$ Bytes) | Rationale |\n|:---|:---|:---|:---|\n| Model Parameters | BF16 (2 bytes) | $2\\Psi$ | Used for forward and backward passes |\n| Parameter Gradients | BF16 (2 bytes) | $2\\Psi$ | Used in backpropagation |\n| Master Weights | FP32 (4 bytes) | $4\\Psi$ | Full precision copy for the update step |\n| Optimizer States (Adam) | FP32 (4+4 bytes) | $8\\Psi$ | Stores 1st and 2nd moment estimates ($4\\Psi$ each) |\n| **Total Static Memory (with Mixed Precision)** |  | **$16\\Psi$** | **The absolute floor for static storage** |\n\nThe largest part of the static memory comes from the **optimizer states**, especially for optimizers like Adam, which maintain both first and second moment statistics. **With Data Parallelism (DP), all these components are duplicated on every GPU** in the data-parallel group, so each device bears the full cost ($16\\Psi$) of these tensors (ignoring activations for now).\n\n::: {.callout-note title=\"Important Caveat: Master Weights Are Part of Optimizer State Sharding\"}\nWhen discussing the sharding of optimizer states in ZeRO, we must also include the **master weights** (the FP32 copy of model parameters used for the optimizer update) in the sharding calculation. Both the optimizer states and these master weights are stored in FP32, and both are sharded together in ZeRO-1. Thus, when you see references to \"optimizer state sharding,\" this always implicitly includes master weights in modern mixed precision training setups.\n:::\n\nWith **ZeRO-1**, the goal is to _shard_ (that is, partition and spread) the **FP32 optimizer states** and **FP32 master weights** across the $N_d$ GPUs, rather than storing them fully on each device. This introduces the following changes:\n\n- Every GPU stores only a **$1/N_d$ -th slice** of the optimizer states ($8\\Psi$) and master weights ($4\\Psi$), rather than the full $12\\Psi$.\n- The parameter tensors and gradients (in BF16) **remain fully replicated** on each GPU for compatibility with forward and backward passes.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1.png\" alt=\"ZeRO-1\" width=\"100%\"/>\n</p>\n\nSo, **ZeRO-1 directly reduces redundant memory used by both FP32 optimizer states and master weights**. The per-GPU memory for these states drops from $12\\Psi$ down to $\\frac{12\\Psi}{N_d}$. The other (BF16) tensors remain replicated for performance and simplicity.\n\nThe resulting **static memory footprint per GPU** with ZeRO-1 sharding thus becomes:\n$$\n\\mathcal{M}_{\\text{ZeRO-1}} = 2\\Psi + 2\\Psi + \\frac{12\\Psi}{N_d}\n$$\n\n- Parameters (BF16): $2\\Psi$\n- Gradients (BF16): $2\\Psi$\n- Optimizer States + Master Weights (FP32): $\\frac{12\\Psi}{N_d}$\n\nWhere the first two terms represent the fully replicated BF16 weights and gradients, and the last term is the optimizer states and FP32 master weights sharded across $N_d$ GPUs.\n\nJust to make this more concrete, let's look at some practical numbers. \nSuppose you have a modern **A100/H100 GPU** with **80GB** of memory. In DP, the largest model you can fit is roughly:\n\n$$\n\\text{Max Parameters (DP)} = \\frac{80~\\text{GB}}{16~\\text{bytes per param}} \\approx 5~\\text{billion parameters}\n$$\n\nBut if you apply ZeRO-1 with **64 GPUs** ($N_d = 64$), the optimizer state and master weights are now only a small shard per GPU:\n\n- $\\frac{12}{64} \\approx 0.1875$ (so just 1.5GB of optimizer/master weights per GPU for a 5B model)\n- The effective static memory per parameter drops from 16 bytes (DP) to about **4.2 bytes** (ZeRO-1).\n\nSo now, the largest model you can train on that same 80GB GPU jumps to:\n\n$$\n\\text{Max Parameters (ZeRO-1, 64 GPUs)} = \\frac{80~\\text{GB}}{4.2~\\text{bytes per param}} \\approx 19~\\text{billion parameters}\n$$\n\nSo, its great that we can train a larger model on the same hardware, but we still need to discuss the communication overhead associated with this approach. \n\nFor the forward pass, we **don't** need to do any communication, as we have all the parameters in each GPU. \n\n<p align=\"center\">\n  <img src=\"assets/zero-1_1.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nNext in the backward pass, we have all the gradients in each GPU. So, we need to do a **All-Reduce** and at this point we have the same gradients on all the GPUs.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1_2.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nBut now on each GPU, we can _discard_ all the other gradients and _keep only the one_ whose corresponding optimizer state is present on that particular GPU. \n\n<p align=\"center\">\n  <img src=\"assets/zero-1_3.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nAfter this, each GPU can _update_ its respective model parameters to its corresponding optimizer state and gradients.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1_4.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nAnd at this point, we need to communicate again, to get the updated model parameters on all the GPUs, as at this point each GPU has the updated model parameters only for its own shard of the optimizer state.\n\nBut what type of communication do we need to do ? \n\nAs each GPU need to _gather_ the updated model parameters from all the other GPUs. So, we need to do a **All-Gather** operation. And this is another communication primitive like **All-Reduce** which we have seen earlier.\n\n<p align=\"center\">\n  <img src=\"assets/all_gather.png\" alt=\"All Gather\" width=\"70%\"/>\n</p>\n\nLet's quickly see an example of how to do this using `torch.distributed.all_gather()`. Here we are creating a tensor on each GPU and performing the **All-Gather** operation on them.\n\n::: {.panel-tabset}\n\n## All-Gather Example\n```python\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_all_gather():\n    tensor = torch.tensor([dist.get_rank() + 1] * 3, dtype=torch.float32).cuda()\n    # Prepare an output list of tensors for all_gather\n    world_size = dist.get_world_size()\n    gathered = [torch.zeros_like(tensor) for _ in range(world_size)]\n    print(f\"Before all_gather on rank {dist.get_rank()}: {tensor}\")\n    dist.all_gather(gathered, tensor)\n    print(f\"After all_gather on rank {dist.get_rank()}: {[t.cpu().tolist() for t in gathered]}\")\n\n# Initialize the process group and set the device, create a tensor on each GPU and perform the All-Gather operation on them.\ninit_process()\nexample_all_gather()\n```\n:::\n\nJust like before, we can run this code with 4 GPUs:\n\n```bash\ntorchrun --nproc_per_node=4 dist_all_gather.py\n```\n\nTypical output:\n\n```bash\nBefore all_gather on rank 2: tensor([3., 3., 3.], device='cuda:2')\nBefore all_gather on rank 0: tensor([1., 1., 1.], device='cuda:0')\nBefore all_gather on rank 1: tensor([2., 2., 2.], device='cuda:1')\nBefore all_gather on rank 3: tensor([4., 4., 4.], device='cuda:3')\n\nAfter all_gather on rank 0: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\nAfter all_gather on rank 1: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\nAfter all_gather on rank 2: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\nAfter all_gather on rank 3: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\n```\n\nWith that, each GPU now has the updated model parameters and now they can start with the next batch and start the forward pass.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1_5.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nSo, this is how ZeRO-1 strategy works. \n\nNow, if go back and carefully look after _All_Reduce_ operation in ZeRO-1, each GPU discarded all the other gradients and kept only the one whose corresponding optimizer state is present on that particular GPU. \n\nWhich makes us think, why we need to keep all the gradients in all the GPUs in the first place ? Why cant shard the gradients as well along with its corresponding optimizer state. And this is exactly what ZeRO-2 strategy does. \n\n### ZeRO-2: Sharding Gradients\n\nLet's now walk through **ZeRO-2** using the same concrete, step-by-step approach.\n\nRecall our key question from ZeRO-1:\n\n> *After the All-Reduce, why keep all gradients on all GPUs? Can't each GPU just hold the gradients it needs?*\n\nThat's exactly what ZeRO-2 does, it further shards the gradients right alongside the optimizer state and master weights. So, each GPU now only needs to store the gradient shard corresponding to its optimizer state shard. \n\n<p align=\"center\">\n  <img src=\"assets/zero-2.png\" alt=\"ZeRO-2\" width=\"100%\"/>\n</p>\n\nJust as we did for ZeRO-1, let's run the numbers for ZeRO-2 sharding to see the dramatic benefits.\nWith ZeRO-2, the memory formula per GPU now becomes:\n$$\n\\mathcal{M}_{\\text{ZeRO-2}} = 2\\Psi + \\frac{2\\Psi + 12\\Psi}{N_d}\n$$\n\n- Parameters (BF16): $2\\Psi$\n- Gradients (BF16): $\\frac{2\\Psi}{N_d}$\n- Optimizer States + Master Weights (FP32): $\\frac{12\\Psi}{N_d}$\n\nIf we again use an **A100/H100 GPU** with **80GB** of memory, and $N_d = 64$ GPUs, then the largest model we can train would be:\n\n$$\n\\text{Max Parameters (ZeRO-2, 64 GPUs)} = \\frac{80~\\text{GB}}{2.2~\\text{bytes per param}} \\approx 36~\\text{billion parameters}\n$$\n\nLet's put this side-by-side:\n\n| Strategy     | Effective Bytes/Param | Max Model on 80GB GPU |\n|--------------|----------------------|-----------------------|\n| **DP**       | 16                   | ~5B                   |\n| **ZeRO-1**   | 4.2                  | ~19B                  |\n| **ZeRO-2**   | 2.2                  | ~36B                  |\n\nSo, **ZeRO-2 nearly doubles the maximum trainable model size compared to ZeRO-1** (and over 7x compared to vanilla Data Parallelism).\n\nLet's see how the communication overhead changes with ZeRO-2. For the forward pass, we **don't** need to do any communication (like in ZeRO-1), as we have all the parameters in each GPU.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_1.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nNext in the backward pass, instead of performing an **All-Reduce** over the gradients, we only perform a **Reduce-Scatter** operation. Another communication primitive like **All-Reduce** and **All-Gather** which we have seen earlier.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_2.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nSo what **Reduce-Scatter** operation does internally is, its first reducing (summing) the gradients across all the GPUs and then scattering the result to the GPUs that need to have the gradient shard.\n\n<p align=\"center\">\n  <img src=\"assets/reduce_scatter.png\" alt=\"Reduce Scatter\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"Computation-communication timeline\"}\n- **ZeRO-1**: We keep a copy of all gradients.\n- **ZeRO-2**: Communicate and release the gradients on the fly.\n- In practice, both use **`reduce-scatter`** for gradients and **`all-gather`** for FP32 copy of parameters.\n- There is no real overhead to using ZeRO-2 over ZeRO-1 besides implementation complexity, and indeed **ZeRO-2 is usually the better option**.\n:::\n\nWe can see how this works with an example.\n\n::: {.panel-tabset}\n\n## Reduce-Scatter Example\n```python\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_reduce_scatter():\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n\n    # Construct a single input tensor, then split into equal chunks (one for each rank)\n    input_tensor = torch.arange(1, world_size * 3 + 1, dtype=torch.float32).cuda()\n    input_list = list(torch.chunk(input_tensor, world_size))\n    output_tensor = torch.zeros(3, dtype=torch.float32).cuda()\n\n    print(f\"Before reduce_scatter on rank {rank}: {[t.cpu().tolist() for t in input_list]}\")\n    dist.reduce_scatter(output_tensor, input_list, op=dist.ReduceOp.SUM)\n    print(f\"After reduce_scatter on rank {rank}: {output_tensor.cpu().tolist()}\")\n\n# Initialize the process group and set device, then perform Reduce-Scatter\ninit_process()\nexample_reduce_scatter()\n```\n:::\n\nJust like before, you can run this code with 4 GPUs:\n\n```bash\ntorchrun --nproc_per_node=4 dist_reduce_scatter.py\n```\n\nTypical output:\n\n```bash\nBefore reduce_scatter on rank 0: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\nBefore reduce_scatter on rank 1: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\nBefore reduce_scatter on rank 2: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\nBefore reduce_scatter on rank 3: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\n\nAfter reduce_scatter on rank 0: [4.0, 8.0, 12.0]\nAfter reduce_scatter on rank 1: [16.0, 20.0, 24.0]\nAfter reduce_scatter on rank 2: [28.0, 32.0, 36.0]\nAfter reduce_scatter on rank 3: [40.0, 44.0, 48.0]\n```\n\nEach rank starts with identical chunked inputs. After `reduce_scatter`, every rank gets the sum (across all ranks) of the *i-th chunk*, distributed such that rank 0 gets chunk 0's sum, rank 1 gets chunk 1's sum, etc.\n\nAfter **Reduce-Scatter** operation, each GPU now has the gradient shard corresponding to its optimizer state shard which it can use to update its model parameters.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_3.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nAnd now, we need to communicate again, to get the updated model parameters on all the GPUs, as at this point each GPU has the updated model parameters only for its own shard of the optimizer state.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_4.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nThis is how ZeRO-2 strategy works. We have come a long way from the vanilla Data Parallelism to ZeRO-2, where we have reduced the memory footprint quite significantly, but can we further scale ? And this is exactly what ZeRO-3 strategy does. \n\n### ZeRO-3: Sharding Parameters\n\nZeRO-3 is the most aggressive form of ZeRO, it shards all the static memory components: parameters, gradients, and optimizer states. So, each GPU now only needs to store the parameter shard corresponding to its optimizer state shard.\n\n<p align=\"center\">\n  <img src=\"assets/zero-3.png\" alt=\"ZeRO-3\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"ZeRO-3 vs. FSDP\"}\n\nYou may have seen the terms **ZeRO-3** and **Fully Sharded Data Parallel (FSDP)** used almost interchangeably in literature, blogs, and PyTorch documentation. That's because the underlying strategy is the same: shard parameters, gradients, and optimizer states across GPUs to minimize memory usage per device.\n\n- **ZeRO-3** originated as a theoretical memory optimization described in the [Microsoft DeepSpeed ZeRO paper](https://arxiv.org/abs/1910.02054), outlining **stage 3** of ZeRO by sharding all model state across different GPUs. Its basically a concept implemented in multiple frameworks like DeepSpeed, etc.\n- **FSDP (Fully Sharded Data Parallel)** is the official PyTorch implementation of this idea. FSDP leverages the ZeRO-3 approach and provides a flexible interface for applying parameter, gradient, and optimizer sharding with PyTorch models in both research and production environments.\n:::\n\nWith ZeRO-3, the memory formula per GPU now becomes:\n$$\n\\mathcal{M}_{\\text{ZeRO-3}} = \\frac{2\\Psi + 2\\Psi + 12\\Psi}{N_d} = \\frac{16\\Psi}{N_d}\n$$\n\n- Parameters (BF16): $\\frac{2\\Psi}{N_d}$\n- Gradients (BF16): $\\frac{2\\Psi}{N_d}$\n- Optimizer States + Master Weights (FP32): $\\frac{12\\Psi}{N_d}$\n\nIf we again use an **A100/H100 GPU** with **80GB** of memory, and $N_d = 64$ GPUs, then the largest model we can train would be:\n\n$$\n\\text{Max Parameters (ZeRO-3, 64 GPUs)} = \\frac{80~\\text{GB}}{0.25~\\text{bytes per param}} \\approx 320~\\text{billion parameters}\n$$\n\nLet's put this all side-by-side:\n\n| Strategy     | Effective Bytes/Param | Max Model on 80GB GPU |\n|--------------|----------------------|-----------------------|\n| **DP**       | 16                   | ~5B                   |\n| **ZeRO-1**   | 4.2                  | ~19B                  |\n| **ZeRO-2**   | 2.2                  | ~36B                  |\n| **ZeRO-3**   | 0.25                 | ~320B                 |\n\nAs you can see, **ZeRO-3/FSDP** can 10x the maximum trainable model size compared to **ZeRO-2**, and over 60x compared to **vanilla DP**.\n\nNow, let's see how the communication overhead changes with ZeRO-3. As the model parameters are now sharded, we have a problem, we can not do the forward pass without any communication, we need to do a **All-Gather** operation to first get the full model parameters on all the GPUs.\n\n<p align=\"center\">\n  <img src=\"assets/zero-3_1.png\" alt=\"ZeRO-3 Forward\" width=\"100%\"/>\n</p>\n\nBut after the forward pass, we can _flush_ the model parameters from memory, as we don't need them anymore for the current forward pass (as we can see above). So, although it reduces the memory footprint, it introduces a communication overhead. \n\nSimilarly in the backward pass, we need to gather the parameters as and when needed using **All-Gather** and then perform **Reduce-Scatter** operation to get the gradient shards on all the GPUs as we did in ZeRO-2.\n\n<p align=\"center\">\n  <img src=\"assets/zero-3_2.png\" alt=\"ZeRO-3 Backward\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"ZeRO-3 Communication and Memory Recap\"}\n\nLet’s recap how communication and memory work with ZeRO-3. For the forward pass, since the parameters are fully sharded, we have to **all-gather** the weights whenever we need them, which gives us a communication cost of $\\Psi$. Because those parameters can be released from memory right after the forward usage, we have to all-gather again as needed in the backward pass, so we pay that $\\Psi$ “tax” a second time. And just like ZeRO-2, we need a **reduce-scatter** for the gradients at the end of backward, which adds yet another $\\Psi$ in communication cost. So in total, the communication bill per iteration comes out to $3\\Psi$, a bit higher than the $2\\Psi$ we saw in ZeRO-2.\n\nOn paper, this sounds like a lot of data being moved around, but in practice it’s not too scary! Thanks to *prefetching,* we can overlap these all-gather operations with computation. Typically, while we’re doing the forward for Layer $n$, we can start all-gathering the parameters for Layer $n+1$ in parallel. Similarly, during the backward pass, we can prefetch the next set of weights needed. This overlap keeps things efficient as long as we aren’t cranking DP up to very large scales (as a rough guideline, keeping DP $\\leq$ 512 is usually safe).\n\nFrom the memory perspective, by sharding everything, we’ve boiled the formula down to its most compact form:\n\n$$\n\\mathcal{M}_{\\text{ZeRO-3}} = \\frac{2\\Psi + 2\\Psi + 12\\Psi}{N_d}\n$$\nIncreasing the DP group size keeps reducing per-GPU model memory, but activation memory still requires tricks like checkpointing and grad accumulation, which we discussed earlier.\n:::\n\nOne important point that can be confusing at first: Even though ZeRO-1, ZeRO-2, and ZeRO-3 shard the model, they are all still types of **Data Parallelism**. \n\nEach GPU still processes the entire forward and backward pass of the model on its own batch of data, just like vanilla DP. The main difference is that **ZeRO changes how the model’s parameters and related tensors are stored and managed across GPUs**, which dramatically reduces memory usage but doesn’t change the core idea of Data Parallelism.\n\n<p align=\"center\">\n  <img src=\"assets/dp_summary.png\" alt=\"DP Summary\" width=\"100%\"/>\n</p>\n\n# Introduction to Ray - A Unified AI Compute Engine\n\nNow that we've explored the ZeRO stages and different data parallel strategies, let's discuss how to put these techniques into practice using Ray and PyTorch and, crucially, why Ray is such a good fit for large-scale distributed training in real-world settings.\n\nUntil now, we've mostly focused on how to leverage multiple GPUs within a single machine. However, scaling up modern deep learning requires distributing the training job not just across several GPUs, but often across many different machines as well. This introduces a host of new challenges — from launching and configuring clusters, to monitoring jobs, handling failures, and minimizing the engineering overhead when scaling up and down.\n\nWhen moving to distributed training at scale, several key requirements and challenges emerge in practice:\n\n- **Scalability and Speed:** Training jobs should be able to leverage more GPUs and machines to finish faster, without painful setup.\n- **Easy Infrastructure Management:** We shouldn't need to spend time manually setting up or configuring clusters, whether on cloud or on-premises resources.\n- **Visibility and Monitoring:** It must be easy to track metrics, logs, and failures across all nodes, so debugging and monitoring don’t turn into a bottleneck.\n- **Reliability and Fault Tolerance:** Hardware failures, network issues, or preempted nodes shouldn't force us to restart training from scratch—resilience and checkpointing are critical.\n- **Minimal Code Changes:** Adapting our code for distributed training shouldn't require a major rewrite of training logic.\n\nRay provides solutions for all of these requirements (and more), making it a compelling choice for large-scale distributed deep learning.\n\nThis is why Ray is increasingly popular in the deep learning ecosystem. By abstracting away the pain points of distributed systems, Ray lets us scale from laptop to GPU cluster without major workflow changes or infrastructure headaches. \n\nIn the sections that follow, we'll see concrete code and recipes showing how easy Ray makes it to scale PyTorch training seamlessly, from simple data parallel jobs to advanced ZeRO and FSDP setups. But before that let's spend some time to understand what Ray is and how it works.\n\n**Ray** is an open-source, unified AI Compute Engine designed to scale Python applications, especially AI/ML workloads, from a single laptop to clusters with thousands of machines, all with minimal code changes. At the core of Ray is **Ray Core**, a low-level distributed computing framework featuring a simple, Pythonic API for building and scaling distributed applications.\n\n<p align=\"center\">\n  <img src=\"assets/ray_01.png\" alt=\"Ray\" width=\"100%\"/>\n</p>\n\n## `Ray Core` Primitives\n\nRay Core provides a minimal, yet powerful set of primitives that let you upgrade normal Python code into distributed code with almost no friction. \n\n<p align=\"center\">\n  <img src=\"assets/ray_00.png\" alt=\"Ray Core Primitives\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"Ray Compute Engine\"}\n**Ray** manages the tough parts (task scheduling, node failures, data transfers, etc.) under the hood, so you don’t have to. Basically, as an ML Engineer/Researcher, you can focus on your model and data, and Ray will take care of the rest (which in our case is the madness of distributed systems and distributed training).\n:::\n\n## An example: For `Stateless` Tasks (Tasks)\n\nLet's try to understand the _core primitives_ of Ray, `tasks` and `actors` with an example. Imagine you’re building a simple app where you need to process a batch of images with a slow transformation (invert the colors). \n\nAt first, you write a `for-loop` to process the images sequentially. It works, but it’s sluggish, using only a single CPU core, even if your laptop has eight. What if you need to process hundreds or thousands of images? This is where **Ray** comes in, and a world of instant scalability. \n\nBelow, we walk step-by-step through the journey: from a plain, sequential Python function—painfully slow! to a parallel powerhouse processed by using Ray Tasks, and finally to coordinated, stateful parallelism with Ray Actors. \n\n:::: {.panel-tabset}\n\n### Step 1. The Baseline: Sequential Processing\n\nWe start at square one with a classic slow `for-loop`. Each image is processed one after another, burning a whole second per image. With 8 images, that's 8 seconds to process all the images.\n\n```python\n# sequential_process.py\nimport time\nimport numpy as np\n\ndef process_image(image: np.ndarray) -> np.ndarray:\n    \"\"\"Simulates a slow 1-second filter.\"\"\"\n    time.sleep(1)\n    return 255 - image\n\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\n\nstart_time = time.time()\n# Sequential: 8 images × 1 sec/image = 8 seconds\nresults = [process_image(img) for img in images]\nend_time = time.time()\n\nprint(f\"Processed {len(results)} images in {end_time - start_time:.2f} seconds.\")\n```\nOur code works, but only uses a single core, leaving the rest idle. Not a good situation. Let's try to parallelize it using `Ray Tasks`.\n\n### Step 2. Parallel Ray Task (Fast)\n\nNow, let's use Ray to parallelize the image processing. Ray's `@ray.remote` decorator instantly upgrades your function to run in parallel, one copy per available CPU core. By switching to `.remote()` calls, you launch as many parallel jobs as you have CPUs, and gather results with `ray.get()`.\n\n```python\n# parallel_process.py\nimport ray\nimport time\nimport numpy as np\n\n# 1. Initialize Ray - autodetects & uses all available CPU cores\nray.init()\n\n# 2. Decorate the function as a remote Ray task\n@ray.remote\ndef process_image(image: np.ndarray) -> np.ndarray:\n    \"\"\"Simulates a slow 1-second filter.\"\"\"\n    time.sleep(1)\n    return 255 - image\n\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\n\nstart_time = time.time()\n\n# 3. Launch tasks in parallel; returns list of ObjectRefs (futures)\nresult_refs = [process_image.remote(img) for img in images]\n\n# 4. Wait for and retrieve finished results via ray.get()\nresults = ray.get(result_refs)\nend_time = time.time()\n\n# On an 8-core machine: ~1 second total runtime!\nprint(f\"Processed {len(results)} images in {end_time - start_time:.2f} seconds.\")\n\nray.shutdown()\n```\n**What we did differently?**  \n\n- We decorated the function with `@ray.remote` decorator to make it a remote task.\n- We called the function with `.remote()` to launch it as a remote task.\n- We waited for the results with `ray.get()`.\n- The secret here is the `ObjectRef`: each `.remote()` call sends off a job in the background, while your main code keeps going. When you call `ray.get(result_refs)`, Ray assembles all results when they're ready.\n\n::::\n\n::: {.callout-tip title=\"Ray Speed-Up\"}\n**By adding just `@ray.remote`, `.remote()`, and `ray.get()`, we get a nearly 8x speedup with 8 CPU cores.**\n:::\n\n## Going Further: For `Stateful` Tasks (Actors)\n\nThe beauty of Ray is that it doesn’t just parallelize our work, it also gives us the right tool for **sharing state across those jobs**. Imagine that you want a running tally (say, the total number of pixels processed across all images), but you can’t use a global variable, because each parallel job runs isolated.\n\nWhat you really want is a service: _a live, remote counter_ that all jobs can update in real-time. That’s what is known as an `Actor` in Ray: a class with its own persistent state, living somewhere on the cluster. \n\nLet's see how to create and use an Actor.\n\n:::: {.panel-tabset}\n\n### Ray Actor (Stateful)\n\n```python\n# actor_counter.py\nimport ray\nimport numpy as np\nimport time\n\nray.init()\n\n# 1. Define the stateful service as a Python Class\n@ray.remote\nclass PixelCounter:\n    # The internal state is defined in __init__\n    def __init__(self):\n        self.total_pixels = 0\n\n    # A method to mutate (update) the internal state\n    def add(self, num_pixels: int):\n        self.total_pixels += num_pixels\n\n    # A method to retrieve the internal state\n    def get_total(self) -> int:\n        return self.total_pixels\n\n# 2. Modify the Task to use the Actor Handle\n@ray.remote\ndef process_image_with_actor(image: np.ndarray, counter_actor: \"ActorHandle\"):\n    # This task calls the Actor's add method remotely\n    counter_actor.add.remote(image.size)\n    time.sleep(1)\n    # The image processing logic is here, but omitted for simplicity\n\n# --- Main Script ---\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\nimage_size = images[0].size\nexpected_total = image_size * len(images) # 8 * 300 = 2400\n\n# 3. Create a single instance (the Actor Handle)\ncounter = PixelCounter.remote()\n\n# 4. Launch 8 parallel tasks, passing the Actor Handle to each\ntask_refs = [process_image_with_actor.remote(img, counter) for img in images]\n\n# Wait for all the image processing tasks to complete\nray.get(task_refs)\n\n# 5. Retrieve the final state from the Actor\nfinal_total_ref = counter.get_total.remote()\nfinal_total = ray.get(final_total_ref)\n\nprint(f\"Expected total pixels: {expected_total}\")\nprint(f\"Actual total from actor: {final_total}\")\n\nray.shutdown()\n```\n\n**What we did differently?** \n\n- Our stateless Ray Tasks each process an image, but the Actor (the remote `PixelCounter` class) lives on, safely tallying up the total pixel count as tasks report in.\n- The Actor’s state persists across many requests, letting you coordinate and aggregate information even in a distributed, parallel environment.\n- The only change from earlier?  \n  - Define a `@ray.remote` class (`PixelCounter`).\n  - Pass its handle to your tasks so they can call `add.remote()`.\n\n::::\n\nThis pattern of combining **Ray Tasks** for stateless (think of Python functions), independent work and **Ray Actors** (think of Python classes) for shared state, is the foundation of scalable Python pipelines for any real-world application (not just any AI applications). And we can build anything with this and make it scalable.\n\nRay's primitives empower us to build scalable, reliable, and maintainable distributed applications, without rewriting all our code or micro-managing threads and processes.\n\n## Ray for Different AI Workloads\n\nWhile Ray Core provides the low-level primitives for building distributed applications, it is not the only or always the best option, especially for **specialized AI workloads**. That's why Ray offers higher-level abstractions tailored to specific AI tasks like **data processing, training, hyperparameter search, RL and model serving**. \n\nRay provides advanced AI Libraries such as:\n\n| Ray Library | Purpose | Key Features / Benefits |\n|:---|:---|:---|\n| `Ray Data` | `Scalable Data Ingest, Processing, Inference` | Effortlessly shards and preprocesses massive datasets; streams data efficiently between CPUs (ETL) and GPUs (training/inference) to maximize hardware utilization. |\n| `Ray Train` | `Distributed Training & Fine-Tuning` | Abstracts away multi-node/GPU orchestration and synchronization for PyTorch, TensorFlow, etc., without the need for boilerplate or manual sync. |\n| `Ray Tune` | `Scalable Hyperparameter Search` | Coordinates and manages hyperparameter trials (search, early stopping, scheduling) across a cluster; includes experiment tracking and best-model picking. |\n| `Ray Serve` | `Fast, Programmable Model Serving` | Deploys models and logic as microservices with auto-scaling; supports model composition and features like traffic splitting and versioning. |\n| `Ray RLlib` | `Scalable Reinforcement Learning` | Provides a comprehensive library for training and evaluating RL algorithms. |                                                                      |\n\nThese libraries are built on top of `Ray Core` and offer a more user-friendly interface for building distributed applications. \n\n::: {.callout-tip title=\"Why Use Higher-Level Ray Libraries?\"}\nIf Ray Core’s Tasks and Actors are this powerful, why bother with higher-level Ray libraries like Ray Train or Ray Data?\n\nThe answer: **Abstraction and Specialization**. While it’s technically possible to build a full distributed training pipeline with only Ray Core, that approach means you shoulder all the complexities—manual data sharding, synchronization (e.g., for PyTorch DDP), distributed checkpointing, fault tolerance, handling resuming, and hyperparameter search. That’s a lot of boilerplate and risk!\n:::\n\nNot only that it also have tight integration with popular frameworks like `PyTorch`, `vLLM`, `Hugging Face`, and more.\n\n<p align=\"center\">\n  <img src=\"assets/ray_02.png\" alt=\"Ray\" width=\"100%\"/>\n</p>\n\nThis unified ecosystem empowers us to build end-to-end distributed AI workflows. Every component from ingest to training to serving, scales seamlessly from our laptop to a full cluster. \n\nIn this blog, we’ll focus on distributed training with `Ray Train`, showing how it can scale our PyTorch training from a single GPU to a full cluster almost effortlessly.\n\n::: {.callout-warning title=\"Why Not Use Only PyTorch Distributed?\"}\n\nWhile **PyTorch Distributed** (such as DDP) is an excellent built-in solution for multi-GPU training, it's primarily designed for *single-node* or *homogeneous, tightly-coupled clusters*. If you're just scaling to multiple GPUs on one machine, PyTorch's distributed tools are often enough. And you can run your training job with `torchrun` command.\n\nHowever, **the challenges multiply dramatically the moment you want to scale across several machines or need to orchestrate complex workflows**. Tasks like:\n\n- Launching jobs and synchronizing them across machines,\n- Managing different workers, node failures, and resuming or monitoring experiments,\n- Efficiently using both CPUs (for pre-processing and data loading) and GPUs (for training/inference) in one seamless workflow,\n- Sharding and streaming large datasets not just for one epoch, but for repeated, distributed, and fault-tolerant training,\n\nbecome **painful and complex** with only PyTorch’s stock tools. \n\nThis is exactly where **Ray shines**.  \n\nIt's abstracts away the low-level engineering required to run distributed workloads at scale. For instance, with **Ray Train** and **Ray Data**, you get seamless multi-GPU, multi-node orchestration, unified CPU-GPU pipelines, resilience and scalability. This helps you focus on your algorithms and models, rather than the underlying infrastructure. \n\n:::\n\n# Distributed Training with Ray Train and PyTorch \n\nNow that we have understood the basics of Ray and how it can help us scale any application, let's now dive deep into distributed training with Ray Train and PyTorch. \n\n<p align=\"center\">\n  <img src=\"assets/ray_03.png\" alt=\"Distributed Training with Ray Train and PyTorch\" width=\"100%\"/>\n</p>\n\nBefore diving into distributed training, let's establish a baseline by looking at a simple single-GPU training loop. This will help us understand what needs to change when we migrate to distributed training.\n\nBefore we dive into distributed training, let's first look at a **simple, single-GPU PyTorch training loop** as our baseline. This example will help ground the core steps that appear in almost any deep learning project, and by keeping things simple (no distributed libraries, no argument parsing), we make it crystal clear what needs to change later for multi-GPU or multi-node scaling.\n\n## Single-GPU PyTorch Training on CIFAR-10\n\nWe'll use a modern Vision Transformer model (`torchvision.models.VisionTransformer`) and the CIFAR-10 dataset. This code works on CPU, GPU (CUDA), or Apple MPS — but it's strictly ordinary, non-distributed PyTorch.\n\nIf you have trained any model with PyTorch on a single machine, you might have seen a similar training loop. The standard way to train a model with PyTorch on a single machine (single GPU) consists of these steps:\n\n1. Download and prepare the `dataset`\n    - Set up `data loaders`\n2. Define the `model` \n    - Move the model to the available device (GPU, MPS, or CPU).\n3. Set up `optimizer` and `loss`.\n4. Run the `training loop`.\n    - Iterate over training data to update model weights.\n    - Check accuracy on validation data.\n5. Optionally, `checkpoint` the model at the end.\n\nThis works perfectly for *one GPU or a single machine*, but doesn't scale automatically. We'll see soon how to migrate this to Ray Train for scaling, but for now, here's the basic setup.\n\n### DataLoader Function\n\nThis function sets up the [DataLoaders](https://pytorch.org/docs/stable/data.html) for the CIFAR-10 training and test splits. Note: We use a `FileLock` to avoid concurrency issues if datasets are being downloaded.\n\n```python\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Normalize, ToTensor\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport os\n\ndef get_dataloaders(batch_size):\n    \"\"\"\n    Create standard PyTorch DataLoaders.\n    No distributed code — just vanilla PyTorch.\n    \"\"\"\n\n    transform = transforms.Compose([\n        ToTensor(),\n        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_data = datasets.CIFAR10(\n            root=\"~/data\", train=True, download=True, transform=transform,\n        )\n        test_data = datasets.CIFAR10(\n            root=\"~/data\", train=False, download=True, transform=transform,\n        )\n\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size)\n\n    return train_loader, test_loader\n```\n\n### Training Function\n\nThis is the standard PyTorch training loop, using `torchvision.models.VisionTransformer`. \n\n```python\nfrom torchvision.models import VisionTransformer\nfrom torch import nn\nimport torch\nfrom tqdm import tqdm\n\ndef train_func(lr=1e-3, epochs=10, batch_size=512):\n    \"\"\"\n    Main training function: single machine, single GPU.\n    \"\"\"\n    # Get data loaders\n    train_loader, val_loader = get_dataloaders(batch_size=batch_size)\n\n    # Create the model\n    model = VisionTransformer(\n        image_size=32,   # CIFAR-10 images are 32x32\n        patch_size=4,    # Reasonable patch size for CIFAR-10\n        num_layers=12,   # Transformer layers\n        num_heads=8,     # Attention heads\n        hidden_dim=384,  # Model width\n        mlp_dim=768,     # Transformer MLP dim\n        num_classes=10   # CIFAR-10\n    )\n\n    # Move model to correct device (GPU/MPS/CPU)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    model.to(device)\n\n    # Set up loss and optimizer\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    # Training loop\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for X, y in tqdm(train_loader, desc=f\"Train Epoch {epoch + 1}\"):\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n\n        # Validation phase\n        model.eval()\n        val_loss, num_correct, num_total = 0, 0, 0\n        with torch.no_grad():\n            for X, y in tqdm(val_loader, desc=f\"Valid Epoch {epoch + 1}\"):\n                X, y = X.to(device), y.to(device)\n                pred = model(X)\n                loss = loss_fn(pred, y)\n                val_loss += loss.item()\n                num_total += y.shape[0]\n                num_correct += (pred.argmax(1) == y).sum().item()\n        val_loss /= len(val_loader)\n        accuracy = num_correct / num_total\n\n        print(f\"  Train Loss: {train_loss:.4f} | Valid Loss: {val_loss:.4f} | Accuracy: {accuracy:.4f} ({100 * accuracy:.2f}%)\")\n\n    # Optional: Save checkpoint\n    checkpoint = {\n        'epoch': epochs,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'accuracy': accuracy,\n    }\n    torch.save(checkpoint, 'checkpoint_single_machine.pth')\n    print(f\"\\nTraining completed! Final accuracy: {100 * accuracy:.2f}%\\nCheckpoint saved to checkpoint_single_machine.pth\")\n```\n\n### Training the Model\nLet's now train the model on a single GPU.\n\n```python\ntrain_func(lr=1e-3, epochs=10, batch_size=512)\n```\n\n<p align=\"center\">\n  <img src=\"assets/gpu_01.png\" alt=\"Training the Model\" width=\"100%\"/>\n</p>\n\nAs you can see, the model is trained on a single GPU while other GPUs are idle. \n\nThis script represents *plain vanilla* PyTorch, suitable for a single GPU or single CPU. There is no distributed logic or Ray involved yet. All of the key logic, especially the `get_dataloaders` function and the structure of the `train_func` will remain mostly the same when we migrate to distributed training with Ray Train!  \n\nLet's now train the model at scale on multiple GPUs across multiple machines.\n\n\n## Distributed Training with Ray Train\n\nNow, let's see how to migrate this `single-machine, single-GPU` training loop to `distributed training` using Ray Train and PyTorch on `multiple machines, multiple GPUs`. \n\n<p align=\"center\">\n  <video controls width=\"700\">\n    <source src=\"assets/ray_train_01.mov\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n  <br>\n  <em>Distributed Training with Ray Train Key Concepts.</em>\n</p>\n\n### Ray Train Architecture  \n\nRay Train's architecture is based on the following components:  \n1. A Ray Train Controller/Driver that schedules the training workers, handles errors, and manages checkpoints  \n2. Ray Train Workers that execute the training code\n\n<p align=\"center\">\n  <img src=\"assets/ray_train__01.png\" width=\"700\"/>\n</p>\n\nBelow are the key API concepts of Ray Train:\n\n1. `train_loop_per_worker`: The core function that contains your model training logic  \n2. `ScalingConfig`: Specifies the number of workers and compute resources (CPUs, GPUs, TPUs)  \n3. `Trainer`: Manages the training process  \n4. `Trainer.fit()`: Starts the distributed training job\n\n<p align=\"center\">\n  <img src=\"assets/ray_train_02.png\" width=\"700\" loading=\"lazy\"/>\n</p>\n\n### Ray Data + Ray Train Integration\n\nHere is a diagram showing the Ray Data and Ray Train integration.\n\n<p align=\"center\">\n  <img src=\"assets/ray_train_03.png\" width=\"800\" loading=\"lazy\"/>\n</p>\n\nWe are not going to go into the details of Ray Data and Ray Train integration in this post. But if you are interested in learning more about it, you can check out the [Ray Data](https://docs.ray.io/en/latest/data/index.html) and [Ray Train](https://docs.ray.io/en/latest/train/index.html) documentation.\n\n\n## Setup the Environment\n\nTo check how many GPUs (and CPUs) are available in your Ray cluster, you can use the script below:\n\n::: {.panel-tabset}\n\n### Check Cluster GPUs\n\n```python\nimport ray\nimport torch\n\ndef check_cluster_gpus():\n    \"\"\"Check GPU count in the entire Ray cluster.\"\"\"\n    # Initialize Ray if not already initialized\n    if not ray.is_initialized():\n        ray.init()\n\n    # Get cluster resources (total GPUs in cluster)\n    cluster_resources = ray.cluster_resources()\n    total_gpus = cluster_resources.get(\"GPU\", 0)\n\n    # Get available resources (currently available GPUs)\n    available_resources = ray.available_resources()\n    available_gpus = available_resources.get(\"GPU\", 0)\n\n    # Get local GPU count (GPUs on this node only)\n    local_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n\n    # Print results\n    print(\"\\n\" + \"=\"*60)\n    print(\"Ray Cluster GPU Information\")\n    print(\"=\"*60)\n    print(f\"Total GPUs in cluster:     {int(total_gpus)}\")\n    print(f\"Available GPUs in cluster: {int(available_gpus)}\")\n    print(f\"Local GPUs (head node):    {local_gpus}\")\n    print(\"=\"*60)\n\n    # Additional cluster info\n    print(\"\\nCluster Resources:\")\n    print(f\"  CPUs (total):     {int(cluster_resources.get('CPU', 0))}\")\n    print(f\"  CPUs (available): {int(available_resources.get('CPU', 0))}\")\n\n    # Show node details if available\n    try:\n        nodes = ray.nodes()\n        print(f\"\\nCluster Nodes: {len(nodes)}\")\n        for i, node in enumerate(nodes):\n            node_resources = node.get('Resources', {})\n            node_gpus = node_resources.get('GPU', 0)\n            print(f\"  Node {i+1}: {int(node_gpus)} GPU(s)\")\n    except Exception as e:\n        print(f\"\\nNote: Could not retrieve node details: {e}\")\n\n    print()\n    return {\n        'total_gpus': int(total_gpus),\n        'available_gpus': int(available_gpus),\n        'local_gpus': local_gpus\n    }\n\nif __name__ == '__main__':\n    check_cluster_gpus()\n```\n:::\n\nAs you can see, the cluster which I have has a total of 8 GPUs. The cluster consists of a total of one Head Node and two Worker Nodes, where each worker node has 4 GPUs each.\n\n```bash\n============================================================\nRay Cluster GPU Information\n============================================================\nTotal GPUs in cluster:     8\nAvailable GPUs in cluster: 8\nLocal GPUs (head node):    0\n============================================================\n\nCluster Resources:\n  CPUs (total):     96\n  CPUs (available): 96\n\nCluster Nodes: 3\n  Node 1: 0 GPU(s)\n  Node 2: 4 GPU(s)\n  Node 3: 4 GPU(s)\n```\n## Distributed Training with Ray Train and PyTorch FSDP\n\nNow that we have understood the basics of Ray Train, and also have a Ray cluster ready, let's now dive into distributed training with Ray Train and PyTorch FSDP.\n\n### 1. Specify Cluster Scaling\n\nFirst, set up how many Ray workers (processes) will participate—typically one per GPU. For a Ray cluster with 8 GPUs:\n\n```python\nscaling_config = ScalingConfig(\n    num_workers=8,  # e.g., 8 GPUs in your cluster\n    use_gpu=True,\n    resources_per_worker={\"CPU\": 2, \"GPU\": 1},\n)\n```\n\n### 2. Data Preparation: PyTorch DataLoaders\n\nData preparation is unchanged from typical PyTorch or DDP usage. Use your usual transforms and DataLoader logic:\n\n```python\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport os\n\ndef get_dataloaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_ds = datasets.CIFAR10(\"~/data\", train=True, download=True, transform=transform)\n        valid_ds = datasets.CIFAR10(\"~/data\", train=False, download=True, transform=transform)\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(valid_ds, batch_size=batch_size),\n    )\n```\n\nNo special considerations are needed for FSDP at this stage.\n\n### 3. Define the Training Function\n\nDefine your Ray Train worker function to train the model. This is the training function that will be executed by each worker. As the model is now being prepared for FSDP, we need to use the `prepare_model` function to prepare the model for FSDP. \n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torchvision.models import VisionTransformer\nimport ray\nimport tempfile\n\ndef train_func_per_worker(config):\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    ctx = ray.train.get_context()\n    rank = ctx.get_world_rank()\n    world_size = ctx.get_world_size()\n\n    if rank == 0:\n        print(f\"Training with FSDP across {world_size} workers...\")\n\n    # Prepare DataLoaders for distributed training\n    train_dl, valid_dl = get_dataloaders(batch_size)\n    train_dl = ray.train.torch.prepare_data_loader(train_dl)\n    valid_dl = ray.train.torch.prepare_data_loader(valid_dl)\n\n    # Define the model\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n\n    # Prepare the model for FSDP\n    model = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, sample_cnt = 0.0, 0\n        for X, y in train_dl:\n            pred = model(X)\n            loss = criterion(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * X.shape[0]\n            sample_cnt += X.shape[0]\n        train_loss = total_loss / sample_cnt\n\n        # Validation loop\n        model.eval()\n        valid_loss, correct, total = 0.0, 0, 0\n        with torch.no_grad():\n            for X, y in valid_dl:\n                pred = model(X)\n                valid_loss += criterion(pred, y).item() * X.shape[0]\n                total += y.shape[0]\n                correct += (pred.argmax(dim=1) == y).sum().item()\n        valid_loss /= total\n        acc = correct / total\n\n        if rank == 0:\n            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} Valid Loss={valid_loss:.4f} Acc={acc:.3%}\")\n\n        metrics = {\"epoch\": epoch+1, \"train_loss\": train_loss, \"valid_loss\": valid_loss, \"accuracy\": acc}\n        # Checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            with tempfile.TemporaryDirectory() as ckpt_dir:\n                torch.save(model.module.state_dict(), os.path.join(ckpt_dir, \"model.pt\"))\n                ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(ckpt_dir))\n        else:\n            ray.train.report(metrics)\n```\n\n::: {.callout-note title=\"Change the parallel strategy to DDP\"}\nTo change the parallel strategy to DDP, simply change the parameter to `\"ddp\"`:\n```python\nmodel = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n```\n:::\n\n### 4. Configure Run Checkpointing and Storage\n\nUse Ray's checkpointing utilities to save the best results and recoverable states:\n\n```python\nfrom ray.train import RunConfig, CheckpointConfig\n\ncheckpoint_config = CheckpointConfig(\n    num_to_keep=2,\n    checkpoint_score_attribute=\"accuracy\",\n    checkpoint_score_order=\"max\",\n)\nrun_config = RunConfig(\n    name=\"cifar10_fsdp_example\",\n    storage_path=\"/mnt/cluster_storage/training/\",  # Use a persistent/shared location\n    checkpoint_config=checkpoint_config,\n)\n```\n\n### 5. Launch Training with TorchTrainer\n\nBring all the configs together and kick off distributed training:\n\n```python\nfrom ray.train.torch import TorchTrainer\n\nglobal_batch_size = 1024\nnum_workers = 8   \nbatch_size_per_worker = global_batch_size // num_workers\n\ntrain_loop_config = {\n    \"lr\": 1e-3,\n    \"epochs\": 20,\n    \"batch_size_per_worker\": batch_size_per_worker,\n}\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nprint(\"Starting FSDP distributed training...\")\nresult = trainer.fit()\n```\n\n### 6. Load and Use Checkpoints\n\nOur model can be restored from the best checkpoint after training:\n\n```python\nimport torch\nfrom torchvision.models import VisionTransformer\nimport os\n\nckpt = result.checkpoint\nwith ckpt.as_directory() as ckpt_dir:\n    model_path = os.path.join(ckpt_dir, \"model.pt\")\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    model.load_state_dict(state_dict)\n```\n\nHere is the complete code for the training script:\n\n::: {.panel-tabset}\n\n## train_fsdp.py\n\n```python\nimport os\nimport tempfile\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom torchvision.models import VisionTransformer\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport ray\nfrom ray.train import ScalingConfig, RunConfig, CheckpointConfig\nfrom ray.train.torch import TorchTrainer\n\ndef get_dataloaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_data = datasets.CIFAR10(root=\"~/data\", train=True, download=True, transform=transform)\n        valid_data = datasets.CIFAR10(root=\"~/data\", train=False, download=True, transform=transform)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_data, batch_size=batch_size)\n    return train_loader, valid_loader\n\ndef train_func_per_worker(config):\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    ctx = ray.train.get_context()\n    world_size = ctx.get_world_size()\n    local_rank = ctx.get_world_rank()\n    if local_rank == 0:\n        print(f\"FSDP Training on {world_size} workers\")\n\n    train_loader, valid_loader = get_dataloaders(batch_size)\n    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n    valid_loader = ray.train.torch.prepare_data_loader(valid_loader)\n\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n    # [FSDP] Key change from DDP:\n    model = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, n = 0.0, 0\n        for X, y in train_loader:\n            pred = model(X)\n            loss = criterion(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * X.shape[0]\n            n += X.shape[0]\n        train_loss /= n\n\n        model.eval()\n        correct, total, valid_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for X, y in valid_loader:\n                pred = model(X)\n                valid_loss += criterion(pred, y).item() * X.shape[0]\n                total += y.shape[0]\n                correct += (pred.argmax(dim=1) == y).sum().item()\n        valid_loss /= total\n        accuracy = correct / total\n        metrics = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"valid_loss\": valid_loss,\n            \"accuracy\": accuracy\n        }\n        # Save a checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            with tempfile.TemporaryDirectory() as tmp_ckpt_dir:\n                torch.save(model.module.state_dict(),\n                           os.path.join(tmp_ckpt_dir, \"model.pt\"))\n                ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(tmp_ckpt_dir))\n        else:\n            ray.train.report(metrics)\n\nscaling_config = ScalingConfig(num_workers=8, use_gpu=True, resources_per_worker={\"CPU\": 2, \"GPU\": 1})\ncheckpoint_config = CheckpointConfig(num_to_keep=2, checkpoint_score_attribute=\"accuracy\", checkpoint_score_order=\"max\")\nrun_config = RunConfig(name=\"cifar10_fsdp_example\", storage_path=\"/mnt/cluster_storage/training/\", checkpoint_config=checkpoint_config)\n\nglobal_batch_size = 1024\nbatch_size_per_worker = global_batch_size // scaling_config.num_workers\ntrain_loop_config = {\"lr\": 1e-3, \"epochs\": 20, \"batch_size_per_worker\": batch_size_per_worker}\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()\n```\n:::\n\nWhile the training is runing, we can see the progress in the Ray dashboard. We can now see that all 8 GPUs are being used for training.\n\n<p align=\"center\">\n    <img src=\"assets/gpu_02.png\" alt=\"Ray Train FSDP Dashboard\" width=\"100%\">\n</p>\n\n## Conclusion\n\nDistributed training from scratch can be daunting, but modern frameworks like Ray Train, combined with PyTorch features such as Fully Sharded Data Parallel (FSDP), make scalable and efficient training approachable and practical. We demonstrated how to set up a distributed pipeline for finetuning a model on multiple GPUs using Ray Train and PyTorch FSDP. \n\nLeveraging Ray's abstractions, you can efficiently utilize your compute resources, monitor experiments, and scale training seamlessly. If you need to push scalability further, techniques such as Pipeline Parallelism, Tensor Parallelism, and Sequence Parallelism can be explored, each offering unique benefits for even larger model and data workloads.\n\n## References & Further Resources\n\n- [Ray Train Documentation](https://docs.ray.io/en/latest/train/index.html)\n- [PyTorch Distributed FSDP](https://pytorch.org/docs/stable/fsdp.html)\n- [Ray AIR: Scalable ML on Ray](https://docs.ray.io/en/latest/ray-air/getting-started.html)\n- [Train Deep Learning Models with Ray](https://docs.ray.io/en/latest/train/dl_guide.html)\n- [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)\n- [DeepSpeed: High-Performance Deep Learning Optimization Library](https://www.deepspeed.ai/)\n- [Ray Data for Large-Scale Data Ingestion & Preprocessing](https://docs.ray.io/en/latest/data/index.html)\n\n\n","srcMarkdownNoYaml":"\n\n\n## Introduction\n\nDeep learning models are becoming massive, and so are the challenges of training them. I am quite new to distributed training, and at present I am trying to learn how to scale the training to multiple GPUs and multiple machines using different forms of parallelism. \n\nAnd in the process, I found that more than the training or the algorithm itself, it is about understanding how distributed systems work and how to manage resources (GPUs and CPUs) efficiently.\n\n<p align=\"center\">\n  <img src=\"assets/model_params.png\" alt=\"Model Parameters vs Training Time\" width=\"100%\"/>\n</p>\n\nAs models grow in size and complexity, the time required to train them is skyrocketing, sometimes taking days, weeks, or even months for a single epoch on one GPU. \n\n\n| Model            | Parameters<br/>(Millions) | Training Time on A100<br/>(GPU Hours) |\n|------------------|:------------------------:|:-----------------------------:|\n| **ResNet-50**    | 26                       | 31                            |\n| **ResNet-101**   | 45                       | 44                            |\n| **BERT-Base**    | 108                      | 84                            |\n| **Turing-NLG 17B** | 17,000                 | TBA                           |\n| **GPT-3 175B**   | 175,000                  | 3,100,000                     |\n\n\nIf we look at this table, we can see the dramatic growth in model parameter counts and the corresponding jump in training time requirements:\n\n- **ResNet-50 and ResNet-101** are manageable on a single GPU, but BERT-Base is already pushing the envelope.\n- **Turing-NLG 17B** and especially **GPT-3 175B** are in a completely different league, requiring immense computing power and time.\n\n::: {.callout-note}\nIf we tried to train GPT-3 on a single GPU, it would take roughly **355 years** to finish. Distributed training is not just useful, it's absolutely essential as model sizes and training time requirements soar\n:::\n\nBut why is this happening? Let's set the stage with some striking observations:\n\n- Model sizes and GPU demand are exploding.\n- Training hours required can reach into the millions for cutting-edge models.\n\nConsider the `LLaMA` family of models. The graph below shows that as you scale up model size (circle diameter), performance increases but so does the training time. Look at the y-axis: we're talking *millions* of GPU hours! Training even one of these models on a single GPU isn't just slow, it's practically impossible.\n\n<p align=\"center\">\n  <img src=\"assets/model-size-vs-performance.png\" alt=\"Model Size vs Performance\" width=\"70%\"/>\n</p>\n\nIn this blog post, we'll try to understand distributed training from first principles and how to scale the training to multiple GPUs and multiple machines using different forms of parallelism. We will also look at how to implement these techniques from scratch using `PyTorch` and later on we will use `Ray` to scale the training.\n\nAs I mentioned earlier, I am quite new to distributed training, and at present I'm still in `epoch` 1 of my journey :). \n\nBefore I get started I would like to thank few of the brilliant researchers, professors, like **Prof Tanmoy Chakraborty**, **Dr Yatin Nandwani** and **Prof. Song Han**. My mentor/teacher **Rohan Shravan**, for his exceptional teaching, guidance and coaching over the years. And also to few of my colleagues and friends like **Dipankar Ranjan Baisya**, **Chris Fregly**, **Zachary Mueller**, **Ram Mohan**, **Debanjan Saha** and **Siddhant Gupta** who have made this journey possible for me. \n\nMost of the content in this blog post is based on the work and lectures of these brilliant researchers, professors, and my mentors/teachers. You can find all the references and further resources at the end of the blog post.\n\n## Deep Learning Training Basics\n\nBefore diving into scaling, let's quickly review the standard model training loop, such as a simple **Multi-Layer Perceptron (MLP)**:\n\n::: {.panel-tabset}\n## Pseudo Code for the Training Loop\n\n```python\n1  model = MLP().to(device)\n2  optimizer = Adam(model.parameters())\n3  criterion = CrossEntropyLoss()\n4  data_loader = DataLoader(dataset)\n5  \n6  for epoch in range(num_epochs):\n7      model.train()  \n8      for inputs, targets in data_loader:\n9          # 1. Move batch to GPU\n10         inputs, targets = inputs.to(device), targets.to(device)\n11         \n12         # 2. Clear gradients\n13         optimizer.zero_grad()\n14         \n15         # 3. Forward pass\n16         outputs = model(inputs)\n17         loss = criterion(outputs, targets)\n18         \n19         # 4. Backpropagation\n20         loss.backward()\n21         \n22         # 5. Optimization\n23         optimizer.step()\n```\n:::\n\n\nIn a typical training loop, after defining the model, the optimizer, the loss function, and the data loader, the training loop trains the model by performing the following steps for each epoch: \n\n1. Iterate over the data in mini-batches (`line 6-8`).\n2. Move each batch of data to the GPU (`line 9-10`).\n3. Zero out any previous gradients (`line 12-13`).\n4. Perform a forward pass to calculate the model outputs and loss (`line 15-17`).\n5. Compute gradients through backpropagation (`line 18-19`).\n6. Update the model parameters using the optimizer (`line 20-21`).\n\n<p align=\"center\">\n  <img src=\"assets/single-gpu.png\" alt=\"Single GPU Training Loop\" width=\"70%\"/>\n</p>\n\nThis pattern is the core of most deep learning training routines.\n\n## Bottlenecks in Single-GPU Training\n\nWhen training deep learning models on a single GPU, high-bandwidth memory (HBM) is utilized by four main types of data:\n\n1. **Model Parameters** ($\\Phi$):  \n   The weights being learned during training.\n\n2. **Parameter Gradients** ($\\nabla \\Phi$):  \n   The gradients computed during backpropagation, required for parameter updates.\n\n3. **Optimizer States** ($\\Phi_{\\text{optim}}$):  \n   Auxiliary variables needed by the optimization algorithm, such as momentum and variance estimates (e.g., in Adam).\n\n4. **Activations** ($\\mathcal{M}_{\\text{act}}$):  \n   The intermediate outputs from each neural network layer required to compute gradients during the backward pass.\n\nOf these, the first three (**Parameters**, **Gradients**, and **Optimizer States**) are considered **static components**. They collectively define the minimum \"static\" memory footprint determined by the model architecture itself.\n\nThe fourth component, **Activations**, is **dynamic** and its memory footprint depends on the input size (such as batch size and sequence length). Thus, activations often become the main bottleneck in large-scale training.\n\n### Static Memory \n\nAnd when we are training if you look at the training loop again, until step `optimizer.step()`, we need to keep everything in the memory. And after `optimizer.step()`, we can discard the activations and the gradients. And we can keep the model parameters and the optimizer states in the memory.\n\nIf $\\Psi$ is the total number of parameters in the model, the total static memory required ($\\mathcal{M}_{static}$) using the Adam optimizer is a fixed amount: **$16\\Psi$ bytes**. \n\n| Component | Precision | Size ($\\Psi$ Bytes) | Rationale |\n|:---|:---|:---|:---|\n| Model Parameters | BF32 (4 bytes) | $4\\Psi$ | Used for forward and backward passes |\n| Parameter Gradients | BF32 (4 bytes) | $4\\Psi$ | Used in backpropagation |\n| Optimizer States (Adam) | FP32 (4+4 bytes) | $8\\Psi$ | Stores 1st and 2nd moment estimates ($4\\Psi$ each) |\n| **Total Static Memory** |  | **$16\\Psi$** | **The absolute floor for static storage** |\n\n::: {.callout-note title=\"Why Adam Optimizer Uses $4+4$ Bytes per Parameter\"}\nAdam maintains two additional FP32 (4-byte) tensors per parameter: the **first moment** (mean of gradients, $m$) and the **second moment** (uncentered variance, $v$). Thus, for each parameter, Adam stores $4$ bytes for $m$ and $4$ bytes for $v$, totaling $8\\Psi$ bytes.\n:::\n\nAnd when it comes to training a model, its all about how smartly we can manage this memory footprint. In modern LLM training, **mixed precision** is employed, typically using BF16 (2 bytes) for fast computation while maintaining a full-precision FP32 (4 bytes) copy of weights and optimizer states for numerical stability.\n\n::: {.callout-tip title=\"Mixed Precision Training\"}\n**Mixed Precision Training** accelerates deep learning and reduces memory use by combining 16-bit (BF16/FP16) and 32-bit (FP32) floating-point operations.\n\n- *How it works*: Forward and backward passes use low-precision (e.g., BF16) for parameters and activations, while a full-precision FP32 `master` copy of weights and optimizer states is kept for numerical stability.\n- *Why it matters*: Enables training of larger models or larger batches within the same hardware footprint. Mixed precision is now standard in large-scale model training.\n:::\n\nSo, with mixed precision, the breakdown is as follows:\n\n| Component | Precision | Size ($\\Psi$ Bytes) | Rationale |\n|:---|:---|:---|:---|\n| Model Parameters | BF16 (2 bytes) | $2\\Psi$ | Used for forward and backward passes |\n| Parameter Gradients | BF16 (2 bytes) | $2\\Psi$ | Used in backpropagation |\n| Master Weights | FP32 (4 bytes) | $4\\Psi$ | Full precision copy for the update step |\n| Optimizer States (Adam) | FP32 (4+4 bytes) | $8\\Psi$ | Stores 1st and 2nd moment estimates ($4\\Psi$ each) |\n| **Total Static Memory (with Mixed Precision)** |  | **$16\\Psi$** | **The absolute floor for static storage** |\n\nYou might notice that the total static memory remains **$16\\Psi$** bytes. So what is the advantage of mixed precision training?\n\nThe key benefits of mixed precision are:\n\n- **Increased Training Speed:** As we are using lower-precision data types (like BF16) during forward and backward passes, computation is faster and less memory bandwidth is used.\n- **Reduced Activation Memory:** Since our model parameters and optimizer states are stored in FP32, the activations, which are stored in BF16 during training, require half the memory compared to FP32, so the dynamic (activation) memory footprint is significantly lower.\n\nWhile the absolute static memory is unchanged, mixed precision allows for faster training and greater memory efficiency, especially for storing activations, enabling larger models or batches to fit within the same hardware limits.\n\nAnd this calculation reveals a significant challenge: a **70 Billion parameter model** requires approximately $70\\text{B} \\times 16 \\text{ bytes} \\approx 1120 \\text{ GB}$ of static memory. With high-end GPUs typically offering only $80 \\text{ GB}$ of memory, loading even the static state of the model becomes impossible, without considering the dynamic activations.\n\n### Dynamic Memory\n\nThis component is dependent on the input batch and is the primary cause of memory bottlenecks.\n\n* **Activations:** The output of each layer. They must be stored until the **backward pass** to compute the gradients.\n    * For a linear layer $y=Wx$, the gradient for $W$ is calculated as:\n        $$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot x^T$$\n    * This requires saving the layer's input, $x$ (the activation from the previous layer).\n\n* **Activation Memory Equation:** The total memory required for activations ($m_{act}$) in mixed precision can be estimated by the following equation:\n    $$\\mathcal{M}_{\\text{act}} = L \\cdot \\text{seq} \\cdot \\text{bs} \\cdot h \\cdot \\left(34 + \\frac{5 \\cdot n_{heads} \\cdot \\text{seq}}{\\text{h}}\\right)$$\n\n    Where:\n\n    - $L$: Number of layers\n    - $\\text{seq}$: Sequence length\n    - $\\text{bs}$: Batch size (number of samples)\n    - $h$: Hidden dimension of the model\n    - $n_{heads}$: Number of attention heads\n\n<p align=\"center\">\n  <img src=\"assets/activations.png\" alt=\"Activation Count\" width=\"70%\"/>\n</p>\n\nAs we can see, activation memory usage is **not static** for a given model; it scales:\n\n- **Linearly** with the **batch size** ($\\text{bs}$)\n- **Quadratically** with the **sequence length** ($\\text{seq}$)\n\nAnd this quadratic scaling with $\\text{seq}^2$ (an effect stemming from the attention matrix) means the activation memory is the part that will **blow up** when you increase the batch size or train with longer sequences.\n\n## Batch Size Intuition\nSo, as we can see that the longer the sequence, the more activations we need and hence the more memory we need. So, even for one single sequence, the memory required is more than 50 GB. And this is a significant challenge for training large models. \n\n::: {.callout-note title=\"Global Batch Size\"}\nWhen we talk about batch size in LLM pre-training, we usually refer to it in terms of the **number of tokens**, not the number of sequences. That is, the token batch size is calculated as **sequence length × number of sequences (micro-batch size)**\n:::\n\nTypically, the global batch size used in pre-training is extremely large—usually in the **millions of tokens**. \nAnd in practice, we start the training with a smaller batch size and gradually increase it over the course of training.\n\n* **Small Batch Size:** Used at the **beginning of training** when the loss is high. It provides **quick, noisy signals** that help the model traverse the loss landscape rapidly toward the minima.\n* **Large Batch Size:** Used as training approaches the optima. It provides a **more accurate gradient direction** (a clearer signal), which reduces noise and ensures confident, stable convergence.\n\n<p align=\"center\">\n  <img src=\"assets/batch-size.png\" alt=\"Batch Size Intuition\" width=\"100%\"/>\n</p>\n\n## Memory usage in Transformer\n\nTo get a sense of the memory usage in a Transformer, let's take a look at the memory usage of `Llama 3.1` {`8B`, `70B` and `13B`} models. \n\n<p align=\"center\">\n  <img src=\"assets/llama-3-1.png\" alt=\"Batch Size Intuition\" width=\"100%\"/>\n</p>\n\nFrom this graph, we can clearly see that forshort sequences (or small batch sizes), memory usage for activations is almost negligible, but from around 4K-16K tokens they start to take up a significant amount of memory (this is because of the quadratic scaling with the sequence length, which we discussed earlier), while usage for parameters, gradients, and optimizer states is roughly independent of the sequence length and batch size.\n\nHow can we solve this problem of `activation explosion`? Can we somehow avoide storing all those activations ? \n\n### Solution 1: Activation Recomputation\n\nRecall why we need to store all those activations in the first place ? It is because we need to compute the gradients for the model parameters. So, if we can somehow avoid storing all those activations, we can save a lot of memory.\n\nOne effective approach is **Gradient Checkpointing** also known as **Activation Recomputation**. With this technique, we discard most of the activations during the forward pass to save memory and recompute them on the fly during the backward pass when gradients are needed. \n\nNormally, we’d store every hidden state between learnable operations (like feedforward layers, layer norm, etc.) to use them during the backward pass. With activation recomputation, we only store activations at specific checkpoints and recalculate everything else during backpropagation. This helps us manage memory while training large models. The process typically looks like this:\n\n<p align=\"center\">\n  <img src=\"assets/gradient-accumulation-1.png\" alt=\"Gradient Accumulation\" width=\"70%\"/>\n</p>\n\nBut as we know in life, there is no free lunch. Although we save memory by discarding most of the activations during the forward pass, we spend some extra compute to recompute these activations on the fly during the backward pass.\n\nThere are a few ways to do activation checkpointing, and each involves different memory and compute tradeoffs. \n\nThe most aggressive approach is called **Full Activation Checkpointing**, where you only store activations at the end of each layer (instead of storing every intermediate activation). This method is great for memory since you’re keeping so little, but it’s the most compute-heavy, often increasing computation time by 30–40% because you have to recompute almost everything during backpropagation.\n\nBut do we really need to treat every part of the model the same? By profiling, we find that the main memory culprit is the activations from the **Multi-Headed Attention (MHA)** layers, since they scale quadratically with sequence length.\n\nThis leads to a more balanced strategy: **Selective Checkpointing**. Here, we only skip storing activations for the heavy MHA layers and still store them for the lighter MLP layers. The payoff is impressive: up to 70% memory savings for only about 2.7% extra computation. \n\n::: {.callout-note title=\"Activation Checkpointing on Llama 3.1 8B model\"}\nAs you can see the graph bellow, on an 8-billion parameter model with a batch size of 1 and sequence length 4096, activation memory without any checkpointing can hit 97 GB, which is enough to break most GPUs. With selective activation checkpointing, that drops to 17 GB. And with full checkpointing, at the extreme, memory usage can go down to just 1 GB!\n:::\n\n<p align=\"center\">\n  <img src=\"assets/llama-checkp.png\" alt=\"Gradient Accumulation\" width=\"100%\"/>\n</p>\n\nNow that we’ve learned about recomputation, we can tame the activation memory usage we saw in the previous graphs!\n\nHowever, activations still have a linear dependence on the batch size, so as we move to larger batch sizes this might become an issue again. So, what can we do to increase the batch size ? And to takle this we have the next trick in our box - **gradient accumulation**, lets discuss that next. \n\n\n### Solution 2: Gradient Accumulation\n\n**Gradient Accumulation** is a technique that allows us to accumulate gradients over multiple micro-batches before performing a single global optimization step. This is particularly useful when we have a large batch size and we want to avoid running out of memory.\n\nThe general idea is to split the batch into smaller micro-batches (let's say 3) and process them one by one. We compute the gradients for each micro-batch and accumulate them (we **do not do** `optimizer.step()` after each micro-batch). And after processing all the micro-batches, **we perform a single** global optimization step.\n\n<p align=\"center\">\n  <img src=\"assets/grad-acc.png\" alt=\"Gradient Accumulation\" width=\"100%\"/>\n</p>\n\nLet's take an example of a simple linear regression model:\n\nLet's use a different analogy: predicting the **score of a student on a test** based on two factors—the number of hours studied ($x_1$) and the number of hours slept the night before ($x_2$). We assume a simple linear relationship between these inputs and the output score:\n\n$$\n\\text{score}_{pred} = x_1 w_1 + x_2 w_2 + b\n$$\n\nOur aim is to use stochastic gradient descent to determine the best values for $w_1$, $w_2$, and $b$ such that the mean squared error (MSE) between the actual score ($\\text{score}_{target}$) and the predicted score ($\\text{score}_{pred}$) is minimized:\n\n$$\n\\underset{w_1, w_2, b}{\\mathrm{argmin}} \\; (\\text{score}_{pred} - \\text{score}_{target})^2\n$$\n\nWithout gradient accumulation, we would update the parameters after each batch of student's data.\n\n::: {.panel-tabset}\n\n## Without Gradient Accumulation\n\n```python\n1  def train_no_accumulate(params: ModelParameters, \n2                         num_epochs: int = 10, \n3                         learning_rate: float = 1e-3):\n4      for epoch in range(1, num_epochs + 1):\n5          for (x1, x2), y_target in training_data:\n6  \n7              # Calculate the output of the model\n8              z1 = x1 * params.w1\n9              z2 = x2 * params.w2\n10             y_pred = z1 + z2 + params.b\n11             loss = (y_pred - y_target) ** 2\n12 \n13             # Calculate the gradients of the loss w.r.t. the parameters\n14             loss.backward()\n15 \n16             # Update the parameters (at each iteration)\n17             with torch.no_grad():\n18                 # Equivalent to calling optimizer.step()\n19                 params.w1 -= learning_rate * params.w1.grad\n20                 params.w2 -= learning_rate * params.w2.grad\n21                 params.b  -= learning_rate * params.b.grad\n22 \n23                 # Reset the gradients to zero\n24                 # Equivalent to calling optimizer.zero_grad()\n25                 params.w1.grad.zero_()\n26                 params.w2.grad.zero_()\n27                 params.b.grad.zero_()\n```\n\n:::\n\nWith **gradient accumulation**, instead of updating the parameters after each batch of data, we accumulate gradients across several micro-batches (`micro_batch_size = 3`) and then update all at once. This allows us to train with larger effective batch sizes even if memory is limited.\n\n::: {.panel-tabset}\n\n## With Gradient Accumulation\n```python\n1  def train_accumulate(params: ModelParameters, \n2                       num_epochs: int = 10, \n3                       learning_rate: float = 1e-3, \n4                       micro_batch_size: int = 3):\n5  \n6      for epoch in range(1, num_epochs + 1):\n7          for index, ((x1, x2), y_target) in enumerate(training_data):\n8  \n9              # Calculate the output of the model\n10             z1 = x1 * params.w1\n11             z2 = x2 * params.w2\n12             y_pred = z1 + z2 + params.b\n13             loss = (y_pred - y_target) ** 2\n14 \n15             # Accumulate gradients\n16             loss.backward()\n17 \n18             # If we have processed 3 micro-batches OR reached the end of the dataset\n19             if (index + 1) % micro_batch_size == 0 or index == len(training_data) - 1:\n20                 with torch.no_grad():\n21                     # Equivalent to optimizer.step()\n22                     params.w1 -= learning_rate * params.w1.grad\n23                     params.w2 -= learning_rate * params.w2.grad\n24                     params.b  -= learning_rate * params.b.grad\n25 \n26                     # Reset the gradients = optimizer.zero_grad()\n27                     params.w1.grad.zero_()\n28                     params.w2.grad.zero_()\n29                     params.b.grad.zero_()\n```\n:::\n\nGradient accumulation allows us to reduce activation memory, which grows linearly with batch size, by processing smaller micro-batches sequentially. This reduces stored activations and gradients since only one micro-batch's worth of activations needs to be kept in memory at a time, which helps reduce the overall activation memory footprint.\n\nAgain there is no free lunch. As gradient accumulation requires multiple consecutive forward/backward passes per optimization step, it increases the compute overhead and slows down training. But it allows us to train with larger effective batch sizes even if memory is limited. \n\nSo far, we've seen how techniques like `gradient checkpointing` and `gradient accumulation` help deal with the memory blowup issue caused by `activations`: the dynamic part of memory usage. Both allow us to fit larger models or batches on a **single GPU**, but mostly by working sequentially and slowing down training. However, these don't address the static memory required for parameters, gradients, and optimizer states, nor do they fully utilize available hardware (assume we have more than one GPU). \n\nTo tackle this, we can scale training across multiple GPUs using **Data Parallelism**. By splitting micro-batches and processing them simultaneously on several GPUs, we address both memory and compute bottlenecks, and that is what we will discuss in the next section.\n\n## Scaling with Multiple GPUs: Data Parallelism (DP)\n\nRecall that in Gradient Accumulation, we were processing **micro-batches (MBS)** sequentially. Since these micro-batches are **independent of each other**, we can process them **parallelly on different GPUs**. Something like this, if you see carefully now we are processing the micro-batches in parallel on different GPUs, w.r.t what we did in Gradient Accumulation where we were processing the micro-batches sequentially on a single GPU:\n\n<p align=\"center\">\n  <img src=\"assets/dp.png\" alt=\"Data Parallelism\" width=\"100%\"/>\n</p>\n\n### The Data Parallel Setup\n\nIn a Data Parallel setup, we distribute the data across multiple GPUs, while maintaining a full, redundant replica of the model parameters, gradients, and optimizer states on each GPU. \n\n1.  **Replication:** We maintain a full, redundant **replica** of the model parameters ($\\Phi$), gradients ($\\nabla \\Phi$), and optimizer states ($\\Phi_{\\text{optim}}$) on **each GPU**.\n\n<p align=\"center\">\n  <img src=\"assets/dp_1.png\" alt=\"Data Parallel Setup\" width=\"100%\"/>\n</p>\n\n2.  **Parallel Processing:** Each GPU processes a unique micro-batch simultaneously. This involves **same operations, different data**.\n\n<p align=\"center\">\n  <img src=\"assets/dp_2.png\" alt=\"Data Parallel Setup\" width=\"100%\"/>\n</p>\n\n3.  **Local Computation:** Each GPU performs its forward pass and backward pass locally and independently, resulting in a local gradient ($\\nabla \\Phi_i$).\n\n<p align=\"center\">\n  <img src=\"assets/dp_3.png\" alt=\"Data Parallel Setup\" width=\"100%\"/>\n</p>\n\nIf you look carefully, we can perform the forward pass and the backward pass in parallel on different GPUs. But we cannot perform the optimizer step and update the parameters independently on different GPUs. If we do that, we will end up training N different models on N different GPUs which is not what we want. \n\nSo, after the backward pass, we need to somehow synchronize the gradients across the GPUs. And this is done by the **All-Reduce** primitive. \n\n### Gradient Synchronization: The All-Reduce Primitive\n\nBefore we dive into the All-Reduce operation, it's important to note that NVIDIA provides a rich set of **communication primitives** as part of its distributed training ecosystem such as NCCL(NVIDIA's collective communication library). These primitives simplify and accelerate multi-GPU (and multi-node) communication, enabling efficient synchronization and sharding operations required for large-scale training.\n\n::: {.callout-note title=\"Communication Primitives in Distributed Training\"}\n**All-Reduce** is just one such primitive—used specifically for synchronizing gradients across GPUs at the end of each backward pass in standard Data Parallel training. However, there are several other primitives (like **All-Gather**, **Reduce-Scatter**, **Broadcast**, etc.) designed for different patterns of communication and parallelism. We will encounter and discuss these additional primitives as we explore more advanced parallelization techniques (e.g., ZeRO, model sharding, tensor parallelism) later in the series.\n::: \n\nFor now, let's look at **All-Reduce** in detail, since this is exactly what we need for synchronizing the gradients during Data Parallel training.\n\nSince each GPU computes a gradient based only on its local micro-batch, we must **add them to get the global gradient** before performing the optimization step. The required communication operation is the **All-Reduce** primitive:\n\n* **Input:** Different tensors (the local gradients $\\nabla \\Phi_1, \\nabla \\Phi_2, \\dots$) on each GPU.\n* **Operation:** A reduction operation (usually summation, $F$) is applied to all tensors.\n* **Output:** The result of the reduction (the global gradient $\\sum \\nabla \\Phi_i$) is made available on **all** GPUs.\n\n<p align=\"center\">\n  <img src=\"assets/all_reduce.png\" alt=\"All Reduce\" width=\"70%\"/>\n</p>\n\nOnce every node receives the global gradient, it performs the **`optimizer.step()`** operation independently, ensuring all model copies remain in sync. These collective operations are defined in the **`torch.distributed`** API.\n\nHere I've a machine with 4 T4 GPUs.\n\n```bash\nray@ip-10-0-69-225:code$ nvidia-smi -L\nGPU 0: Tesla T4 (UUID: GPU-31a1b562-c769-c7f1-ede1-48847cec8d53)\nGPU 1: Tesla T4 (UUID: GPU-1beaf204-f6f7-182d-67f8-aee6c58128df)\nGPU 2: Tesla T4 (UUID: GPU-934ca246-df7e-2c7f-4bdd-b07859e46b2d)\nGPU 3: Tesla T4 (UUID: GPU-141171cb-db62-b770-97ff-955f8c7f2265)\n```\n\nNow let's create a simple example to demonstrate the **All-Reduce** operation by creating 4 tensors on each GPU and performing the **All-Reduce** operation on them.\n\n::: {.panel-tabset}\n\n## All-Reduce Example\n```python\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_all_reduce():\n    tensor = torch.tensor([dist.get_rank() + 1] * 3, dtype=torch.float32).cuda()\n    print(f\"Before all_reduce on rank {dist.get_rank()}: {tensor}\")\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    print(f\"After all_reduce on rank {dist.get_rank()}: {tensor}\")\n\n# Initialize the process group and set the device, create a tensor on each GPU and perform the All-Reduce operation on them.\ninit_process()\nexample_all_reduce()\n```\n:::\n\nWe can run this code on 4 GPUs using the following command:\n\n```bash\ntorchrun --nproc_per_node=4 dist_all_reduce.py\n```\n\nWe will get the following output:\n\n```bash\nBefore all_reduce on rank 3: tensor([4., 4., 4.], device='cuda:3')\nBefore all_reduce on rank 0: tensor([1., 1., 1.], device='cuda:0')\nBefore all_reduce on rank 2: tensor([3., 3., 3.], device='cuda:2')\nBefore all_reduce on rank 1: tensor([2., 2., 2.], device='cuda:1')\n\nAfter all_reduce on rank 3: tensor([10., 10., 10.], device='cuda:3') \nAfter all_reduce on rank 0: tensor([10., 10., 10.], device='cuda:0') \nAfter all_reduce on rank 2: tensor([10., 10., 10.], device='cuda:2') \nAfter all_reduce on rank 1: tensor([10., 10., 10.], device='cuda:1') \n```\n\n### Overlapping Communication and Computation\n\nIn a naive DP implementation, the GPUs sit **idle** during the communication phase, as the **All-Reduce** operation begins only after **all** gradients are computed in the backward pass. This is inefficient.\n\n<p align=\"center\">\n  <img src=\"assets/all_reduce.gif\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\nTo eliminate this idle time, we **overlap** the communication and computation.\n\n* **Method:** As soon as the gradient for a specific layer is computed during the backward pass (e.g., $\\nabla L_2$), we immediately trigger the **All-Reduce** for that gradient **in the background**.\n* **Rationale:** The computation of the next layer's gradient ($\\nabla L_1$) is independent of the communication of the previous layer's gradient ($\\nabla L_2$).\n* **Implementation:** This technique is implemented via **hooks** in PyTorch (like `post_accumulate_grad_hook`), allowing the next computation step to proceed while the communication step runs concurrently, significantly improving throughput. It attach an all-reduce hook function to each parameter that requires gradients. So, now it communicates more frequently but in smaller packets.\n\n::: {.panel-tabset}\n\n## Overlapping Communication and Computation\n```python\ndef register_backward_hook(self, hook):\n    \"\"\"\n    Registers a backward hook for all parameters of the model that \n    require gradients.\n    \"\"\"\n    for p in self.module.parameters():\n        if p.requires_grad is True:\n            p.register_post_accumulate_grad_hook(hook)\n```\n:::\n\nBefore while communication was happening, we were waiting for all the gradients to be computed in the backward pass. \n\n<p align=\"center\">\n  <img src=\"assets/dp_overlap1.png\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\nBut now, we are overlapping the communication and computation. So, we are not waiting for all the gradients to be computed in the backward pass. We are computing the gradients for the next layer while the communication for the previous layer is happening.\n\n<p align=\"center\">\n  <img src=\"assets/dp_overlap2.png\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\nWe can infact do better and communicate more efficiently by grouping the gradients into larger buckets and performing the All-Reduce operation on them. Its like packing items into boxes before shipping (have you seen at times while placing an order on Amazon, they offer us to pack multiple items into a single box to save on shipping costs ?, well thats what we are doing here but with gradients).\n\nWith this we can significantly reduce the communication overhead and speed up the computation operations.\n\n<p align=\"center\">\n  <img src=\"assets/dp_overlap3.png\" alt=\"All Reduce\" width=\"100%\"/>\n</p>\n\n## The Limitations of Simple Data Parallelism (DP)\n\nNow, that we have seen how to scale out the training with multiple GPUs using Data Parallelism, we can ask ourselves a question - is this scaling **lossless** ? \n\nThe answer is no. There's a communication overhead associated with the Data Parallelism. And as the degree of data parallelism increases, there's a noticeable drop in tokens per second per GPU (throughput). Although we are overlapping the communication and computation, we are still waiting for the gradients to be computed in the backward pass.\n\nMore importantly, all these discussions about Data Parallelism (DP) so far have assumed that the **entire model** can fit on a single GPU. But what if the model is too large (e.g. GPT-3 with 175B parameters) to fit in the memory of a single GPU (NVIDIA A100 with 80GB of memory)? \n\n<p align=\"center\">\n  <img src=\"assets/model_hw.png\" alt=\"Model Exceeds GPU Memory\" width=\"50%\"/>\n</p>\n\nAs model sizes grow, it becomes common that a single accelerator (GPU in our case) cannot contain all model parameters, optimizer states, and gradients. Therefore, we need to find additional ways to scale training beyond simple DP, which can allow us to train models that **don't fit on a single GPU**.\n\nAnd that is what we will discuss in the next section - **ZeRO** (Zero Redundancy Optimizer).\n\n# ZeRO: Zero Redundancy Optimizer\n\nZeRO (Zero Redundancy Optimizer) is a family of techniques that addresses constrain of static memory (parameters, gradients, optimizer states) on a single GPU. With ZeRO, we can train models that don't fit on a single GPU, and it does that by sharding the static memory components across multiple GPUs.\n\nThis approach is organized into three possible optimization stages:\n\n- **ZeRO-1**: optimizer state sharding\n- **ZeRO-2**: optimizer state + gradient sharding\n- **ZeRO-3**: optimizer state + gradient + parameter sharding\n\n<p align=\"center\">\n  <img src=\"assets/zero.png\" alt=\"ZeRO\" width=\"100%\"/>\n</p>\n\nWithout even going further, you can probably guess that, with this approch we need to do a lot of communication between the GPUs. But as we have seen in the previous section, we can overlap the communication and computation to some extent. So, we can reduce the communication overhead by overlapping the communication and computation.\n\nLet's discuss each of these techniques in detail, starting with **ZeRO-1**.\n\n### ZeRO-1: Sharding Optimizer States\n\nRecall from our earlier discussion, the **static memory footprint** per GPU specifically for **mixed precision (using BF16 + FP32)**:\n\n| Component | Precision | Size ($\\Psi$ Bytes) | Rationale |\n|:---|:---|:---|:---|\n| Model Parameters | BF16 (2 bytes) | $2\\Psi$ | Used for forward and backward passes |\n| Parameter Gradients | BF16 (2 bytes) | $2\\Psi$ | Used in backpropagation |\n| Master Weights | FP32 (4 bytes) | $4\\Psi$ | Full precision copy for the update step |\n| Optimizer States (Adam) | FP32 (4+4 bytes) | $8\\Psi$ | Stores 1st and 2nd moment estimates ($4\\Psi$ each) |\n| **Total Static Memory (with Mixed Precision)** |  | **$16\\Psi$** | **The absolute floor for static storage** |\n\nThe largest part of the static memory comes from the **optimizer states**, especially for optimizers like Adam, which maintain both first and second moment statistics. **With Data Parallelism (DP), all these components are duplicated on every GPU** in the data-parallel group, so each device bears the full cost ($16\\Psi$) of these tensors (ignoring activations for now).\n\n::: {.callout-note title=\"Important Caveat: Master Weights Are Part of Optimizer State Sharding\"}\nWhen discussing the sharding of optimizer states in ZeRO, we must also include the **master weights** (the FP32 copy of model parameters used for the optimizer update) in the sharding calculation. Both the optimizer states and these master weights are stored in FP32, and both are sharded together in ZeRO-1. Thus, when you see references to \"optimizer state sharding,\" this always implicitly includes master weights in modern mixed precision training setups.\n:::\n\nWith **ZeRO-1**, the goal is to _shard_ (that is, partition and spread) the **FP32 optimizer states** and **FP32 master weights** across the $N_d$ GPUs, rather than storing them fully on each device. This introduces the following changes:\n\n- Every GPU stores only a **$1/N_d$ -th slice** of the optimizer states ($8\\Psi$) and master weights ($4\\Psi$), rather than the full $12\\Psi$.\n- The parameter tensors and gradients (in BF16) **remain fully replicated** on each GPU for compatibility with forward and backward passes.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1.png\" alt=\"ZeRO-1\" width=\"100%\"/>\n</p>\n\nSo, **ZeRO-1 directly reduces redundant memory used by both FP32 optimizer states and master weights**. The per-GPU memory for these states drops from $12\\Psi$ down to $\\frac{12\\Psi}{N_d}$. The other (BF16) tensors remain replicated for performance and simplicity.\n\nThe resulting **static memory footprint per GPU** with ZeRO-1 sharding thus becomes:\n$$\n\\mathcal{M}_{\\text{ZeRO-1}} = 2\\Psi + 2\\Psi + \\frac{12\\Psi}{N_d}\n$$\n\n- Parameters (BF16): $2\\Psi$\n- Gradients (BF16): $2\\Psi$\n- Optimizer States + Master Weights (FP32): $\\frac{12\\Psi}{N_d}$\n\nWhere the first two terms represent the fully replicated BF16 weights and gradients, and the last term is the optimizer states and FP32 master weights sharded across $N_d$ GPUs.\n\nJust to make this more concrete, let's look at some practical numbers. \nSuppose you have a modern **A100/H100 GPU** with **80GB** of memory. In DP, the largest model you can fit is roughly:\n\n$$\n\\text{Max Parameters (DP)} = \\frac{80~\\text{GB}}{16~\\text{bytes per param}} \\approx 5~\\text{billion parameters}\n$$\n\nBut if you apply ZeRO-1 with **64 GPUs** ($N_d = 64$), the optimizer state and master weights are now only a small shard per GPU:\n\n- $\\frac{12}{64} \\approx 0.1875$ (so just 1.5GB of optimizer/master weights per GPU for a 5B model)\n- The effective static memory per parameter drops from 16 bytes (DP) to about **4.2 bytes** (ZeRO-1).\n\nSo now, the largest model you can train on that same 80GB GPU jumps to:\n\n$$\n\\text{Max Parameters (ZeRO-1, 64 GPUs)} = \\frac{80~\\text{GB}}{4.2~\\text{bytes per param}} \\approx 19~\\text{billion parameters}\n$$\n\nSo, its great that we can train a larger model on the same hardware, but we still need to discuss the communication overhead associated with this approach. \n\nFor the forward pass, we **don't** need to do any communication, as we have all the parameters in each GPU. \n\n<p align=\"center\">\n  <img src=\"assets/zero-1_1.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nNext in the backward pass, we have all the gradients in each GPU. So, we need to do a **All-Reduce** and at this point we have the same gradients on all the GPUs.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1_2.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nBut now on each GPU, we can _discard_ all the other gradients and _keep only the one_ whose corresponding optimizer state is present on that particular GPU. \n\n<p align=\"center\">\n  <img src=\"assets/zero-1_3.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nAfter this, each GPU can _update_ its respective model parameters to its corresponding optimizer state and gradients.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1_4.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nAnd at this point, we need to communicate again, to get the updated model parameters on all the GPUs, as at this point each GPU has the updated model parameters only for its own shard of the optimizer state.\n\nBut what type of communication do we need to do ? \n\nAs each GPU need to _gather_ the updated model parameters from all the other GPUs. So, we need to do a **All-Gather** operation. And this is another communication primitive like **All-Reduce** which we have seen earlier.\n\n<p align=\"center\">\n  <img src=\"assets/all_gather.png\" alt=\"All Gather\" width=\"70%\"/>\n</p>\n\nLet's quickly see an example of how to do this using `torch.distributed.all_gather()`. Here we are creating a tensor on each GPU and performing the **All-Gather** operation on them.\n\n::: {.panel-tabset}\n\n## All-Gather Example\n```python\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_all_gather():\n    tensor = torch.tensor([dist.get_rank() + 1] * 3, dtype=torch.float32).cuda()\n    # Prepare an output list of tensors for all_gather\n    world_size = dist.get_world_size()\n    gathered = [torch.zeros_like(tensor) for _ in range(world_size)]\n    print(f\"Before all_gather on rank {dist.get_rank()}: {tensor}\")\n    dist.all_gather(gathered, tensor)\n    print(f\"After all_gather on rank {dist.get_rank()}: {[t.cpu().tolist() for t in gathered]}\")\n\n# Initialize the process group and set the device, create a tensor on each GPU and perform the All-Gather operation on them.\ninit_process()\nexample_all_gather()\n```\n:::\n\nJust like before, we can run this code with 4 GPUs:\n\n```bash\ntorchrun --nproc_per_node=4 dist_all_gather.py\n```\n\nTypical output:\n\n```bash\nBefore all_gather on rank 2: tensor([3., 3., 3.], device='cuda:2')\nBefore all_gather on rank 0: tensor([1., 1., 1.], device='cuda:0')\nBefore all_gather on rank 1: tensor([2., 2., 2.], device='cuda:1')\nBefore all_gather on rank 3: tensor([4., 4., 4.], device='cuda:3')\n\nAfter all_gather on rank 0: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\nAfter all_gather on rank 1: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\nAfter all_gather on rank 2: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\nAfter all_gather on rank 3: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]\n```\n\nWith that, each GPU now has the updated model parameters and now they can start with the next batch and start the forward pass.\n\n<p align=\"center\">\n  <img src=\"assets/zero-1_5.png\" alt=\"ZeRO-1 Backward\" width=\"100%\"/>\n</p>\n\nSo, this is how ZeRO-1 strategy works. \n\nNow, if go back and carefully look after _All_Reduce_ operation in ZeRO-1, each GPU discarded all the other gradients and kept only the one whose corresponding optimizer state is present on that particular GPU. \n\nWhich makes us think, why we need to keep all the gradients in all the GPUs in the first place ? Why cant shard the gradients as well along with its corresponding optimizer state. And this is exactly what ZeRO-2 strategy does. \n\n### ZeRO-2: Sharding Gradients\n\nLet's now walk through **ZeRO-2** using the same concrete, step-by-step approach.\n\nRecall our key question from ZeRO-1:\n\n> *After the All-Reduce, why keep all gradients on all GPUs? Can't each GPU just hold the gradients it needs?*\n\nThat's exactly what ZeRO-2 does, it further shards the gradients right alongside the optimizer state and master weights. So, each GPU now only needs to store the gradient shard corresponding to its optimizer state shard. \n\n<p align=\"center\">\n  <img src=\"assets/zero-2.png\" alt=\"ZeRO-2\" width=\"100%\"/>\n</p>\n\nJust as we did for ZeRO-1, let's run the numbers for ZeRO-2 sharding to see the dramatic benefits.\nWith ZeRO-2, the memory formula per GPU now becomes:\n$$\n\\mathcal{M}_{\\text{ZeRO-2}} = 2\\Psi + \\frac{2\\Psi + 12\\Psi}{N_d}\n$$\n\n- Parameters (BF16): $2\\Psi$\n- Gradients (BF16): $\\frac{2\\Psi}{N_d}$\n- Optimizer States + Master Weights (FP32): $\\frac{12\\Psi}{N_d}$\n\nIf we again use an **A100/H100 GPU** with **80GB** of memory, and $N_d = 64$ GPUs, then the largest model we can train would be:\n\n$$\n\\text{Max Parameters (ZeRO-2, 64 GPUs)} = \\frac{80~\\text{GB}}{2.2~\\text{bytes per param}} \\approx 36~\\text{billion parameters}\n$$\n\nLet's put this side-by-side:\n\n| Strategy     | Effective Bytes/Param | Max Model on 80GB GPU |\n|--------------|----------------------|-----------------------|\n| **DP**       | 16                   | ~5B                   |\n| **ZeRO-1**   | 4.2                  | ~19B                  |\n| **ZeRO-2**   | 2.2                  | ~36B                  |\n\nSo, **ZeRO-2 nearly doubles the maximum trainable model size compared to ZeRO-1** (and over 7x compared to vanilla Data Parallelism).\n\nLet's see how the communication overhead changes with ZeRO-2. For the forward pass, we **don't** need to do any communication (like in ZeRO-1), as we have all the parameters in each GPU.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_1.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nNext in the backward pass, instead of performing an **All-Reduce** over the gradients, we only perform a **Reduce-Scatter** operation. Another communication primitive like **All-Reduce** and **All-Gather** which we have seen earlier.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_2.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nSo what **Reduce-Scatter** operation does internally is, its first reducing (summing) the gradients across all the GPUs and then scattering the result to the GPUs that need to have the gradient shard.\n\n<p align=\"center\">\n  <img src=\"assets/reduce_scatter.png\" alt=\"Reduce Scatter\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"Computation-communication timeline\"}\n- **ZeRO-1**: We keep a copy of all gradients.\n- **ZeRO-2**: Communicate and release the gradients on the fly.\n- In practice, both use **`reduce-scatter`** for gradients and **`all-gather`** for FP32 copy of parameters.\n- There is no real overhead to using ZeRO-2 over ZeRO-1 besides implementation complexity, and indeed **ZeRO-2 is usually the better option**.\n:::\n\nWe can see how this works with an example.\n\n::: {.panel-tabset}\n\n## Reduce-Scatter Example\n```python\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_reduce_scatter():\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n\n    # Construct a single input tensor, then split into equal chunks (one for each rank)\n    input_tensor = torch.arange(1, world_size * 3 + 1, dtype=torch.float32).cuda()\n    input_list = list(torch.chunk(input_tensor, world_size))\n    output_tensor = torch.zeros(3, dtype=torch.float32).cuda()\n\n    print(f\"Before reduce_scatter on rank {rank}: {[t.cpu().tolist() for t in input_list]}\")\n    dist.reduce_scatter(output_tensor, input_list, op=dist.ReduceOp.SUM)\n    print(f\"After reduce_scatter on rank {rank}: {output_tensor.cpu().tolist()}\")\n\n# Initialize the process group and set device, then perform Reduce-Scatter\ninit_process()\nexample_reduce_scatter()\n```\n:::\n\nJust like before, you can run this code with 4 GPUs:\n\n```bash\ntorchrun --nproc_per_node=4 dist_reduce_scatter.py\n```\n\nTypical output:\n\n```bash\nBefore reduce_scatter on rank 0: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\nBefore reduce_scatter on rank 1: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\nBefore reduce_scatter on rank 2: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\nBefore reduce_scatter on rank 3: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]\n\nAfter reduce_scatter on rank 0: [4.0, 8.0, 12.0]\nAfter reduce_scatter on rank 1: [16.0, 20.0, 24.0]\nAfter reduce_scatter on rank 2: [28.0, 32.0, 36.0]\nAfter reduce_scatter on rank 3: [40.0, 44.0, 48.0]\n```\n\nEach rank starts with identical chunked inputs. After `reduce_scatter`, every rank gets the sum (across all ranks) of the *i-th chunk*, distributed such that rank 0 gets chunk 0's sum, rank 1 gets chunk 1's sum, etc.\n\nAfter **Reduce-Scatter** operation, each GPU now has the gradient shard corresponding to its optimizer state shard which it can use to update its model parameters.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_3.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nAnd now, we need to communicate again, to get the updated model parameters on all the GPUs, as at this point each GPU has the updated model parameters only for its own shard of the optimizer state.\n\n<p align=\"center\">\n  <img src=\"assets/zero-2_4.png\" alt=\"ZeRO-2 Backward\" width=\"100%\"/>\n</p>\n\nThis is how ZeRO-2 strategy works. We have come a long way from the vanilla Data Parallelism to ZeRO-2, where we have reduced the memory footprint quite significantly, but can we further scale ? And this is exactly what ZeRO-3 strategy does. \n\n### ZeRO-3: Sharding Parameters\n\nZeRO-3 is the most aggressive form of ZeRO, it shards all the static memory components: parameters, gradients, and optimizer states. So, each GPU now only needs to store the parameter shard corresponding to its optimizer state shard.\n\n<p align=\"center\">\n  <img src=\"assets/zero-3.png\" alt=\"ZeRO-3\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"ZeRO-3 vs. FSDP\"}\n\nYou may have seen the terms **ZeRO-3** and **Fully Sharded Data Parallel (FSDP)** used almost interchangeably in literature, blogs, and PyTorch documentation. That's because the underlying strategy is the same: shard parameters, gradients, and optimizer states across GPUs to minimize memory usage per device.\n\n- **ZeRO-3** originated as a theoretical memory optimization described in the [Microsoft DeepSpeed ZeRO paper](https://arxiv.org/abs/1910.02054), outlining **stage 3** of ZeRO by sharding all model state across different GPUs. Its basically a concept implemented in multiple frameworks like DeepSpeed, etc.\n- **FSDP (Fully Sharded Data Parallel)** is the official PyTorch implementation of this idea. FSDP leverages the ZeRO-3 approach and provides a flexible interface for applying parameter, gradient, and optimizer sharding with PyTorch models in both research and production environments.\n:::\n\nWith ZeRO-3, the memory formula per GPU now becomes:\n$$\n\\mathcal{M}_{\\text{ZeRO-3}} = \\frac{2\\Psi + 2\\Psi + 12\\Psi}{N_d} = \\frac{16\\Psi}{N_d}\n$$\n\n- Parameters (BF16): $\\frac{2\\Psi}{N_d}$\n- Gradients (BF16): $\\frac{2\\Psi}{N_d}$\n- Optimizer States + Master Weights (FP32): $\\frac{12\\Psi}{N_d}$\n\nIf we again use an **A100/H100 GPU** with **80GB** of memory, and $N_d = 64$ GPUs, then the largest model we can train would be:\n\n$$\n\\text{Max Parameters (ZeRO-3, 64 GPUs)} = \\frac{80~\\text{GB}}{0.25~\\text{bytes per param}} \\approx 320~\\text{billion parameters}\n$$\n\nLet's put this all side-by-side:\n\n| Strategy     | Effective Bytes/Param | Max Model on 80GB GPU |\n|--------------|----------------------|-----------------------|\n| **DP**       | 16                   | ~5B                   |\n| **ZeRO-1**   | 4.2                  | ~19B                  |\n| **ZeRO-2**   | 2.2                  | ~36B                  |\n| **ZeRO-3**   | 0.25                 | ~320B                 |\n\nAs you can see, **ZeRO-3/FSDP** can 10x the maximum trainable model size compared to **ZeRO-2**, and over 60x compared to **vanilla DP**.\n\nNow, let's see how the communication overhead changes with ZeRO-3. As the model parameters are now sharded, we have a problem, we can not do the forward pass without any communication, we need to do a **All-Gather** operation to first get the full model parameters on all the GPUs.\n\n<p align=\"center\">\n  <img src=\"assets/zero-3_1.png\" alt=\"ZeRO-3 Forward\" width=\"100%\"/>\n</p>\n\nBut after the forward pass, we can _flush_ the model parameters from memory, as we don't need them anymore for the current forward pass (as we can see above). So, although it reduces the memory footprint, it introduces a communication overhead. \n\nSimilarly in the backward pass, we need to gather the parameters as and when needed using **All-Gather** and then perform **Reduce-Scatter** operation to get the gradient shards on all the GPUs as we did in ZeRO-2.\n\n<p align=\"center\">\n  <img src=\"assets/zero-3_2.png\" alt=\"ZeRO-3 Backward\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"ZeRO-3 Communication and Memory Recap\"}\n\nLet’s recap how communication and memory work with ZeRO-3. For the forward pass, since the parameters are fully sharded, we have to **all-gather** the weights whenever we need them, which gives us a communication cost of $\\Psi$. Because those parameters can be released from memory right after the forward usage, we have to all-gather again as needed in the backward pass, so we pay that $\\Psi$ “tax” a second time. And just like ZeRO-2, we need a **reduce-scatter** for the gradients at the end of backward, which adds yet another $\\Psi$ in communication cost. So in total, the communication bill per iteration comes out to $3\\Psi$, a bit higher than the $2\\Psi$ we saw in ZeRO-2.\n\nOn paper, this sounds like a lot of data being moved around, but in practice it’s not too scary! Thanks to *prefetching,* we can overlap these all-gather operations with computation. Typically, while we’re doing the forward for Layer $n$, we can start all-gathering the parameters for Layer $n+1$ in parallel. Similarly, during the backward pass, we can prefetch the next set of weights needed. This overlap keeps things efficient as long as we aren’t cranking DP up to very large scales (as a rough guideline, keeping DP $\\leq$ 512 is usually safe).\n\nFrom the memory perspective, by sharding everything, we’ve boiled the formula down to its most compact form:\n\n$$\n\\mathcal{M}_{\\text{ZeRO-3}} = \\frac{2\\Psi + 2\\Psi + 12\\Psi}{N_d}\n$$\nIncreasing the DP group size keeps reducing per-GPU model memory, but activation memory still requires tricks like checkpointing and grad accumulation, which we discussed earlier.\n:::\n\nOne important point that can be confusing at first: Even though ZeRO-1, ZeRO-2, and ZeRO-3 shard the model, they are all still types of **Data Parallelism**. \n\nEach GPU still processes the entire forward and backward pass of the model on its own batch of data, just like vanilla DP. The main difference is that **ZeRO changes how the model’s parameters and related tensors are stored and managed across GPUs**, which dramatically reduces memory usage but doesn’t change the core idea of Data Parallelism.\n\n<p align=\"center\">\n  <img src=\"assets/dp_summary.png\" alt=\"DP Summary\" width=\"100%\"/>\n</p>\n\n# Introduction to Ray - A Unified AI Compute Engine\n\nNow that we've explored the ZeRO stages and different data parallel strategies, let's discuss how to put these techniques into practice using Ray and PyTorch and, crucially, why Ray is such a good fit for large-scale distributed training in real-world settings.\n\nUntil now, we've mostly focused on how to leverage multiple GPUs within a single machine. However, scaling up modern deep learning requires distributing the training job not just across several GPUs, but often across many different machines as well. This introduces a host of new challenges — from launching and configuring clusters, to monitoring jobs, handling failures, and minimizing the engineering overhead when scaling up and down.\n\nWhen moving to distributed training at scale, several key requirements and challenges emerge in practice:\n\n- **Scalability and Speed:** Training jobs should be able to leverage more GPUs and machines to finish faster, without painful setup.\n- **Easy Infrastructure Management:** We shouldn't need to spend time manually setting up or configuring clusters, whether on cloud or on-premises resources.\n- **Visibility and Monitoring:** It must be easy to track metrics, logs, and failures across all nodes, so debugging and monitoring don’t turn into a bottleneck.\n- **Reliability and Fault Tolerance:** Hardware failures, network issues, or preempted nodes shouldn't force us to restart training from scratch—resilience and checkpointing are critical.\n- **Minimal Code Changes:** Adapting our code for distributed training shouldn't require a major rewrite of training logic.\n\nRay provides solutions for all of these requirements (and more), making it a compelling choice for large-scale distributed deep learning.\n\nThis is why Ray is increasingly popular in the deep learning ecosystem. By abstracting away the pain points of distributed systems, Ray lets us scale from laptop to GPU cluster without major workflow changes or infrastructure headaches. \n\nIn the sections that follow, we'll see concrete code and recipes showing how easy Ray makes it to scale PyTorch training seamlessly, from simple data parallel jobs to advanced ZeRO and FSDP setups. But before that let's spend some time to understand what Ray is and how it works.\n\n**Ray** is an open-source, unified AI Compute Engine designed to scale Python applications, especially AI/ML workloads, from a single laptop to clusters with thousands of machines, all with minimal code changes. At the core of Ray is **Ray Core**, a low-level distributed computing framework featuring a simple, Pythonic API for building and scaling distributed applications.\n\n<p align=\"center\">\n  <img src=\"assets/ray_01.png\" alt=\"Ray\" width=\"100%\"/>\n</p>\n\n## `Ray Core` Primitives\n\nRay Core provides a minimal, yet powerful set of primitives that let you upgrade normal Python code into distributed code with almost no friction. \n\n<p align=\"center\">\n  <img src=\"assets/ray_00.png\" alt=\"Ray Core Primitives\" width=\"100%\"/>\n</p>\n\n::: {.callout-note title=\"Ray Compute Engine\"}\n**Ray** manages the tough parts (task scheduling, node failures, data transfers, etc.) under the hood, so you don’t have to. Basically, as an ML Engineer/Researcher, you can focus on your model and data, and Ray will take care of the rest (which in our case is the madness of distributed systems and distributed training).\n:::\n\n## An example: For `Stateless` Tasks (Tasks)\n\nLet's try to understand the _core primitives_ of Ray, `tasks` and `actors` with an example. Imagine you’re building a simple app where you need to process a batch of images with a slow transformation (invert the colors). \n\nAt first, you write a `for-loop` to process the images sequentially. It works, but it’s sluggish, using only a single CPU core, even if your laptop has eight. What if you need to process hundreds or thousands of images? This is where **Ray** comes in, and a world of instant scalability. \n\nBelow, we walk step-by-step through the journey: from a plain, sequential Python function—painfully slow! to a parallel powerhouse processed by using Ray Tasks, and finally to coordinated, stateful parallelism with Ray Actors. \n\n:::: {.panel-tabset}\n\n### Step 1. The Baseline: Sequential Processing\n\nWe start at square one with a classic slow `for-loop`. Each image is processed one after another, burning a whole second per image. With 8 images, that's 8 seconds to process all the images.\n\n```python\n# sequential_process.py\nimport time\nimport numpy as np\n\ndef process_image(image: np.ndarray) -> np.ndarray:\n    \"\"\"Simulates a slow 1-second filter.\"\"\"\n    time.sleep(1)\n    return 255 - image\n\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\n\nstart_time = time.time()\n# Sequential: 8 images × 1 sec/image = 8 seconds\nresults = [process_image(img) for img in images]\nend_time = time.time()\n\nprint(f\"Processed {len(results)} images in {end_time - start_time:.2f} seconds.\")\n```\nOur code works, but only uses a single core, leaving the rest idle. Not a good situation. Let's try to parallelize it using `Ray Tasks`.\n\n### Step 2. Parallel Ray Task (Fast)\n\nNow, let's use Ray to parallelize the image processing. Ray's `@ray.remote` decorator instantly upgrades your function to run in parallel, one copy per available CPU core. By switching to `.remote()` calls, you launch as many parallel jobs as you have CPUs, and gather results with `ray.get()`.\n\n```python\n# parallel_process.py\nimport ray\nimport time\nimport numpy as np\n\n# 1. Initialize Ray - autodetects & uses all available CPU cores\nray.init()\n\n# 2. Decorate the function as a remote Ray task\n@ray.remote\ndef process_image(image: np.ndarray) -> np.ndarray:\n    \"\"\"Simulates a slow 1-second filter.\"\"\"\n    time.sleep(1)\n    return 255 - image\n\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\n\nstart_time = time.time()\n\n# 3. Launch tasks in parallel; returns list of ObjectRefs (futures)\nresult_refs = [process_image.remote(img) for img in images]\n\n# 4. Wait for and retrieve finished results via ray.get()\nresults = ray.get(result_refs)\nend_time = time.time()\n\n# On an 8-core machine: ~1 second total runtime!\nprint(f\"Processed {len(results)} images in {end_time - start_time:.2f} seconds.\")\n\nray.shutdown()\n```\n**What we did differently?**  \n\n- We decorated the function with `@ray.remote` decorator to make it a remote task.\n- We called the function with `.remote()` to launch it as a remote task.\n- We waited for the results with `ray.get()`.\n- The secret here is the `ObjectRef`: each `.remote()` call sends off a job in the background, while your main code keeps going. When you call `ray.get(result_refs)`, Ray assembles all results when they're ready.\n\n::::\n\n::: {.callout-tip title=\"Ray Speed-Up\"}\n**By adding just `@ray.remote`, `.remote()`, and `ray.get()`, we get a nearly 8x speedup with 8 CPU cores.**\n:::\n\n## Going Further: For `Stateful` Tasks (Actors)\n\nThe beauty of Ray is that it doesn’t just parallelize our work, it also gives us the right tool for **sharing state across those jobs**. Imagine that you want a running tally (say, the total number of pixels processed across all images), but you can’t use a global variable, because each parallel job runs isolated.\n\nWhat you really want is a service: _a live, remote counter_ that all jobs can update in real-time. That’s what is known as an `Actor` in Ray: a class with its own persistent state, living somewhere on the cluster. \n\nLet's see how to create and use an Actor.\n\n:::: {.panel-tabset}\n\n### Ray Actor (Stateful)\n\n```python\n# actor_counter.py\nimport ray\nimport numpy as np\nimport time\n\nray.init()\n\n# 1. Define the stateful service as a Python Class\n@ray.remote\nclass PixelCounter:\n    # The internal state is defined in __init__\n    def __init__(self):\n        self.total_pixels = 0\n\n    # A method to mutate (update) the internal state\n    def add(self, num_pixels: int):\n        self.total_pixels += num_pixels\n\n    # A method to retrieve the internal state\n    def get_total(self) -> int:\n        return self.total_pixels\n\n# 2. Modify the Task to use the Actor Handle\n@ray.remote\ndef process_image_with_actor(image: np.ndarray, counter_actor: \"ActorHandle\"):\n    # This task calls the Actor's add method remotely\n    counter_actor.add.remote(image.size)\n    time.sleep(1)\n    # The image processing logic is here, but omitted for simplicity\n\n# --- Main Script ---\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\nimage_size = images[0].size\nexpected_total = image_size * len(images) # 8 * 300 = 2400\n\n# 3. Create a single instance (the Actor Handle)\ncounter = PixelCounter.remote()\n\n# 4. Launch 8 parallel tasks, passing the Actor Handle to each\ntask_refs = [process_image_with_actor.remote(img, counter) for img in images]\n\n# Wait for all the image processing tasks to complete\nray.get(task_refs)\n\n# 5. Retrieve the final state from the Actor\nfinal_total_ref = counter.get_total.remote()\nfinal_total = ray.get(final_total_ref)\n\nprint(f\"Expected total pixels: {expected_total}\")\nprint(f\"Actual total from actor: {final_total}\")\n\nray.shutdown()\n```\n\n**What we did differently?** \n\n- Our stateless Ray Tasks each process an image, but the Actor (the remote `PixelCounter` class) lives on, safely tallying up the total pixel count as tasks report in.\n- The Actor’s state persists across many requests, letting you coordinate and aggregate information even in a distributed, parallel environment.\n- The only change from earlier?  \n  - Define a `@ray.remote` class (`PixelCounter`).\n  - Pass its handle to your tasks so they can call `add.remote()`.\n\n::::\n\nThis pattern of combining **Ray Tasks** for stateless (think of Python functions), independent work and **Ray Actors** (think of Python classes) for shared state, is the foundation of scalable Python pipelines for any real-world application (not just any AI applications). And we can build anything with this and make it scalable.\n\nRay's primitives empower us to build scalable, reliable, and maintainable distributed applications, without rewriting all our code or micro-managing threads and processes.\n\n## Ray for Different AI Workloads\n\nWhile Ray Core provides the low-level primitives for building distributed applications, it is not the only or always the best option, especially for **specialized AI workloads**. That's why Ray offers higher-level abstractions tailored to specific AI tasks like **data processing, training, hyperparameter search, RL and model serving**. \n\nRay provides advanced AI Libraries such as:\n\n| Ray Library | Purpose | Key Features / Benefits |\n|:---|:---|:---|\n| `Ray Data` | `Scalable Data Ingest, Processing, Inference` | Effortlessly shards and preprocesses massive datasets; streams data efficiently between CPUs (ETL) and GPUs (training/inference) to maximize hardware utilization. |\n| `Ray Train` | `Distributed Training & Fine-Tuning` | Abstracts away multi-node/GPU orchestration and synchronization for PyTorch, TensorFlow, etc., without the need for boilerplate or manual sync. |\n| `Ray Tune` | `Scalable Hyperparameter Search` | Coordinates and manages hyperparameter trials (search, early stopping, scheduling) across a cluster; includes experiment tracking and best-model picking. |\n| `Ray Serve` | `Fast, Programmable Model Serving` | Deploys models and logic as microservices with auto-scaling; supports model composition and features like traffic splitting and versioning. |\n| `Ray RLlib` | `Scalable Reinforcement Learning` | Provides a comprehensive library for training and evaluating RL algorithms. |                                                                      |\n\nThese libraries are built on top of `Ray Core` and offer a more user-friendly interface for building distributed applications. \n\n::: {.callout-tip title=\"Why Use Higher-Level Ray Libraries?\"}\nIf Ray Core’s Tasks and Actors are this powerful, why bother with higher-level Ray libraries like Ray Train or Ray Data?\n\nThe answer: **Abstraction and Specialization**. While it’s technically possible to build a full distributed training pipeline with only Ray Core, that approach means you shoulder all the complexities—manual data sharding, synchronization (e.g., for PyTorch DDP), distributed checkpointing, fault tolerance, handling resuming, and hyperparameter search. That’s a lot of boilerplate and risk!\n:::\n\nNot only that it also have tight integration with popular frameworks like `PyTorch`, `vLLM`, `Hugging Face`, and more.\n\n<p align=\"center\">\n  <img src=\"assets/ray_02.png\" alt=\"Ray\" width=\"100%\"/>\n</p>\n\nThis unified ecosystem empowers us to build end-to-end distributed AI workflows. Every component from ingest to training to serving, scales seamlessly from our laptop to a full cluster. \n\nIn this blog, we’ll focus on distributed training with `Ray Train`, showing how it can scale our PyTorch training from a single GPU to a full cluster almost effortlessly.\n\n::: {.callout-warning title=\"Why Not Use Only PyTorch Distributed?\"}\n\nWhile **PyTorch Distributed** (such as DDP) is an excellent built-in solution for multi-GPU training, it's primarily designed for *single-node* or *homogeneous, tightly-coupled clusters*. If you're just scaling to multiple GPUs on one machine, PyTorch's distributed tools are often enough. And you can run your training job with `torchrun` command.\n\nHowever, **the challenges multiply dramatically the moment you want to scale across several machines or need to orchestrate complex workflows**. Tasks like:\n\n- Launching jobs and synchronizing them across machines,\n- Managing different workers, node failures, and resuming or monitoring experiments,\n- Efficiently using both CPUs (for pre-processing and data loading) and GPUs (for training/inference) in one seamless workflow,\n- Sharding and streaming large datasets not just for one epoch, but for repeated, distributed, and fault-tolerant training,\n\nbecome **painful and complex** with only PyTorch’s stock tools. \n\nThis is exactly where **Ray shines**.  \n\nIt's abstracts away the low-level engineering required to run distributed workloads at scale. For instance, with **Ray Train** and **Ray Data**, you get seamless multi-GPU, multi-node orchestration, unified CPU-GPU pipelines, resilience and scalability. This helps you focus on your algorithms and models, rather than the underlying infrastructure. \n\n:::\n\n# Distributed Training with Ray Train and PyTorch \n\nNow that we have understood the basics of Ray and how it can help us scale any application, let's now dive deep into distributed training with Ray Train and PyTorch. \n\n<p align=\"center\">\n  <img src=\"assets/ray_03.png\" alt=\"Distributed Training with Ray Train and PyTorch\" width=\"100%\"/>\n</p>\n\nBefore diving into distributed training, let's establish a baseline by looking at a simple single-GPU training loop. This will help us understand what needs to change when we migrate to distributed training.\n\nBefore we dive into distributed training, let's first look at a **simple, single-GPU PyTorch training loop** as our baseline. This example will help ground the core steps that appear in almost any deep learning project, and by keeping things simple (no distributed libraries, no argument parsing), we make it crystal clear what needs to change later for multi-GPU or multi-node scaling.\n\n## Single-GPU PyTorch Training on CIFAR-10\n\nWe'll use a modern Vision Transformer model (`torchvision.models.VisionTransformer`) and the CIFAR-10 dataset. This code works on CPU, GPU (CUDA), or Apple MPS — but it's strictly ordinary, non-distributed PyTorch.\n\nIf you have trained any model with PyTorch on a single machine, you might have seen a similar training loop. The standard way to train a model with PyTorch on a single machine (single GPU) consists of these steps:\n\n1. Download and prepare the `dataset`\n    - Set up `data loaders`\n2. Define the `model` \n    - Move the model to the available device (GPU, MPS, or CPU).\n3. Set up `optimizer` and `loss`.\n4. Run the `training loop`.\n    - Iterate over training data to update model weights.\n    - Check accuracy on validation data.\n5. Optionally, `checkpoint` the model at the end.\n\nThis works perfectly for *one GPU or a single machine*, but doesn't scale automatically. We'll see soon how to migrate this to Ray Train for scaling, but for now, here's the basic setup.\n\n### DataLoader Function\n\nThis function sets up the [DataLoaders](https://pytorch.org/docs/stable/data.html) for the CIFAR-10 training and test splits. Note: We use a `FileLock` to avoid concurrency issues if datasets are being downloaded.\n\n```python\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Normalize, ToTensor\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport os\n\ndef get_dataloaders(batch_size):\n    \"\"\"\n    Create standard PyTorch DataLoaders.\n    No distributed code — just vanilla PyTorch.\n    \"\"\"\n\n    transform = transforms.Compose([\n        ToTensor(),\n        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_data = datasets.CIFAR10(\n            root=\"~/data\", train=True, download=True, transform=transform,\n        )\n        test_data = datasets.CIFAR10(\n            root=\"~/data\", train=False, download=True, transform=transform,\n        )\n\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size)\n\n    return train_loader, test_loader\n```\n\n### Training Function\n\nThis is the standard PyTorch training loop, using `torchvision.models.VisionTransformer`. \n\n```python\nfrom torchvision.models import VisionTransformer\nfrom torch import nn\nimport torch\nfrom tqdm import tqdm\n\ndef train_func(lr=1e-3, epochs=10, batch_size=512):\n    \"\"\"\n    Main training function: single machine, single GPU.\n    \"\"\"\n    # Get data loaders\n    train_loader, val_loader = get_dataloaders(batch_size=batch_size)\n\n    # Create the model\n    model = VisionTransformer(\n        image_size=32,   # CIFAR-10 images are 32x32\n        patch_size=4,    # Reasonable patch size for CIFAR-10\n        num_layers=12,   # Transformer layers\n        num_heads=8,     # Attention heads\n        hidden_dim=384,  # Model width\n        mlp_dim=768,     # Transformer MLP dim\n        num_classes=10   # CIFAR-10\n    )\n\n    # Move model to correct device (GPU/MPS/CPU)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    model.to(device)\n\n    # Set up loss and optimizer\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    # Training loop\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for X, y in tqdm(train_loader, desc=f\"Train Epoch {epoch + 1}\"):\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n\n        # Validation phase\n        model.eval()\n        val_loss, num_correct, num_total = 0, 0, 0\n        with torch.no_grad():\n            for X, y in tqdm(val_loader, desc=f\"Valid Epoch {epoch + 1}\"):\n                X, y = X.to(device), y.to(device)\n                pred = model(X)\n                loss = loss_fn(pred, y)\n                val_loss += loss.item()\n                num_total += y.shape[0]\n                num_correct += (pred.argmax(1) == y).sum().item()\n        val_loss /= len(val_loader)\n        accuracy = num_correct / num_total\n\n        print(f\"  Train Loss: {train_loss:.4f} | Valid Loss: {val_loss:.4f} | Accuracy: {accuracy:.4f} ({100 * accuracy:.2f}%)\")\n\n    # Optional: Save checkpoint\n    checkpoint = {\n        'epoch': epochs,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'accuracy': accuracy,\n    }\n    torch.save(checkpoint, 'checkpoint_single_machine.pth')\n    print(f\"\\nTraining completed! Final accuracy: {100 * accuracy:.2f}%\\nCheckpoint saved to checkpoint_single_machine.pth\")\n```\n\n### Training the Model\nLet's now train the model on a single GPU.\n\n```python\ntrain_func(lr=1e-3, epochs=10, batch_size=512)\n```\n\n<p align=\"center\">\n  <img src=\"assets/gpu_01.png\" alt=\"Training the Model\" width=\"100%\"/>\n</p>\n\nAs you can see, the model is trained on a single GPU while other GPUs are idle. \n\nThis script represents *plain vanilla* PyTorch, suitable for a single GPU or single CPU. There is no distributed logic or Ray involved yet. All of the key logic, especially the `get_dataloaders` function and the structure of the `train_func` will remain mostly the same when we migrate to distributed training with Ray Train!  \n\nLet's now train the model at scale on multiple GPUs across multiple machines.\n\n\n## Distributed Training with Ray Train\n\nNow, let's see how to migrate this `single-machine, single-GPU` training loop to `distributed training` using Ray Train and PyTorch on `multiple machines, multiple GPUs`. \n\n<p align=\"center\">\n  <video controls width=\"700\">\n    <source src=\"assets/ray_train_01.mov\" type=\"video/mp4\">\n    Your browser does not support the video tag.\n  </video>\n  <br>\n  <em>Distributed Training with Ray Train Key Concepts.</em>\n</p>\n\n### Ray Train Architecture  \n\nRay Train's architecture is based on the following components:  \n1. A Ray Train Controller/Driver that schedules the training workers, handles errors, and manages checkpoints  \n2. Ray Train Workers that execute the training code\n\n<p align=\"center\">\n  <img src=\"assets/ray_train__01.png\" width=\"700\"/>\n</p>\n\nBelow are the key API concepts of Ray Train:\n\n1. `train_loop_per_worker`: The core function that contains your model training logic  \n2. `ScalingConfig`: Specifies the number of workers and compute resources (CPUs, GPUs, TPUs)  \n3. `Trainer`: Manages the training process  \n4. `Trainer.fit()`: Starts the distributed training job\n\n<p align=\"center\">\n  <img src=\"assets/ray_train_02.png\" width=\"700\" loading=\"lazy\"/>\n</p>\n\n### Ray Data + Ray Train Integration\n\nHere is a diagram showing the Ray Data and Ray Train integration.\n\n<p align=\"center\">\n  <img src=\"assets/ray_train_03.png\" width=\"800\" loading=\"lazy\"/>\n</p>\n\nWe are not going to go into the details of Ray Data and Ray Train integration in this post. But if you are interested in learning more about it, you can check out the [Ray Data](https://docs.ray.io/en/latest/data/index.html) and [Ray Train](https://docs.ray.io/en/latest/train/index.html) documentation.\n\n\n## Setup the Environment\n\nTo check how many GPUs (and CPUs) are available in your Ray cluster, you can use the script below:\n\n::: {.panel-tabset}\n\n### Check Cluster GPUs\n\n```python\nimport ray\nimport torch\n\ndef check_cluster_gpus():\n    \"\"\"Check GPU count in the entire Ray cluster.\"\"\"\n    # Initialize Ray if not already initialized\n    if not ray.is_initialized():\n        ray.init()\n\n    # Get cluster resources (total GPUs in cluster)\n    cluster_resources = ray.cluster_resources()\n    total_gpus = cluster_resources.get(\"GPU\", 0)\n\n    # Get available resources (currently available GPUs)\n    available_resources = ray.available_resources()\n    available_gpus = available_resources.get(\"GPU\", 0)\n\n    # Get local GPU count (GPUs on this node only)\n    local_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n\n    # Print results\n    print(\"\\n\" + \"=\"*60)\n    print(\"Ray Cluster GPU Information\")\n    print(\"=\"*60)\n    print(f\"Total GPUs in cluster:     {int(total_gpus)}\")\n    print(f\"Available GPUs in cluster: {int(available_gpus)}\")\n    print(f\"Local GPUs (head node):    {local_gpus}\")\n    print(\"=\"*60)\n\n    # Additional cluster info\n    print(\"\\nCluster Resources:\")\n    print(f\"  CPUs (total):     {int(cluster_resources.get('CPU', 0))}\")\n    print(f\"  CPUs (available): {int(available_resources.get('CPU', 0))}\")\n\n    # Show node details if available\n    try:\n        nodes = ray.nodes()\n        print(f\"\\nCluster Nodes: {len(nodes)}\")\n        for i, node in enumerate(nodes):\n            node_resources = node.get('Resources', {})\n            node_gpus = node_resources.get('GPU', 0)\n            print(f\"  Node {i+1}: {int(node_gpus)} GPU(s)\")\n    except Exception as e:\n        print(f\"\\nNote: Could not retrieve node details: {e}\")\n\n    print()\n    return {\n        'total_gpus': int(total_gpus),\n        'available_gpus': int(available_gpus),\n        'local_gpus': local_gpus\n    }\n\nif __name__ == '__main__':\n    check_cluster_gpus()\n```\n:::\n\nAs you can see, the cluster which I have has a total of 8 GPUs. The cluster consists of a total of one Head Node and two Worker Nodes, where each worker node has 4 GPUs each.\n\n```bash\n============================================================\nRay Cluster GPU Information\n============================================================\nTotal GPUs in cluster:     8\nAvailable GPUs in cluster: 8\nLocal GPUs (head node):    0\n============================================================\n\nCluster Resources:\n  CPUs (total):     96\n  CPUs (available): 96\n\nCluster Nodes: 3\n  Node 1: 0 GPU(s)\n  Node 2: 4 GPU(s)\n  Node 3: 4 GPU(s)\n```\n## Distributed Training with Ray Train and PyTorch FSDP\n\nNow that we have understood the basics of Ray Train, and also have a Ray cluster ready, let's now dive into distributed training with Ray Train and PyTorch FSDP.\n\n### 1. Specify Cluster Scaling\n\nFirst, set up how many Ray workers (processes) will participate—typically one per GPU. For a Ray cluster with 8 GPUs:\n\n```python\nscaling_config = ScalingConfig(\n    num_workers=8,  # e.g., 8 GPUs in your cluster\n    use_gpu=True,\n    resources_per_worker={\"CPU\": 2, \"GPU\": 1},\n)\n```\n\n### 2. Data Preparation: PyTorch DataLoaders\n\nData preparation is unchanged from typical PyTorch or DDP usage. Use your usual transforms and DataLoader logic:\n\n```python\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport os\n\ndef get_dataloaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_ds = datasets.CIFAR10(\"~/data\", train=True, download=True, transform=transform)\n        valid_ds = datasets.CIFAR10(\"~/data\", train=False, download=True, transform=transform)\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(valid_ds, batch_size=batch_size),\n    )\n```\n\nNo special considerations are needed for FSDP at this stage.\n\n### 3. Define the Training Function\n\nDefine your Ray Train worker function to train the model. This is the training function that will be executed by each worker. As the model is now being prepared for FSDP, we need to use the `prepare_model` function to prepare the model for FSDP. \n\n```python\nimport os\nimport torch\nfrom torch import nn\nfrom torchvision.models import VisionTransformer\nimport ray\nimport tempfile\n\ndef train_func_per_worker(config):\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    ctx = ray.train.get_context()\n    rank = ctx.get_world_rank()\n    world_size = ctx.get_world_size()\n\n    if rank == 0:\n        print(f\"Training with FSDP across {world_size} workers...\")\n\n    # Prepare DataLoaders for distributed training\n    train_dl, valid_dl = get_dataloaders(batch_size)\n    train_dl = ray.train.torch.prepare_data_loader(train_dl)\n    valid_dl = ray.train.torch.prepare_data_loader(valid_dl)\n\n    # Define the model\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n\n    # Prepare the model for FSDP\n    model = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, sample_cnt = 0.0, 0\n        for X, y in train_dl:\n            pred = model(X)\n            loss = criterion(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * X.shape[0]\n            sample_cnt += X.shape[0]\n        train_loss = total_loss / sample_cnt\n\n        # Validation loop\n        model.eval()\n        valid_loss, correct, total = 0.0, 0, 0\n        with torch.no_grad():\n            for X, y in valid_dl:\n                pred = model(X)\n                valid_loss += criterion(pred, y).item() * X.shape[0]\n                total += y.shape[0]\n                correct += (pred.argmax(dim=1) == y).sum().item()\n        valid_loss /= total\n        acc = correct / total\n\n        if rank == 0:\n            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} Valid Loss={valid_loss:.4f} Acc={acc:.3%}\")\n\n        metrics = {\"epoch\": epoch+1, \"train_loss\": train_loss, \"valid_loss\": valid_loss, \"accuracy\": acc}\n        # Checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            with tempfile.TemporaryDirectory() as ckpt_dir:\n                torch.save(model.module.state_dict(), os.path.join(ckpt_dir, \"model.pt\"))\n                ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(ckpt_dir))\n        else:\n            ray.train.report(metrics)\n```\n\n::: {.callout-note title=\"Change the parallel strategy to DDP\"}\nTo change the parallel strategy to DDP, simply change the parameter to `\"ddp\"`:\n```python\nmodel = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n```\n:::\n\n### 4. Configure Run Checkpointing and Storage\n\nUse Ray's checkpointing utilities to save the best results and recoverable states:\n\n```python\nfrom ray.train import RunConfig, CheckpointConfig\n\ncheckpoint_config = CheckpointConfig(\n    num_to_keep=2,\n    checkpoint_score_attribute=\"accuracy\",\n    checkpoint_score_order=\"max\",\n)\nrun_config = RunConfig(\n    name=\"cifar10_fsdp_example\",\n    storage_path=\"/mnt/cluster_storage/training/\",  # Use a persistent/shared location\n    checkpoint_config=checkpoint_config,\n)\n```\n\n### 5. Launch Training with TorchTrainer\n\nBring all the configs together and kick off distributed training:\n\n```python\nfrom ray.train.torch import TorchTrainer\n\nglobal_batch_size = 1024\nnum_workers = 8   \nbatch_size_per_worker = global_batch_size // num_workers\n\ntrain_loop_config = {\n    \"lr\": 1e-3,\n    \"epochs\": 20,\n    \"batch_size_per_worker\": batch_size_per_worker,\n}\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nprint(\"Starting FSDP distributed training...\")\nresult = trainer.fit()\n```\n\n### 6. Load and Use Checkpoints\n\nOur model can be restored from the best checkpoint after training:\n\n```python\nimport torch\nfrom torchvision.models import VisionTransformer\nimport os\n\nckpt = result.checkpoint\nwith ckpt.as_directory() as ckpt_dir:\n    model_path = os.path.join(ckpt_dir, \"model.pt\")\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    model.load_state_dict(state_dict)\n```\n\nHere is the complete code for the training script:\n\n::: {.panel-tabset}\n\n## train_fsdp.py\n\n```python\nimport os\nimport tempfile\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom torchvision.models import VisionTransformer\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport ray\nfrom ray.train import ScalingConfig, RunConfig, CheckpointConfig\nfrom ray.train.torch import TorchTrainer\n\ndef get_dataloaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_data = datasets.CIFAR10(root=\"~/data\", train=True, download=True, transform=transform)\n        valid_data = datasets.CIFAR10(root=\"~/data\", train=False, download=True, transform=transform)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_data, batch_size=batch_size)\n    return train_loader, valid_loader\n\ndef train_func_per_worker(config):\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    ctx = ray.train.get_context()\n    world_size = ctx.get_world_size()\n    local_rank = ctx.get_world_rank()\n    if local_rank == 0:\n        print(f\"FSDP Training on {world_size} workers\")\n\n    train_loader, valid_loader = get_dataloaders(batch_size)\n    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n    valid_loader = ray.train.torch.prepare_data_loader(valid_loader)\n\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n    # [FSDP] Key change from DDP:\n    model = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, n = 0.0, 0\n        for X, y in train_loader:\n            pred = model(X)\n            loss = criterion(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * X.shape[0]\n            n += X.shape[0]\n        train_loss /= n\n\n        model.eval()\n        correct, total, valid_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for X, y in valid_loader:\n                pred = model(X)\n                valid_loss += criterion(pred, y).item() * X.shape[0]\n                total += y.shape[0]\n                correct += (pred.argmax(dim=1) == y).sum().item()\n        valid_loss /= total\n        accuracy = correct / total\n        metrics = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"valid_loss\": valid_loss,\n            \"accuracy\": accuracy\n        }\n        # Save a checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            with tempfile.TemporaryDirectory() as tmp_ckpt_dir:\n                torch.save(model.module.state_dict(),\n                           os.path.join(tmp_ckpt_dir, \"model.pt\"))\n                ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(tmp_ckpt_dir))\n        else:\n            ray.train.report(metrics)\n\nscaling_config = ScalingConfig(num_workers=8, use_gpu=True, resources_per_worker={\"CPU\": 2, \"GPU\": 1})\ncheckpoint_config = CheckpointConfig(num_to_keep=2, checkpoint_score_attribute=\"accuracy\", checkpoint_score_order=\"max\")\nrun_config = RunConfig(name=\"cifar10_fsdp_example\", storage_path=\"/mnt/cluster_storage/training/\", checkpoint_config=checkpoint_config)\n\nglobal_batch_size = 1024\nbatch_size_per_worker = global_batch_size // scaling_config.num_workers\ntrain_loop_config = {\"lr\": 1e-3, \"epochs\": 20, \"batch_size_per_worker\": batch_size_per_worker}\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()\n```\n:::\n\nWhile the training is runing, we can see the progress in the Ray dashboard. We can now see that all 8 GPUs are being used for training.\n\n<p align=\"center\">\n    <img src=\"assets/gpu_02.png\" alt=\"Ray Train FSDP Dashboard\" width=\"100%\">\n</p>\n\n## Conclusion\n\nDistributed training from scratch can be daunting, but modern frameworks like Ray Train, combined with PyTorch features such as Fully Sharded Data Parallel (FSDP), make scalable and efficient training approachable and practical. We demonstrated how to set up a distributed pipeline for finetuning a model on multiple GPUs using Ray Train and PyTorch FSDP. \n\nLeveraging Ray's abstractions, you can efficiently utilize your compute resources, monitor experiments, and scale training seamlessly. If you need to push scalability further, techniques such as Pipeline Parallelism, Tensor Parallelism, and Sequence Parallelism can be explored, each offering unique benefits for even larger model and data workloads.\n\n## References & Further Resources\n\n- [Ray Train Documentation](https://docs.ray.io/en/latest/train/index.html)\n- [PyTorch Distributed FSDP](https://pytorch.org/docs/stable/fsdp.html)\n- [Ray AIR: Scalable ML on Ray](https://docs.ray.io/en/latest/ray-air/getting-started.html)\n- [Train Deep Learning Models with Ray](https://docs.ray.io/en/latest/train/dl_guide.html)\n- [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)\n- [DeepSpeed: High-Performance Deep Learning Optimization Library](https://www.deepspeed.ai/)\n- [Ray Data for Large-Scale Data Ingestion & Preprocessing](https://docs.ray.io/en/latest/data/index.html)\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":["cosmo","brand"],"title-block-banner":true,"title":"Distributed Training From Scratch","author":"Suman Debnath","date":"2025-11-30","categories":["distributed-training","deep-learning","ray","pytorch"],"image":"assets/ray.jpg","description":"A comprehensive guide to understanding and implementing distributed training for deep learning models from first principles"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Suman Debnath">
<meta name="dcterms.date" content="2025-11-15">
<meta name="description" content="A comprehensive guide to understanding and implementing distributed training for deep learning models from first principles">

<title>Distributed Training From Scratch – My Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/debnsuma"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/_sumand"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Distributed Training From Scratch</h1>
                  <div>
        <div class="description">
          A comprehensive guide to understanding and implementing distributed training for deep learning models from first principles
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">distributed-systems</div>
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">ray</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Suman Debnath </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 15, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#deep-learning-training-basics" id="toc-deep-learning-training-basics" class="nav-link" data-scroll-target="#deep-learning-training-basics">Deep Learning Training Basics</a></li>
  <li><a href="#bottlenecks-in-single-gpu-training" id="toc-bottlenecks-in-single-gpu-training" class="nav-link" data-scroll-target="#bottlenecks-in-single-gpu-training">Bottlenecks in Single-GPU Training</a>
  <ul class="collapse">
  <li><a href="#static-memory" id="toc-static-memory" class="nav-link" data-scroll-target="#static-memory">Static Memory</a></li>
  <li><a href="#dynamic-memory" id="toc-dynamic-memory" class="nav-link" data-scroll-target="#dynamic-memory">Dynamic Memory</a></li>
  </ul></li>
  <li><a href="#batch-size-intuition" id="toc-batch-size-intuition" class="nav-link" data-scroll-target="#batch-size-intuition">Batch Size Intuition</a></li>
  <li><a href="#memory-usage-in-transformer" id="toc-memory-usage-in-transformer" class="nav-link" data-scroll-target="#memory-usage-in-transformer">Memory usage in Transformer</a>
  <ul class="collapse">
  <li><a href="#solution-1-activation-recomputation" id="toc-solution-1-activation-recomputation" class="nav-link" data-scroll-target="#solution-1-activation-recomputation">Solution 1: Activation Recomputation</a></li>
  <li><a href="#solution-2-gradient-accumulation" id="toc-solution-2-gradient-accumulation" class="nav-link" data-scroll-target="#solution-2-gradient-accumulation">Solution 2: Gradient Accumulation</a></li>
  </ul></li>
  <li><a href="#scaling-with-multiple-gpus-data-parallelism-dp" id="toc-scaling-with-multiple-gpus-data-parallelism-dp" class="nav-link" data-scroll-target="#scaling-with-multiple-gpus-data-parallelism-dp">Scaling with Multiple GPUs: Data Parallelism (DP)</a>
  <ul class="collapse">
  <li><a href="#the-data-parallel-setup" id="toc-the-data-parallel-setup" class="nav-link" data-scroll-target="#the-data-parallel-setup">The Data Parallel Setup</a></li>
  <li><a href="#gradient-synchronization-the-all-reduce-primitive" id="toc-gradient-synchronization-the-all-reduce-primitive" class="nav-link" data-scroll-target="#gradient-synchronization-the-all-reduce-primitive">Gradient Synchronization: The All-Reduce Primitive</a></li>
  <li><a href="#overlapping-communication-and-computation" id="toc-overlapping-communication-and-computation" class="nav-link" data-scroll-target="#overlapping-communication-and-computation">Overlapping Communication and Computation</a></li>
  </ul></li>
  <li><a href="#the-limitations-of-simple-data-parallelism-dp" id="toc-the-limitations-of-simple-data-parallelism-dp" class="nav-link" data-scroll-target="#the-limitations-of-simple-data-parallelism-dp">The Limitations of Simple Data Parallelism (DP)</a></li>
  <li><a href="#zero-zero-redundancy-optimizer" id="toc-zero-zero-redundancy-optimizer" class="nav-link" data-scroll-target="#zero-zero-redundancy-optimizer">ZeRO: Zero Redundancy Optimizer</a>
  <ul class="collapse">
  <li><a href="#zero-1-sharding-optimizer-states" id="toc-zero-1-sharding-optimizer-states" class="nav-link" data-scroll-target="#zero-1-sharding-optimizer-states">ZeRO-1: Sharding Optimizer States</a></li>
  <li><a href="#zero-2-sharding-gradients" id="toc-zero-2-sharding-gradients" class="nav-link" data-scroll-target="#zero-2-sharding-gradients">ZeRO-2: Sharding Gradients</a></li>
  <li><a href="#zero-3-sharding-parameters" id="toc-zero-3-sharding-parameters" class="nav-link" data-scroll-target="#zero-3-sharding-parameters">ZeRO-3: Sharding Parameters</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Assume that you are preparing for a marathon. If you plot <strong>your performance</strong> versus <strong>amount of training hours you put in</strong>, the relationship is non-linear, performance gains begin to diminish after a certain point (as we will hit the human capacity).</p>
<p align="center">
<img src="assets/performance-vs-training-hours.png" alt="Performance vs Training Hours" width="50%">
</p>
<p>This concept holds true for LLMs as well. For example, in the LLaMA family (shown below), as the model size increases, the performance improves but the training time increases. The size of the circle represents the size of the model. And here we can see that as the model size increases, the performance improves but the training time increases.</p>
<p align="center">
<img src="assets/model-size-vs-performance.png" alt="Model Size vs Performance" width="70%">
</p>
<p>But then it requires more training. The number of GPU hours required for training are really huge. If you see the <code>y-axis</code>, it is measured in <code>millions of GPU hours</code>. So how are we supposed to do it in one lifetime if we were to do it on one GPU? It is not feasible.</p>
<p>So, we need to focus on how we can <strong>efficiently exploit multiple GPUs</strong> using different forms of parallelism. And in this blog, we will look at how to do that and what are the different forms of parallelism available and how we can implement them from scratch using <code>PyTorch</code> and later on we will use <code>Ray</code> to scale the training.</p>
</section>
<section id="deep-learning-training-basics" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-training-basics">Deep Learning Training Basics</h2>
<p>Before diving into scaling, let’s quickly review the standard model training loop, such as a simple <strong>Multi-Layer Perceptron (MLP)</strong>:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Pseudo Code for the Training Loop</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>  model <span class="op">=</span> MLP().to(device)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>  optimizer <span class="op">=</span> Adam(model.parameters())</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>  criterion <span class="op">=</span> CrossEntropyLoss()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>  data_loader <span class="op">=</span> DataLoader(dataset)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>  </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>  <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>      model.train()  </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span>      <span class="cf">for</span> inputs, targets <span class="kw">in</span> data_loader:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span>          <span class="co"># 1. Move batch to GPU</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span>         inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span>         </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="dv">12</span>         <span class="co"># 2. Clear gradients</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="dv">13</span>         optimizer.zero_grad()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="dv">14</span>         </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span>         <span class="co"># 3. Forward pass</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="dv">16</span>         outputs <span class="op">=</span> model(inputs)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="dv">17</span>         loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="dv">18</span>         </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="dv">19</span>         <span class="co"># 4. Backpropagation</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="dv">20</span>         loss.backward()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="dv">21</span>         </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="dv">22</span>         <span class="co"># 5. Optimization</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span>         optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>In a typical training loop, after defining the model, the optimizer, the loss function, and the data loader, the training loop trains the model by performing the following steps for each epoch:</p>
<ol type="1">
<li>Iterate over the data in mini-batches (<code>line 6-8</code>).</li>
<li>Move each batch of data to the GPU (<code>line 9-10</code>).</li>
<li>Zero out any previous gradients (<code>line 12-13</code>).</li>
<li>Perform a forward pass to calculate the model outputs and loss (<code>line 15-17</code>).</li>
<li>Compute gradients through backpropagation (<code>line 18-19</code>).</li>
<li>Update the model parameters using the optimizer (<code>line 20-21</code>).</li>
</ol>
<p align="center">
<img src="assets/single-gpu.png" alt="Single GPU Training Loop" width="70%">
</p>
<p>This pattern is the core of most deep learning training routines.</p>
</section>
<section id="bottlenecks-in-single-gpu-training" class="level2">
<h2 class="anchored" data-anchor-id="bottlenecks-in-single-gpu-training">Bottlenecks in Single-GPU Training</h2>
<p>When training deep learning models on a single GPU, high-bandwidth memory (HBM) is utilized by four main types of data:</p>
<ol type="1">
<li><p><strong>Model Parameters</strong> (<span class="math inline">\(\Phi\)</span>):<br>
The weights being learned during training.</p></li>
<li><p><strong>Parameter Gradients</strong> (<span class="math inline">\(\nabla \Phi\)</span>):<br>
The gradients computed during backpropagation, required for parameter updates.</p></li>
<li><p><strong>Optimizer States</strong> (<span class="math inline">\(\Phi_{\text{optim}}\)</span>):<br>
Auxiliary variables needed by the optimization algorithm, such as momentum and variance estimates (e.g., in Adam).</p></li>
<li><p><strong>Activations</strong> (<span class="math inline">\(\mathcal{M}_{\text{act}}\)</span>):<br>
The intermediate outputs from each neural network layer required to compute gradients during the backward pass.</p></li>
</ol>
<p>Of these, the first three (<strong>Parameters</strong>, <strong>Gradients</strong>, and <strong>Optimizer States</strong>) are considered <strong>static components</strong>. They collectively define the minimum “static” memory footprint determined by the model architecture itself.</p>
<p>The fourth component, <strong>Activations</strong>, is <strong>dynamic</strong> and its memory footprint depends on the input size (such as batch size and sequence length). Thus, activations often become the main bottleneck in large-scale training.</p>
<section id="static-memory" class="level3">
<h3 class="anchored" data-anchor-id="static-memory">Static Memory</h3>
<p>And when we are training if you look at the training loop again, until step <code>optimizer.step()</code>, we need to keep everything in the memory. And after <code>optimizer.step()</code>, we can discard the activations and the gradients. And we can keep the model parameters and the optimizer states in the memory.</p>
<p>If <span class="math inline">\(\Psi\)</span> is the total number of parameters in the model, the total static memory required (<span class="math inline">\(\mathcal{M}_{static}\)</span>) using the Adam optimizer is a fixed amount: <strong><span class="math inline">\(16\Psi\)</span> bytes</strong>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Size (<span class="math inline">\(\Psi\)</span> Bytes)</th>
<th style="text-align: left;">Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model Parameters</td>
<td style="text-align: left;">BF32 (4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(4\Psi\)</span></td>
<td style="text-align: left;">Used for forward and backward passes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Parameter Gradients</td>
<td style="text-align: left;">BF32 (4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(4\Psi\)</span></td>
<td style="text-align: left;">Used in backpropagation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Optimizer States (Adam)</td>
<td style="text-align: left;">FP32 (4+4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(8\Psi\)</span></td>
<td style="text-align: left;">Stores 1st and 2nd moment estimates (<span class="math inline">\(4\Psi\)</span> each)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Total Static Memory</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong><span class="math inline">\(16\Psi\)</span></strong></td>
<td style="text-align: left;"><strong>The absolute floor for static storage</strong></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled" title="Why Adam Optimizer Uses $4+4$ Bytes per Parameter">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Why Adam Optimizer Uses <span class="math inline">\(4+4\)</span> Bytes per Parameter
</div>
</div>
<div class="callout-body-container callout-body">
<p>Adam maintains two additional FP32 (4-byte) tensors per parameter: the <strong>first moment</strong> (mean of gradients, <span class="math inline">\(m\)</span>) and the <strong>second moment</strong> (uncentered variance, <span class="math inline">\(v\)</span>). Thus, for each parameter, Adam stores <span class="math inline">\(4\)</span> bytes for <span class="math inline">\(m\)</span> and <span class="math inline">\(4\)</span> bytes for <span class="math inline">\(v\)</span>, totaling <span class="math inline">\(8\Psi\)</span> bytes.</p>
</div>
</div>
<p>And when it comes to training a model, its all about how smartly we can manage this memory footprint. In modern LLM training, <strong>mixed precision</strong> is employed, typically using BF16 (2 bytes) for fast computation while maintaining a full-precision FP32 (4 bytes) copy of weights and optimizer states for numerical stability.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Mixed Precision Training">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Mixed Precision Training
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Mixed Precision Training</strong> accelerates deep learning and reduces memory use by combining 16-bit (BF16/FP16) and 32-bit (FP32) floating-point operations.</p>
<ul>
<li><em>How it works</em>: Forward and backward passes use low-precision (e.g., BF16) for parameters and activations, while a full-precision FP32 <code>master</code> copy of weights and optimizer states is kept for numerical stability.</li>
<li><em>Why it matters</em>: Enables training of larger models or larger batches within the same hardware footprint. Mixed precision is now standard in large-scale model training.</li>
</ul>
</div>
</div>
<p>So, with mixed precision, the breakdown is as follows:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Size (<span class="math inline">\(\Psi\)</span> Bytes)</th>
<th style="text-align: left;">Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model Parameters</td>
<td style="text-align: left;">BF16 (2 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(2\Psi\)</span></td>
<td style="text-align: left;">Used for forward and backward passes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Parameter Gradients</td>
<td style="text-align: left;">BF16 (2 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(2\Psi\)</span></td>
<td style="text-align: left;">Used in backpropagation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Master Weights</td>
<td style="text-align: left;">FP32 (4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(4\Psi\)</span></td>
<td style="text-align: left;">Full precision copy for the update step</td>
</tr>
<tr class="even">
<td style="text-align: left;">Optimizer States (Adam)</td>
<td style="text-align: left;">FP32 (4+4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(8\Psi\)</span></td>
<td style="text-align: left;">Stores 1st and 2nd moment estimates (<span class="math inline">\(4\Psi\)</span> each)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Total Static Memory (with Mixed Precision)</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong><span class="math inline">\(16\Psi\)</span></strong></td>
<td style="text-align: left;"><strong>The absolute floor for static storage</strong></td>
</tr>
</tbody>
</table>
<p>You might notice that the total static memory remains <strong><span class="math inline">\(16\Psi\)</span></strong> bytes. So what is the advantage of mixed precision training?</p>
<p>The key benefits of mixed precision are:</p>
<ul>
<li><strong>Increased Training Speed:</strong> As we are using lower-precision data types (like BF16) during forward and backward passes, computation is faster and less memory bandwidth is used.</li>
<li><strong>Reduced Activation Memory:</strong> Since our model parameters and optimizer states are stored in FP32, the activations, which are stored in BF16 during training, require half the memory compared to FP32, so the dynamic (activation) memory footprint is significantly lower.</li>
</ul>
<p>While the absolute static memory is unchanged, mixed precision allows for faster training and greater memory efficiency, especially for storing activations, enabling larger models or batches to fit within the same hardware limits.</p>
<p>And this calculation reveals a significant challenge: a <strong>70 Billion parameter model</strong> requires approximately <span class="math inline">\(70\text{B} \times 16 \text{ bytes} \approx 1120 \text{ GB}\)</span> of static memory. With high-end GPUs typically offering only <span class="math inline">\(80 \text{ GB}\)</span> of memory, loading even the static state of the model becomes impossible, without considering the dynamic activations.</p>
</section>
<section id="dynamic-memory" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-memory">Dynamic Memory</h3>
<p>This component is dependent on the input batch and is the primary cause of memory bottlenecks.</p>
<ul>
<li><p><strong>Activations:</strong> The output of each layer. They must be stored until the <strong>backward pass</strong> to compute the gradients.</p>
<ul>
<li>For a linear layer <span class="math inline">\(y=Wx\)</span>, the gradient for <span class="math inline">\(W\)</span> is calculated as: <span class="math display">\[\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y} \cdot x^T\]</span></li>
<li>This requires saving the layer’s input, <span class="math inline">\(x\)</span> (the activation from the previous layer).</li>
</ul></li>
<li><p><strong>Activation Memory Equation:</strong> The total memory required for activations (<span class="math inline">\(m_{act}\)</span>) in mixed precision can be estimated by the following equation: <span class="math display">\[\mathcal{M}_{\text{act}} = L \cdot \text{seq} \cdot \text{bs} \cdot h \cdot \left(34 + \frac{5 \cdot n_{heads} \cdot \text{seq}}{\text{h}}\right)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(L\)</span>: Number of layers</li>
<li><span class="math inline">\(\text{seq}\)</span>: Sequence length</li>
<li><span class="math inline">\(\text{bs}\)</span>: Batch size (number of samples)</li>
<li><span class="math inline">\(h\)</span>: Hidden dimension of the model</li>
<li><span class="math inline">\(n_{heads}\)</span>: Number of attention heads</li>
</ul></li>
</ul>
<p align="center">
<img src="assets/activations.png" alt="Activation Count" width="70%">
</p>
<p>As we can see, activation memory usage is <strong>not static</strong> for a given model; it scales:</p>
<ul>
<li><strong>Linearly</strong> with the <strong>batch size</strong> (<span class="math inline">\(\text{bs}\)</span>)</li>
<li><strong>Quadratically</strong> with the <strong>sequence length</strong> (<span class="math inline">\(\text{seq}\)</span>)</li>
</ul>
<p>And this quadratic scaling with <span class="math inline">\(\text{seq}^2\)</span> (an effect stemming from the attention matrix) means the activation memory is the part that will <strong>blow up</strong> when you increase the batch size or train with longer sequences.</p>
</section>
</section>
<section id="batch-size-intuition" class="level2">
<h2 class="anchored" data-anchor-id="batch-size-intuition">Batch Size Intuition</h2>
<p>So, as we can see that the longer the sequence, the more activations we need and hence the more memory we need. So, even for one single sequence, the memory required is more than 50 GB. And this is a significant challenge for training large models.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Global Batch Size">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Global Batch Size
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we talk about batch size in LLM pre-training, we usually refer to it in terms of the <strong>number of tokens</strong>, not the number of sequences. That is, the token batch size is calculated as <strong>sequence length × number of sequences (micro-batch size)</strong></p>
</div>
</div>
<p>Typically, the global batch size used in pre-training is extremely large—usually in the <strong>millions of tokens</strong>. And in practice, we start the training with a smaller batch size and gradually increase it over the course of training.</p>
<ul>
<li><strong>Small Batch Size:</strong> Used at the <strong>beginning of training</strong> when the loss is high. It provides <strong>quick, noisy signals</strong> that help the model traverse the loss landscape rapidly toward the minima.</li>
<li><strong>Large Batch Size:</strong> Used as training approaches the optima. It provides a <strong>more accurate gradient direction</strong> (a clearer signal), which reduces noise and ensures confident, stable convergence.</li>
</ul>
<p align="center">
<img src="assets/batch-size.png" alt="Batch Size Intuition" width="100%">
</p>
</section>
<section id="memory-usage-in-transformer" class="level2">
<h2 class="anchored" data-anchor-id="memory-usage-in-transformer">Memory usage in Transformer</h2>
<p>To get a sense of the memory usage in a Transformer, let’s take a look at the memory usage of <code>Llama 3.1</code> {<code>8B</code>, <code>70B</code> and <code>13B</code>} models.</p>
<p align="center">
<img src="assets/llama-3-1.png" alt="Batch Size Intuition" width="100%">
</p>
<p>From this graph, we can clearly see that forshort sequences (or small batch sizes), memory usage for activations is almost negligible, but from around 4K-16K tokens they start to take up a significant amount of memory (this is because of the quadratic scaling with the sequence length, which we discussed earlier), while usage for parameters, gradients, and optimizer states is roughly independent of the sequence length and batch size.</p>
<p>How can we solve this problem of <code>activation explosion</code>? Can we somehow avoide storing all those activations ?</p>
<section id="solution-1-activation-recomputation" class="level3">
<h3 class="anchored" data-anchor-id="solution-1-activation-recomputation">Solution 1: Activation Recomputation</h3>
<p>Recall why we need to store all those activations in the first place ? It is because we need to compute the gradients for the model parameters. So, if we can somehow avoid storing all those activations, we can save a lot of memory.</p>
<p>One effective approach is <strong>Gradient Checkpointing</strong> also known as <strong>Activation Recomputation</strong>. With this technique, we discard most of the activations during the forward pass to save memory and recompute them on the fly during the backward pass when gradients are needed.</p>
<p>Normally, we’d store every hidden state between learnable operations (like feedforward layers, layer norm, etc.) to use them during the backward pass. With activation recomputation, we only store activations at specific checkpoints and recalculate everything else during backpropagation. This helps us manage memory while training large models. The process typically looks like this:</p>
<p align="center">
<img src="assets/gradient-accumulation-1.png" alt="Gradient Accumulation" width="70%">
</p>
<p>But as we know in life, there is no free lunch. Although we save memory by discarding most of the activations during the forward pass, we spend some extra compute to recompute these activations on the fly during the backward pass.</p>
<p>There are a few ways to do activation checkpointing, and each involves different memory and compute tradeoffs.</p>
<p>The most aggressive approach is called <strong>Full Activation Checkpointing</strong>, where you only store activations at the end of each layer (instead of storing every intermediate activation). This method is great for memory since you’re keeping so little, but it’s the most compute-heavy, often increasing computation time by 30–40% because you have to recompute almost everything during backpropagation.</p>
<p>But do we really need to treat every part of the model the same? By profiling, we find that the main memory culprit is the activations from the <strong>Multi-Headed Attention (MHA)</strong> layers, since they scale quadratically with sequence length.</p>
<p>This leads to a more balanced strategy: <strong>Selective Checkpointing</strong>. Here, we only skip storing activations for the heavy MHA layers and still store them for the lighter MLP layers. The payoff is impressive: up to 70% memory savings for only about 2.7% extra computation.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Activation Checkpointing on Llama 3.1 8B model">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Activation Checkpointing on Llama 3.1 8B model
</div>
</div>
<div class="callout-body-container callout-body">
<p>As you can see the graph bellow, on an 8-billion parameter model with a batch size of 1 and sequence length 4096, activation memory without any checkpointing can hit 97 GB, which is enough to break most GPUs. With selective activation checkpointing, that drops to 17 GB. And with full checkpointing, at the extreme, memory usage can go down to just 1 GB!</p>
</div>
</div>
<p align="center">
<img src="assets/llama-checkp.png" alt="Gradient Accumulation" width="100%">
</p>
<p>Now that we’ve learned about recomputation, we can tame the activation memory usage we saw in the previous graphs!</p>
<p>However, activations still have a linear dependence on the batch size, so as we move to larger batch sizes this might become an issue again. So, what can we do to increase the batch size ? And to takle this we have the next trick in our box - <strong>gradient accumulation</strong>, lets discuss that next.</p>
</section>
<section id="solution-2-gradient-accumulation" class="level3">
<h3 class="anchored" data-anchor-id="solution-2-gradient-accumulation">Solution 2: Gradient Accumulation</h3>
<p><strong>Gradient Accumulation</strong> is a technique that allows us to accumulate gradients over multiple micro-batches before performing a single global optimization step. This is particularly useful when we have a large batch size and we want to avoid running out of memory.</p>
<p>The general idea is to split the batch into smaller micro-batches (let’s say 3) and process them one by one. We compute the gradients for each micro-batch and accumulate them (we <strong>do not do</strong> <code>optimizer.step()</code> after each micro-batch). And after processing all the micro-batches, <strong>we perform a single</strong> global optimization step.</p>
<p align="center">
<img src="assets/grad-acc.png" alt="Gradient Accumulation" width="100%">
</p>
<p>Let’s take an example of a simple linear regression model:</p>
<p>Let’s use a different analogy: predicting the <strong>score of a student on a test</strong> based on two factors—the number of hours studied (<span class="math inline">\(x_1\)</span>) and the number of hours slept the night before (<span class="math inline">\(x_2\)</span>). We assume a simple linear relationship between these inputs and the output score:</p>
<p><span class="math display">\[
\text{score}_{pred} = x_1 w_1 + x_2 w_2 + b
\]</span></p>
<p>Our aim is to use stochastic gradient descent to determine the best values for <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, and <span class="math inline">\(b\)</span> such that the mean squared error (MSE) between the actual score (<span class="math inline">\(\text{score}_{target}\)</span>) and the predicted score (<span class="math inline">\(\text{score}_{pred}\)</span>) is minimized:</p>
<p><span class="math display">\[
\underset{w_1, w_2, b}{\mathrm{argmin}} \; (\text{score}_{pred} - \text{score}_{target})^2
\]</span></p>
<p>Without gradient accumulation, we would update the parameters after each batch of student’s data.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true" href="">Without Gradient Accumulation</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>  <span class="kw">def</span> train_no_accumulate(params: ModelParameters, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>                         num_epochs: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>                         learning_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-3</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>      <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>          <span class="cf">for</span> (x1, x2), y_target <span class="kw">in</span> training_data:</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>  </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>              <span class="co"># Calculate the output of the model</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span>              z1 <span class="op">=</span> x1 <span class="op">*</span> params.w1</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span>              z2 <span class="op">=</span> x2 <span class="op">*</span> params.w2</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span>             y_pred <span class="op">=</span> z1 <span class="op">+</span> z2 <span class="op">+</span> params.b</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span>             loss <span class="op">=</span> (y_pred <span class="op">-</span> y_target) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="dv">12</span> </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="dv">13</span>             <span class="co"># Calculate the gradients of the loss w.r.t. the parameters</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="dv">14</span>             loss.backward()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span> </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="dv">16</span>             <span class="co"># Update the parameters (at each iteration)</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="dv">17</span>             <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="dv">18</span>                 <span class="co"># Equivalent to calling optimizer.step()</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="dv">19</span>                 params.w1 <span class="op">-=</span> learning_rate <span class="op">*</span> params.w1.grad</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="dv">20</span>                 params.w2 <span class="op">-=</span> learning_rate <span class="op">*</span> params.w2.grad</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="dv">21</span>                 params.b  <span class="op">-=</span> learning_rate <span class="op">*</span> params.b.grad</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="dv">22</span> </span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span>                 <span class="co"># Reset the gradients to zero</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="dv">24</span>                 <span class="co"># Equivalent to calling optimizer.zero_grad()</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="dv">25</span>                 params.w1.grad.zero_()</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="dv">26</span>                 params.w2.grad.zero_()</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="dv">27</span>                 params.b.grad.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>With <strong>gradient accumulation</strong>, instead of updating the parameters after each batch of data, we accumulate gradients across several micro-batches (<code>micro_batch_size = 3</code>) and then update all at once. This allows us to train with larger effective batch sizes even if memory is limited.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true" href="">With Gradient Accumulation</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>  <span class="kw">def</span> train_accumulate(params: ModelParameters, </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>                       num_epochs: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>                       learning_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-3</span>, </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>                       micro_batch_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>  </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>      <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>          <span class="cf">for</span> index, ((x1, x2), y_target) <span class="kw">in</span> <span class="bu">enumerate</span>(training_data):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="dv">8</span>  </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="dv">9</span>              <span class="co"># Calculate the output of the model</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span>             z1 <span class="op">=</span> x1 <span class="op">*</span> params.w1</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="dv">11</span>             z2 <span class="op">=</span> x2 <span class="op">*</span> params.w2</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="dv">12</span>             y_pred <span class="op">=</span> z1 <span class="op">+</span> z2 <span class="op">+</span> params.b</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="dv">13</span>             loss <span class="op">=</span> (y_pred <span class="op">-</span> y_target) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="dv">14</span> </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="dv">15</span>             <span class="co"># Accumulate gradients</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="dv">16</span>             loss.backward()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="dv">17</span> </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="dv">18</span>             <span class="co"># If we have processed 3 micro-batches OR reached the end of the dataset</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="dv">19</span>             <span class="cf">if</span> (index <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> micro_batch_size <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> index <span class="op">==</span> <span class="bu">len</span>(training_data) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="dv">20</span>                 <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="dv">21</span>                     <span class="co"># Equivalent to optimizer.step()</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="dv">22</span>                     params.w1 <span class="op">-=</span> learning_rate <span class="op">*</span> params.w1.grad</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="dv">23</span>                     params.w2 <span class="op">-=</span> learning_rate <span class="op">*</span> params.w2.grad</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="dv">24</span>                     params.b  <span class="op">-=</span> learning_rate <span class="op">*</span> params.b.grad</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="dv">25</span> </span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="dv">26</span>                     <span class="co"># Reset the gradients = optimizer.zero_grad()</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="dv">27</span>                     params.w1.grad.zero_()</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="dv">28</span>                     params.w2.grad.zero_()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="dv">29</span>                     params.b.grad.zero_()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>Gradient accumulation allows us to reduce activation memory, which grows linearly with batch size, by processing smaller micro-batches sequentially. This reduces stored activations and gradients since only one micro-batch’s worth of activations needs to be kept in memory at a time, which helps reduce the overall activation memory footprint.</p>
<p>Again there is no free lunch. As gradient accumulation requires multiple consecutive forward/backward passes per optimization step, it increases the compute overhead and slows down training. But it allows us to train with larger effective batch sizes even if memory is limited.</p>
<p>So far, we’ve seen how techniques like <code>gradient checkpointing</code> and <code>gradient accumulation</code> help deal with the memory blowup issue caused by <code>activations</code>: the dynamic part of memory usage. Both allow us to fit larger models or batches on a <strong>single GPU</strong>, but mostly by working sequentially and slowing down training. However, these don’t address the static memory required for parameters, gradients, and optimizer states, nor do they fully utilize available hardware (assume we have more than one GPU).</p>
<p>To tackle this, we can scale training across multiple GPUs using <strong>Data Parallelism</strong>. By splitting micro-batches and processing them simultaneously on several GPUs, we address both memory and compute bottlenecks, and that is what we will discuss in the next section.</p>
</section>
</section>
<section id="scaling-with-multiple-gpus-data-parallelism-dp" class="level2">
<h2 class="anchored" data-anchor-id="scaling-with-multiple-gpus-data-parallelism-dp">Scaling with Multiple GPUs: Data Parallelism (DP)</h2>
<p>Recall that in Gradient Accumulation, we were processing <strong>micro-batches (MBS)</strong> sequentially. Since these micro-batches are <strong>independent of each other</strong>, we can process them <strong>parallelly on different GPUs</strong>. Something like this, if you see carefully now we are processing the micro-batches in parallel on different GPUs, w.r.t what we did in Gradient Accumulation where we were processing the micro-batches sequentially on a single GPU:</p>
<p align="center">
<img src="assets/dp.png" alt="Data Parallelism" width="100%">
</p>
<section id="the-data-parallel-setup" class="level3">
<h3 class="anchored" data-anchor-id="the-data-parallel-setup">The Data Parallel Setup</h3>
<p>In a Data Parallel setup, we distribute the data across multiple GPUs, while maintaining a full, redundant replica of the model parameters, gradients, and optimizer states on each GPU.</p>
<ol type="1">
<li><strong>Replication:</strong> We maintain a full, redundant <strong>replica</strong> of the model parameters (<span class="math inline">\(\Phi\)</span>), gradients (<span class="math inline">\(\nabla \Phi\)</span>), and optimizer states (<span class="math inline">\(\Phi_{\text{optim}}\)</span>) on <strong>each GPU</strong>.</li>
</ol>
<p align="center">
<img src="assets/dp_1.png" alt="Data Parallel Setup" width="100%">
</p>
<ol start="2" type="1">
<li><strong>Parallel Processing:</strong> Each GPU processes a unique micro-batch simultaneously. This involves <strong>same operations, different data</strong>.</li>
</ol>
<p align="center">
<img src="assets/dp_2.png" alt="Data Parallel Setup" width="100%">
</p>
<ol start="3" type="1">
<li><strong>Local Computation:</strong> Each GPU performs its forward pass and backward pass locally and independently, resulting in a local gradient (<span class="math inline">\(\nabla \Phi_i\)</span>).</li>
</ol>
<p align="center">
<img src="assets/dp_3.png" alt="Data Parallel Setup" width="100%">
</p>
<p>If you look carefully, we can perform the forward pass and the backward pass in parallel on different GPUs. But we cannot perform the optimizer step and update the parameters independently on different GPUs. If we do that, we will end up training N different models on N different GPUs which is not what we want.</p>
<p>So, after the backward pass, we need to somehow synchronize the gradients across the GPUs. And this is done by the <strong>All-Reduce</strong> primitive.</p>
</section>
<section id="gradient-synchronization-the-all-reduce-primitive" class="level3">
<h3 class="anchored" data-anchor-id="gradient-synchronization-the-all-reduce-primitive">Gradient Synchronization: The All-Reduce Primitive</h3>
<p>Before we dive into the All-Reduce operation, it’s important to note that NVIDIA provides a rich set of <strong>communication primitives</strong> as part of its distributed training ecosystem (such as NCCL). These primitives simplify and accelerate multi-GPU (and multi-node) communication, enabling efficient synchronization and sharding operations required for large-scale training.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Communication Primitives in Distributed Training">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Communication Primitives in Distributed Training
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>All-Reduce</strong> is just one such primitive—used specifically for synchronizing gradients across GPUs at the end of each backward pass in standard Data Parallel training. However, there are several other primitives (like <strong>All-Gather</strong>, <strong>Reduce-Scatter</strong>, <strong>Broadcast</strong>, etc.) designed for different patterns of communication and parallelism. We will encounter and discuss these additional primitives as we explore more advanced parallelization techniques (e.g., ZeRO, model sharding, tensor parallelism) later in the series.</p>
</div>
</div>
<p>For now, let’s look at <strong>All-Reduce</strong> in detail, since this is exactly what we need for synchronizing the gradients during Data Parallel training.</p>
<p>Since each GPU computes a gradient based only on its local micro-batch, we must <strong>add them to get the global gradient</strong> before performing the optimization step. The required communication operation is the <strong>All-Reduce</strong> primitive:</p>
<ul>
<li><strong>Input:</strong> Different tensors (the local gradients <span class="math inline">\(\nabla \Phi_1, \nabla \Phi_2, \dots\)</span>) on each GPU.</li>
<li><strong>Operation:</strong> A reduction operation (usually summation, <span class="math inline">\(F\)</span>) is applied to all tensors.</li>
<li><strong>Output:</strong> The result of the reduction (the global gradient <span class="math inline">\(\sum \nabla \Phi_i\)</span>) is made available on <strong>all</strong> GPUs.</li>
</ul>
<p align="center">
<img src="assets/all_reduce.png" alt="All Reduce" width="70%">
</p>
<p>Once every node receives the global gradient, it performs the <strong><code>optimizer.step()</code></strong> operation independently, ensuring all model copies remain in sync. These collective operations are defined in the <strong><code>torch.distributed</code></strong> API.</p>
<p>Here I’ve a machine with 4 T4 GPUs.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ray@ip-10-0-69-225:code$</span> nvidia-smi <span class="at">-L</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU</span> 0: Tesla T4 <span class="er">(</span><span class="ex">UUID:</span> GPU-31a1b562-c769-c7f1-ede1-48847cec8d53<span class="kw">)</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU</span> 1: Tesla T4 <span class="er">(</span><span class="ex">UUID:</span> GPU-1beaf204-f6f7-182d-67f8-aee6c58128df<span class="kw">)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU</span> 2: Tesla T4 <span class="er">(</span><span class="ex">UUID:</span> GPU-934ca246-df7e-2c7f-4bdd-b07859e46b2d<span class="kw">)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="ex">GPU</span> 3: Tesla T4 <span class="er">(</span><span class="ex">UUID:</span> GPU-141171cb-db62-b770-97ff-955f8c7f2265<span class="kw">)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Now let’s create a simple example to demonstrate the <strong>All-Reduce</strong> operation by creating 4 tensors on each GPU and performing the <strong>All-Reduce</strong> operation on them.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true" href="">All-Reduce Example</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_process():</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initializes the process group using the efficient nccl backend</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    dist.init_process_group(backend<span class="op">=</span><span class="st">'nccl'</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(dist.get_rank())</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> example_all_reduce():</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> torch.tensor([dist.get_rank() <span class="op">+</span> <span class="dv">1</span>] <span class="op">*</span> <span class="dv">3</span>, dtype<span class="op">=</span>torch.float32).cuda()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Before all_reduce on rank </span><span class="sc">{</span>dist<span class="sc">.</span>get_rank()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tensor<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    dist.all_reduce(tensor, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"After all_reduce on rank </span><span class="sc">{</span>dist<span class="sc">.</span>get_rank()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tensor<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the process group and set the device, create a tensor on each GPU and perform the All-Reduce operation on them.</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>init_process()</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>example_all_reduce()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>We can run this code on 4 GPUs using the following command:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>4 dist_all_reduce.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>We will get the following output:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_reduce on rank 3: tensor<span class="er">(</span><span class="ex">[4.,</span> 4., 4.], device=<span class="st">'cuda:3'</span><span class="kw">)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_reduce on rank 0: tensor<span class="er">(</span><span class="ex">[1.,</span> 1., 1.], device=<span class="st">'cuda:0'</span><span class="kw">)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_reduce on rank 2: tensor<span class="er">(</span><span class="ex">[3.,</span> 3., 3.], device=<span class="st">'cuda:2'</span><span class="kw">)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_reduce on rank 1: tensor<span class="er">(</span><span class="ex">[2.,</span> 2., 2.], device=<span class="st">'cuda:1'</span><span class="kw">)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_reduce on rank 3: tensor<span class="er">(</span><span class="ex">[10.,</span> 10., 10.], device=<span class="st">'cuda:3'</span><span class="kw">)</span> </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_reduce on rank 0: tensor<span class="er">(</span><span class="ex">[10.,</span> 10., 10.], device=<span class="st">'cuda:0'</span><span class="kw">)</span> </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_reduce on rank 2: tensor<span class="er">(</span><span class="ex">[10.,</span> 10., 10.], device=<span class="st">'cuda:2'</span><span class="kw">)</span> </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_reduce on rank 1: tensor<span class="er">(</span><span class="ex">[10.,</span> 10., 10.], device=<span class="st">'cuda:1'</span><span class="kw">)</span> </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="overlapping-communication-and-computation" class="level3">
<h3 class="anchored" data-anchor-id="overlapping-communication-and-computation">Overlapping Communication and Computation</h3>
<p>In a naive DP implementation, the GPUs sit <strong>idle</strong> during the communication phase, as the <strong>All-Reduce</strong> operation begins only after <strong>all</strong> gradients are computed in the backward pass. This is inefficient.</p>
<p align="center">
<img src="assets/all_reduce.gif" alt="All Reduce" width="100%">
</p>
<p>To eliminate this idle time, we <strong>overlap</strong> the communication and computation.</p>
<ul>
<li><strong>Method:</strong> As soon as the gradient for a specific layer is computed during the backward pass (e.g., <span class="math inline">\(\nabla L_2\)</span>), we immediately trigger the <strong>All-Reduce</strong> for that gradient <strong>in the background</strong>.</li>
<li><strong>Rationale:</strong> The computation of the next layer’s gradient (<span class="math inline">\(\nabla L_1\)</span>) is independent of the communication of the previous layer’s gradient (<span class="math inline">\(\nabla L_2\)</span>).</li>
<li><strong>Implementation:</strong> This technique is implemented via <strong>hooks</strong> in PyTorch (like <code>post_accumulate_grad_hook</code>), allowing the next computation step to proceed while the communication step runs concurrently, significantly improving throughput. It attach an all-reduce hook function to each parameter that requires gradients. So, now it communicates more frequently but in smaller packets.</li>
</ul>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true" href="">Overlapping Communication and Computation</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> register_backward_hook(<span class="va">self</span>, hook):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Registers a backward hook for all parameters of the model that </span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    require gradients.</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.module.parameters():</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> p.requires_grad <span class="kw">is</span> <span class="va">True</span>:</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>            p.register_post_accumulate_grad_hook(hook)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>Before while communication was happening, we were waiting for all the gradients to be computed in the backward pass.</p>
<p align="center">
<img src="assets/dp_overlap1.png" alt="All Reduce" width="100%">
</p>
<p>But now, we are overlapping the communication and computation. So, we are not waiting for all the gradients to be computed in the backward pass. We are computing the gradients for the next layer while the communication for the previous layer is happening.</p>
<p align="center">
<img src="assets/dp_overlap2.png" alt="All Reduce" width="100%">
</p>
<p>We can infact do better and communicate more efficiently by grouping the gradients into larger buckets and performing the All-Reduce operation on them. Its like packing items into boxes before shipping (have you seen at times while placing an order on Amazon, they offer us to pack multiple items into a single box to save on shipping costs ?, well thats what we are doing here but with gradients).</p>
<p>With this we can significantly reduce the communication overhead and speed up the computation operations.</p>
<p align="center">
<img src="assets/dp_overlap3.png" alt="All Reduce" width="100%">
</p>
</section>
</section>
<section id="the-limitations-of-simple-data-parallelism-dp" class="level2">
<h2 class="anchored" data-anchor-id="the-limitations-of-simple-data-parallelism-dp">The Limitations of Simple Data Parallelism (DP)</h2>
<p>Now, that we have seen how to scale out the training with multiple GPUs using Data Parallelism, we can ask ourselves a question - is this scaling <strong>lossless</strong> ?</p>
<p>The answer is no. There’s a communication overhead associated with the Data Parallelism. And as the degree of data parallelism increases, there’s a noticeable drop in tokens per second per GPU (throughput). Although we are overlapping the communication and computation, we are still waiting for the gradients to be computed in the backward pass.</p>
<p>More importantly, all these discussions about Data Parallelism (DP) so far have assumed that the <strong>entire model</strong> can fit on a single GPU. But what if the model is too large (e.g.&nbsp;GPT-3 with 175B parameters) to fit in the memory of a single GPU (NVIDIA A100 with 80GB of memory)?</p>
<p align="center">
<img src="assets/model_hw.png" alt="Model Exceeds GPU Memory" width="50%">
</p>
<p>As model sizes grow, it becomes common that a single accelerator (GPU in our case) cannot contain all model parameters, optimizer states, and gradients. Therefore, we need to find additional ways to scale training beyond simple DP, which can allow us to train models that <strong>don’t fit on a single GPU</strong>.</p>
<p>And that is what we will discuss in the next section - <strong>ZeRO</strong> (Zero Redundancy Optimizer).</p>
</section>
<section id="zero-zero-redundancy-optimizer" class="level1">
<h1>ZeRO: Zero Redundancy Optimizer</h1>
<p>ZeRO (Zero Redundancy Optimizer) is a family of techniques that addresses constrain of static memory (parameters, gradients, optimizer states) on a single GPU. With ZeRO, we can train models that don’t fit on a single GPU, and it does that by sharding the static memory components across multiple GPUs.</p>
<p>This approach is organized into three possible optimization stages:</p>
<ul>
<li><strong>ZeRO-1</strong>: optimizer state sharding</li>
<li><strong>ZeRO-2</strong>: optimizer state + gradient sharding</li>
<li><strong>ZeRO-3</strong>: optimizer state + gradient + parameter sharding</li>
</ul>
<p align="center">
<img src="assets/zero.png" alt="ZeRO" width="100%">
</p>
<p>Without even going further, you can probably guess that, with this approch we need to do a lot of communication between the GPUs. But as we have seen in the previous section, we can overlap the communication and computation to some extent. So, we can reduce the communication overhead by overlapping the communication and computation.</p>
<p>Let’s discuss each of these techniques in detail, starting with <strong>ZeRO-1</strong>.</p>
<section id="zero-1-sharding-optimizer-states" class="level3">
<h3 class="anchored" data-anchor-id="zero-1-sharding-optimizer-states">ZeRO-1: Sharding Optimizer States</h3>
<p>Recall from our earlier discussion, the <strong>static memory footprint</strong> per GPU specifically for <strong>mixed precision (using BF16 + FP32)</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Size (<span class="math inline">\(\Psi\)</span> Bytes)</th>
<th style="text-align: left;">Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model Parameters</td>
<td style="text-align: left;">BF16 (2 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(2\Psi\)</span></td>
<td style="text-align: left;">Used for forward and backward passes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Parameter Gradients</td>
<td style="text-align: left;">BF16 (2 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(2\Psi\)</span></td>
<td style="text-align: left;">Used in backpropagation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Master Weights</td>
<td style="text-align: left;">FP32 (4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(4\Psi\)</span></td>
<td style="text-align: left;">Full precision copy for the update step</td>
</tr>
<tr class="even">
<td style="text-align: left;">Optimizer States (Adam)</td>
<td style="text-align: left;">FP32 (4+4 bytes)</td>
<td style="text-align: left;"><span class="math inline">\(8\Psi\)</span></td>
<td style="text-align: left;">Stores 1st and 2nd moment estimates (<span class="math inline">\(4\Psi\)</span> each)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Total Static Memory (with Mixed Precision)</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><strong><span class="math inline">\(16\Psi\)</span></strong></td>
<td style="text-align: left;"><strong>The absolute floor for static storage</strong></td>
</tr>
</tbody>
</table>
<p>The largest part of the static memory comes from the <strong>optimizer states</strong>, especially for optimizers like Adam, which maintain both first and second moment statistics. <strong>With Data Parallelism (DP), all these components are duplicated on every GPU</strong> in the data-parallel group, so each device bears the full cost (<span class="math inline">\(16\Psi\)</span>) of these tensors (ignoring activations for now).</p>
<div class="callout callout-style-default callout-note callout-titled" title="Important Caveat: Master Weights Are Part of Optimizer State Sharding">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Important Caveat: Master Weights Are Part of Optimizer State Sharding
</div>
</div>
<div class="callout-body-container callout-body">
<p>When discussing the sharding of optimizer states in ZeRO, we must also include the <strong>master weights</strong> (the FP32 copy of model parameters used for the optimizer update) in the sharding calculation. Both the optimizer states and these master weights are stored in FP32, and both are sharded together in ZeRO-1. Thus, when you see references to “optimizer state sharding,” this always implicitly includes master weights in modern mixed precision training setups.</p>
</div>
</div>
<p>With <strong>ZeRO-1</strong>, the goal is to <em>shard</em> (that is, partition and spread) the <strong>FP32 optimizer states</strong> and <strong>FP32 master weights</strong> across the <span class="math inline">\(N_d\)</span> GPUs, rather than storing them fully on each device. This introduces the following changes:</p>
<ul>
<li>Every GPU stores only a <strong><span class="math inline">\(1/N_d\)</span> -th slice</strong> of the optimizer states (<span class="math inline">\(8\Psi\)</span>) and master weights (<span class="math inline">\(4\Psi\)</span>), rather than the full <span class="math inline">\(12\Psi\)</span>.</li>
<li>The parameter tensors and gradients (in BF16) <strong>remain fully replicated</strong> on each GPU for compatibility with forward and backward passes.</li>
</ul>
<p align="center">
<img src="assets/zero-1.png" alt="ZeRO-1" width="100%">
</p>
<p>So, <strong>ZeRO-1 directly reduces redundant memory used by both FP32 optimizer states and master weights</strong>. The per-GPU memory for these states drops from <span class="math inline">\(12\Psi\)</span> down to <span class="math inline">\(\frac{12\Psi}{N_d}\)</span>. The other (BF16) tensors remain replicated for performance and simplicity.</p>
<p>The resulting <strong>static memory footprint per GPU</strong> with ZeRO-1 sharding thus becomes: <span class="math display">\[
\mathcal{M}_{\text{ZeRO-1}} = 2\Psi + 2\Psi + \frac{12\Psi}{N_d}
\]</span></p>
<ul>
<li>Parameters (BF16): <span class="math inline">\(2\Psi\)</span></li>
<li>Gradients (BF16): <span class="math inline">\(2\Psi\)</span></li>
<li>Optimizer States + Master Weights (FP32): <span class="math inline">\(\frac{12\Psi}{N_d}\)</span></li>
</ul>
<p>Where the first two terms represent the fully replicated BF16 weights and gradients, and the last term is the optimizer states and FP32 master weights sharded across <span class="math inline">\(N_d\)</span> GPUs.</p>
<p>Just to make this more concrete, let’s look at some practical numbers. Suppose you have a modern <strong>A100/H100 GPU</strong> with <strong>80GB</strong> of memory. In DP, the largest model you can fit is roughly:</p>
<p><span class="math display">\[
\text{Max Parameters (DP)} = \frac{80~\text{GB}}{16~\text{bytes per param}} \approx 5~\text{billion parameters}
\]</span></p>
<p>But if you apply ZeRO-1 with <strong>64 GPUs</strong> (<span class="math inline">\(N_d = 64\)</span>), the optimizer state and master weights are now only a small shard per GPU:</p>
<ul>
<li><span class="math inline">\(\frac{12}{64} \approx 0.1875\)</span> (so just 1.5GB of optimizer/master weights per GPU for a 5B model)</li>
<li>The effective static memory per parameter drops from 16 bytes (DP) to about <strong>4.2 bytes</strong> (ZeRO-1).</li>
</ul>
<p>So now, the largest model you can train on that same 80GB GPU jumps to:</p>
<p><span class="math display">\[
\text{Max Parameters (ZeRO-1, 64 GPUs)} = \frac{80~\text{GB}}{4.2~\text{bytes per param}} \approx 19~\text{billion parameters}
\]</span></p>
<p>So, its great that we can train a larger model on the same hardware, but we still need to discuss the communication overhead associated with this approach.</p>
<p>For the forward pass, we <strong>don’t</strong> need to do any communication, as we have all the parameters in each GPU.</p>
<p align="center">
<img src="assets/zero-1_1.png" alt="ZeRO-1 Backward" width="100%">
</p>
<p>Next in the backward pass, we have all the gradients in each GPU. So, we need to do a <strong>All-Reduce</strong> and at this point we have the same gradients on all the GPUs.</p>
<p align="center">
<img src="assets/zero-1_2.png" alt="ZeRO-1 Backward" width="100%">
</p>
<p>But now on each GPU, we can <em>discard</em> all the other gradients and <em>keep only the one</em> whose corresponding optimizer state is present on that particular GPU.</p>
<p align="center">
<img src="assets/zero-1_3.png" alt="ZeRO-1 Backward" width="100%">
</p>
<p>After this, each GPU can <em>update</em> its respective model parameters to its corresponding optimizer state and gradients.</p>
<p align="center">
<img src="assets/zero-1_4.png" alt="ZeRO-1 Backward" width="100%">
</p>
<p>And at this point, we need to communicate again, to get the updated model parameters on all the GPUs, as at this point each GPU has the updated model parameters only for its own shard of the optimizer state.</p>
<p>But what type of communication do we need to do ?</p>
<p>As each GPU need to <em>gather</em> the updated model parameters from all the other GPUs. So, we need to do a <strong>All-Gather</strong> operation. And this is another communication primitive like <strong>All-Reduce</strong> which we have seen earlier.</p>
<p align="center">
<img src="assets/all_gather.png" alt="All Gather" width="70%">
</p>
<p>Let’s quickly see an example of how to do this using <code>torch.distributed.all_gather()</code>. Here we are creating a tensor on each GPU and performing the <strong>All-Gather</strong> operation on them.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true" href="">All-Gather Example</a></li></ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_process():</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initializes the process group using the efficient nccl backend</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    dist.init_process_group(backend<span class="op">=</span><span class="st">'nccl'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(dist.get_rank())</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> example_all_gather():</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> torch.tensor([dist.get_rank() <span class="op">+</span> <span class="dv">1</span>] <span class="op">*</span> <span class="dv">3</span>, dtype<span class="op">=</span>torch.float32).cuda()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prepare an output list of tensors for all_gather</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    world_size <span class="op">=</span> dist.get_world_size()</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    gathered <span class="op">=</span> [torch.zeros_like(tensor) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Before all_gather on rank </span><span class="sc">{</span>dist<span class="sc">.</span>get_rank()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tensor<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    dist.all_gather(gathered, tensor)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"After all_gather on rank </span><span class="sc">{</span>dist<span class="sc">.</span>get_rank()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>[t.cpu().tolist() <span class="cf">for</span> t <span class="kw">in</span> gathered]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the process group and set the device, create a tensor on each GPU and perform the All-Gather operation on them.</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>init_process()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>example_all_gather()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>Just like before, we can run this code with 4 GPUs:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>4 dist_all_gather.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Typical output:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_gather on rank 2: tensor<span class="er">(</span><span class="ex">[3.,</span> 3., 3.], device=<span class="st">'cuda:2'</span><span class="kw">)</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_gather on rank 0: tensor<span class="er">(</span><span class="ex">[1.,</span> 1., 1.], device=<span class="st">'cuda:0'</span><span class="kw">)</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_gather on rank 1: tensor<span class="er">(</span><span class="ex">[2.,</span> 2., 2.], device=<span class="st">'cuda:1'</span><span class="kw">)</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> all_gather on rank 3: tensor<span class="er">(</span><span class="ex">[4.,</span> 4., 4.], device=<span class="st">'cuda:3'</span><span class="kw">)</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_gather on rank 0: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_gather on rank 1: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_gather on rank 2: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> all_gather on rank 3: [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0], [4.0, 4.0, 4.0]]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>With that, each GPU now has the updated model parameters and now they can start with the next batch and start the forward pass.</p>
<p align="center">
<img src="assets/zero-1_5.png" alt="ZeRO-1 Backward" width="100%">
</p>
<p>So, this is how ZeRO-1 strategy works.</p>
<p>Now, if go back and carefully look after <em>All_Reduce</em> operation in ZeRO-1, each GPU discarded all the other gradients and kept only the one whose corresponding optimizer state is present on that particular GPU.</p>
<p>Which makes us think, why we need to keep all the gradients in all the GPUs in the first place ? Why cant shard the gradients as well along with its corresponding optimizer state. And this is exactly what ZeRO-2 strategy does.</p>
</section>
<section id="zero-2-sharding-gradients" class="level3">
<h3 class="anchored" data-anchor-id="zero-2-sharding-gradients">ZeRO-2: Sharding Gradients</h3>
<p>Let’s now walk through <strong>ZeRO-2</strong> using the same concrete, step-by-step approach.</p>
<p>Recall our key question from ZeRO-1:</p>
<blockquote class="blockquote">
<p><em>After the All-Reduce, why keep all gradients on all GPUs? Can’t each GPU just hold the gradients it needs?</em></p>
</blockquote>
<p>That’s exactly what ZeRO-2 does, it further shards the gradients right alongside the optimizer state and master weights. So, each GPU now only needs to store the gradient shard corresponding to its optimizer state shard.</p>
<p align="center">
<img src="assets/zero-2.png" alt="ZeRO-2" width="100%">
</p>
<p>Just as we did for ZeRO-1, let’s run the numbers for ZeRO-2 sharding to see the dramatic benefits. With ZeRO-2, the memory formula per GPU now becomes: <span class="math display">\[
\mathcal{M}_{\text{ZeRO-2}} = 2\Psi + \frac{2\Psi + 12\Psi}{N_d}
\]</span></p>
<ul>
<li>Parameters (BF16): <span class="math inline">\(2\Psi\)</span></li>
<li>Gradients (BF16): <span class="math inline">\(\frac{2\Psi}{N_d}\)</span></li>
<li>Optimizer States + Master Weights (FP32): <span class="math inline">\(\frac{12\Psi}{N_d}\)</span></li>
</ul>
<p>If we again use an <strong>A100/H100 GPU</strong> with <strong>80GB</strong> of memory, and <span class="math inline">\(N_d = 64\)</span> GPUs, then the largest model we can train would be:</p>
<p><span class="math display">\[
\text{Max Parameters (ZeRO-2, 64 GPUs)} = \frac{80~\text{GB}}{2.2~\text{bytes per param}} \approx 36~\text{billion parameters}
\]</span></p>
<p>Let’s put this side-by-side:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Strategy</th>
<th>Effective Bytes/Param</th>
<th>Max Model on 80GB GPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DP</strong></td>
<td>16</td>
<td>~5B</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>4.2</td>
<td>~19B</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>2.2</td>
<td>~36B</td>
</tr>
</tbody>
</table>
<p>So, <strong>ZeRO-2 nearly doubles the maximum trainable model size compared to ZeRO-1</strong> (and over 7x compared to vanilla Data Parallelism).</p>
<p>Let’s see how the communication overhead changes with ZeRO-2. For the forward pass, we <strong>don’t</strong> need to do any communication (like in ZeRO-1), as we have all the parameters in each GPU.</p>
<p align="center">
<img src="assets/zero-2_1.png" alt="ZeRO-2 Backward" width="100%">
</p>
<p>Next in the backward pass, instead of performing an <strong>All-Reduce</strong> over the gradients, we only perform a <strong>Reduce-Scatter</strong> operation. Another communication primitive like <strong>All-Reduce</strong> and <strong>All-Gather</strong> which we have seen earlier.</p>
<p align="center">
<img src="assets/zero-2_2.png" alt="ZeRO-2 Backward" width="100%">
</p>
<p>So what <strong>Reduce-Scatter</strong> operation does internally is, its first reducing (summing) the gradients across all the GPUs and then scattering the result to the GPUs that need to have the gradient shard.</p>
<p align="center">
<img src="assets/reduce_scatter.png" alt="Reduce Scatter" width="100%">
</p>
<div class="callout callout-style-default callout-note callout-titled" title="Computation-communication timeline">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Computation-communication timeline
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>ZeRO-1</strong>: We keep a copy of all gradients.</li>
<li><strong>ZeRO-2</strong>: Communicate and release the gradients on the fly.</li>
<li>In practice, both use <strong><code>reduce-scatter</code></strong> for gradients and <strong><code>all-gather</code></strong> for FP32 copy of parameters.</li>
<li>There is no real overhead to using ZeRO-2 over ZeRO-1 besides implementation complexity, and indeed <strong>ZeRO-2 is usually the better option</strong>.</li>
</ul>
</div>
</div>
<p>We can see how this works with an example.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true" href="">Reduce-Scatter Example</a></li></ul>
<div class="tab-content">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_process():</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initializes the process group using the efficient nccl backend</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    dist.init_process_group(backend<span class="op">=</span><span class="st">'nccl'</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(dist.get_rank())</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> example_reduce_scatter():</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    rank <span class="op">=</span> dist.get_rank()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    world_size <span class="op">=</span> dist.get_world_size()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Construct a single input tensor, then split into equal chunks (one for each rank)</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    input_tensor <span class="op">=</span> torch.arange(<span class="dv">1</span>, world_size <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>torch.float32).cuda()</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    input_list <span class="op">=</span> <span class="bu">list</span>(torch.chunk(input_tensor, world_size))</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    output_tensor <span class="op">=</span> torch.zeros(<span class="dv">3</span>, dtype<span class="op">=</span>torch.float32).cuda()</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Before reduce_scatter on rank </span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>[t.cpu().tolist() <span class="cf">for</span> t <span class="kw">in</span> input_list]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    dist.reduce_scatter(output_tensor, input_list, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"After reduce_scatter on rank </span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>output_tensor<span class="sc">.</span>cpu()<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the process group and set device, then perform Reduce-Scatter</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>init_process()</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>example_reduce_scatter()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
</div>
<p>Just like before, you can run this code with 4 GPUs:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">torchrun</span> <span class="at">--nproc_per_node</span><span class="op">=</span>4 dist_reduce_scatter.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Typical output:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> reduce_scatter on rank 0: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> reduce_scatter on rank 1: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> reduce_scatter on rank 2: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ex">Before</span> reduce_scatter on rank 3: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> reduce_scatter on rank 0: [4.0, 8.0, 12.0]</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> reduce_scatter on rank 1: [16.0, 20.0, 24.0]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> reduce_scatter on rank 2: [28.0, 32.0, 36.0]</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="ex">After</span> reduce_scatter on rank 3: [40.0, 44.0, 48.0]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>Each rank starts with identical chunked inputs. After <code>reduce_scatter</code>, every rank gets the sum (across all ranks) of the <em>i-th chunk</em>, distributed such that rank 0 gets chunk 0’s sum, rank 1 gets chunk 1’s sum, etc.</p>
<p>After <strong>Reduce-Scatter</strong> operation, each GPU now has the gradient shard corresponding to its optimizer state shard which it can use to update its model parameters.</p>
<p align="center">
<img src="assets/zero-2_3.png" alt="ZeRO-2 Backward" width="100%">
</p>
<p>And now, we need to communicate again, to get the updated model parameters on all the GPUs, as at this point each GPU has the updated model parameters only for its own shard of the optimizer state.</p>
<p align="center">
<img src="assets/zero-2_4.png" alt="ZeRO-2 Backward" width="100%">
</p>
<p>This is how ZeRO-2 strategy works. We have come a long way from the vanilla Data Parallelism to ZeRO-2, where we have reduced the memory footprint quite significantly, but can we further scale ? And this is exactly what ZeRO-3 strategy does.</p>
</section>
<section id="zero-3-sharding-parameters" class="level3">
<h3 class="anchored" data-anchor-id="zero-3-sharding-parameters">ZeRO-3: Sharding Parameters</h3>
<p>ZeRO-3 is the most aggressive form of ZeRO, it shards all the static memory components: parameters, gradients, and optimizer states. So, each GPU now only needs to store the parameter shard corresponding to its optimizer state shard.</p>
<p align="center">
<img src="assets/zero-3.png" alt="ZeRO-3" width="100%">
</p>
<div class="callout callout-style-default callout-note callout-titled" title="ZeRO-3 vs. FSDP">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ZeRO-3 vs.&nbsp;FSDP
</div>
</div>
<div class="callout-body-container callout-body">
<p>You may have seen the terms <strong>ZeRO-3</strong> and <strong>Fully Sharded Data Parallel (FSDP)</strong> used almost interchangeably in literature, blogs, and PyTorch documentation. That’s because the underlying strategy is the same: shard parameters, gradients, and optimizer states across GPUs to minimize memory usage per device.</p>
<ul>
<li><strong>ZeRO-3</strong> originated as a theoretical memory optimization described in the <a href="https://arxiv.org/abs/1910.02054">Microsoft DeepSpeed ZeRO paper</a>, outlining <strong>stage 3</strong> of ZeRO by sharding all model state across different GPUs. Its basically a concept implemented in multiple frameworks like DeepSpeed, etc.</li>
<li><strong>FSDP (Fully Sharded Data Parallel)</strong> is the official PyTorch implementation of this idea. FSDP leverages the ZeRO-3 approach and provides a flexible interface for applying parameter, gradient, and optimizer sharding with PyTorch models in both research and production environments.</li>
</ul>
</div>
</div>
<p>With ZeRO-3, the memory formula per GPU now becomes: <span class="math display">\[
\mathcal{M}_{\text{ZeRO-3}} = \frac{2\Psi + 2\Psi + 12\Psi}{N_d} = \frac{16\Psi}{N_d}
\]</span></p>
<ul>
<li>Parameters (BF16): <span class="math inline">\(\frac{2\Psi}{N_d}\)</span></li>
<li>Gradients (BF16): <span class="math inline">\(\frac{2\Psi}{N_d}\)</span></li>
<li>Optimizer States + Master Weights (FP32): <span class="math inline">\(\frac{12\Psi}{N_d}\)</span></li>
</ul>
<p>If we again use an <strong>A100/H100 GPU</strong> with <strong>80GB</strong> of memory, and <span class="math inline">\(N_d = 64\)</span> GPUs, then the largest model we can train would be:</p>
<p><span class="math display">\[
\text{Max Parameters (ZeRO-3, 64 GPUs)} = \frac{80~\text{GB}}{0.25~\text{bytes per param}} \approx 320~\text{billion parameters}
\]</span></p>
<p>Let’s put this all side-by-side:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Strategy</th>
<th>Effective Bytes/Param</th>
<th>Max Model on 80GB GPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>DP</strong></td>
<td>16</td>
<td>~5B</td>
</tr>
<tr class="even">
<td><strong>ZeRO-1</strong></td>
<td>4.2</td>
<td>~19B</td>
</tr>
<tr class="odd">
<td><strong>ZeRO-2</strong></td>
<td>2.2</td>
<td>~36B</td>
</tr>
<tr class="even">
<td><strong>ZeRO-3</strong></td>
<td>0.25</td>
<td>~320B</td>
</tr>
</tbody>
</table>
<p>As you can see, <strong>ZeRO-3/FSDP</strong> can 10x the maximum trainable model size compared to <strong>ZeRO-2</strong>, and over 60x compared to <strong>vanilla DP</strong>.</p>
<p>Now, let’s see how the communication overhead changes with ZeRO-3. As the model parameters are now sharded, we have a problem, we can not do the forward pass without any communication, we need to do a <strong>All-Gather</strong> operation to first get the full model parameters on all the GPUs.</p>
<p align="center">
<img src="assets/zero-3_1.png" alt="ZeRO-3 Forward" width="100%">
</p>
<p>But after the forward pass, we can <em>flush</em> the model parameters from memory, as we don’t need them anymore for the current forward pass (as we can see above). So, although it reduces the memory footprint, it introduces a communication overhead.</p>
<p>Similarly in the backward pass, we need to gather the parameters as and when needed using <strong>All-Gather</strong> and then perform <strong>Reduce-Scatter</strong> operation to get the gradient shards on all the GPUs as we did in ZeRO-2.</p>
<p align="center">
<img src="assets/zero-3_2.png" alt="ZeRO-3 Backward" width="100%">
</p>
<div class="callout callout-style-default callout-note callout-titled" title="ZeRO-3 Communication and Memory Recap">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ZeRO-3 Communication and Memory Recap
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s recap how communication and memory work with ZeRO-3. For the forward pass, since the parameters are fully sharded, we have to <strong>all-gather</strong> the weights whenever we need them, which gives us a communication cost of <span class="math inline">\(\Psi\)</span>. Because those parameters can be released from memory right after the forward usage, we have to all-gather again as needed in the backward pass, so we pay that <span class="math inline">\(\Psi\)</span> “tax” a second time. And just like ZeRO-2, we need a <strong>reduce-scatter</strong> for the gradients at the end of backward, which adds yet another <span class="math inline">\(\Psi\)</span> in communication cost. So in total, the communication bill per iteration comes out to <span class="math inline">\(3\Psi\)</span>, a bit higher than the <span class="math inline">\(2\Psi\)</span> we saw in ZeRO-2.</p>
<p>On paper, this sounds like a lot of data being moved around, but in practice it’s not too scary! Thanks to <em>prefetching,</em> we can overlap these all-gather operations with computation. Typically, while we’re doing the forward for Layer <span class="math inline">\(n\)</span>, we can start all-gathering the parameters for Layer <span class="math inline">\(n+1\)</span> in parallel. Similarly, during the backward pass, we can prefetch the next set of weights needed. This overlap keeps things efficient as long as we aren’t cranking DP up to very large scales (as a rough guideline, keeping DP <span class="math inline">\(\leq\)</span> 512 is usually safe).</p>
<p>From the memory perspective, by sharding everything, we’ve boiled the formula down to its most compact form:</p>
<p><span class="math display">\[
\mathcal{M}_{\text{ZeRO-3}} = \frac{2\Psi + 2\Psi + 12\Psi}{N_d}
\]</span> Increasing the DP group size keeps reducing per-GPU model memory, but activation memory still requires tricks like checkpointing and grad accumulation, which we discussed earlier.</p>
</div>
</div>
<p>One important point that can be confusing at first: Even though ZeRO-1, ZeRO-2, and ZeRO-3 shard the model, they are all still types of <strong>Data Parallelism</strong>.</p>
<p>Each GPU still processes the entire forward and backward pass of the model on its own batch of data, just like vanilla DP. The main difference is that <strong>ZeRO changes how the model’s parameters and related tensors are stored and managed across GPUs</strong>, which dramatically reduces memory usage but doesn’t change the core idea of Data Parallelism.</p>
<p align="center">
<img src="assets/dp_summary.png" alt="DP Summary" width="100%">
</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/debnsuma\.github\.io\/my-blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
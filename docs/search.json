[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html",
    "href": "posts/distributed-training-from-scratch/index.html",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "",
    "text": "Deep learning models are growing larger and more complex by the day and so are the challenges in training them. I’m actually quite new to distributed training myself, currently trying to figure out how to scale training across multiple GPUs and even multiple machines using different forms of parallelism.\nWhat I’ve quickly discovered is that the bigger challenge isn’t the training algorithm itself, but understanding how distributed systems work and how to manage resources (GPUs and CPUs) efficiently!\n\n\n\nAs models get larger, the time to train explodes, sometimes taking days, weeks, or even months just for a single epoch on one GPU.\n\n\n\n\n\n\n\n\nModel\nParameters(Millions)\nTraining Time on A100(GPU Hours)\n\n\n\n\nResNet-50\n26\n31\n\n\nResNet-101\n45\n44\n\n\nBERT-Base\n108\n84\n\n\nTuring-NLG 17B\n17,000\nTBA\n\n\nGPT-3 175B\n175,000\n3,100,000\n\n\n\nLooking at this table, you can see a dramatic surge in both parameter counts and training times:\n\nResNet-50 and ResNet-101 are manageable with a single GPU, but BERT-Base is already pushing the limits.\nTuring-NLG 17B and especially GPT-3 175B enter a whole new league, demanding immense computing power and time.\n\n\n\n\n\n\n\nNote\n\n\n\nIf we tried to train GPT-3 on a single GPU, it would take roughly 355 years to finish. Distributed training is not just useful, it’s absolutely essential as model sizes and training time requirements soar\n\n\nBut why does this happen?\n\nModel sizes and GPU demand are exploding.\nTraining these models can require millions of GPU hours.\n\nTake the LLaMA family of models as an example. The graph below illustrates that as you increase model size (shown by circle diameter), you get better performance, but you’ll need even more training time. Look at the y-axis: we’re talking millions of GPU hours! Training any of these giants on a single GPU is not just slow, it’s practically impossible.\n\n\n\nIn this blog post, we’ll explore distributed training from the ground up, learning how to scale deep learning to multiple GPUs and machines with various parallelism techniques. We’ll see how to implement these strategies from scratch using PyTorch, then level up by using Ray for scalable training.\nAs I mentioned, I’m still in the first epoch of my distributed training journey! And as I learn more shall keep updating this writeup.\nBefore I dive in, I’d like to thank some of the brilliant minds, mentors, and friends who’ve helped me along the way: Prof. Tanmoy Chakraborty, Dr. Yatin Nandwani, Prof. Song Han, my mentor Rohan Shravan (for his exceptional teaching and guidance over the years), and friends/colleagues like Dipankar Ranjan Baisya, Chris Fregly, Zachary Mueller, Ram Mohan, Debanjan Saha, and Siddhant Gupta.\nMost of the content here is inspired by their work, lectures, and advice. All references and resources are at the end of this post."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#introduction",
    "href": "posts/distributed-training-from-scratch/index.html#introduction",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "",
    "text": "Deep learning models are growing larger and more complex by the day and so are the challenges in training them. I’m actually quite new to distributed training myself, currently trying to figure out how to scale training across multiple GPUs and even multiple machines using different forms of parallelism.\nWhat I’ve quickly discovered is that the bigger challenge isn’t the training algorithm itself, but understanding how distributed systems work and how to manage resources (GPUs and CPUs) efficiently!\n\n\n\nAs models get larger, the time to train explodes, sometimes taking days, weeks, or even months just for a single epoch on one GPU.\n\n\n\n\n\n\n\n\nModel\nParameters(Millions)\nTraining Time on A100(GPU Hours)\n\n\n\n\nResNet-50\n26\n31\n\n\nResNet-101\n45\n44\n\n\nBERT-Base\n108\n84\n\n\nTuring-NLG 17B\n17,000\nTBA\n\n\nGPT-3 175B\n175,000\n3,100,000\n\n\n\nLooking at this table, you can see a dramatic surge in both parameter counts and training times:\n\nResNet-50 and ResNet-101 are manageable with a single GPU, but BERT-Base is already pushing the limits.\nTuring-NLG 17B and especially GPT-3 175B enter a whole new league, demanding immense computing power and time.\n\n\n\n\n\n\n\nNote\n\n\n\nIf we tried to train GPT-3 on a single GPU, it would take roughly 355 years to finish. Distributed training is not just useful, it’s absolutely essential as model sizes and training time requirements soar\n\n\nBut why does this happen?\n\nModel sizes and GPU demand are exploding.\nTraining these models can require millions of GPU hours.\n\nTake the LLaMA family of models as an example. The graph below illustrates that as you increase model size (shown by circle diameter), you get better performance, but you’ll need even more training time. Look at the y-axis: we’re talking millions of GPU hours! Training any of these giants on a single GPU is not just slow, it’s practically impossible.\n\n\n\nIn this blog post, we’ll explore distributed training from the ground up, learning how to scale deep learning to multiple GPUs and machines with various parallelism techniques. We’ll see how to implement these strategies from scratch using PyTorch, then level up by using Ray for scalable training.\nAs I mentioned, I’m still in the first epoch of my distributed training journey! And as I learn more shall keep updating this writeup.\nBefore I dive in, I’d like to thank some of the brilliant minds, mentors, and friends who’ve helped me along the way: Prof. Tanmoy Chakraborty, Dr. Yatin Nandwani, Prof. Song Han, my mentor Rohan Shravan (for his exceptional teaching and guidance over the years), and friends/colleagues like Dipankar Ranjan Baisya, Chris Fregly, Zachary Mueller, Ram Mohan, Debanjan Saha, and Siddhant Gupta.\nMost of the content here is inspired by their work, lectures, and advice. All references and resources are at the end of this post."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#deep-learning-training-basics",
    "href": "posts/distributed-training-from-scratch/index.html#deep-learning-training-basics",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Deep Learning Training Basics",
    "text": "Deep Learning Training Basics\nBefore diving into scaling, let’s quickly review the standard model training loop, such as a simple Multi-Layer Perceptron (MLP):\n\nPseudo Code for the Training Loop\n\n\n1  model = MLP().to(device)\n2  optimizer = Adam(model.parameters())\n3  criterion = CrossEntropyLoss()\n4  data_loader = DataLoader(dataset)\n5  \n6  for epoch in range(num_epochs):\n7      model.train()  \n8      for inputs, targets in data_loader:\n9          # 1. Move batch to GPU\n10         inputs, targets = inputs.to(device), targets.to(device)\n11         \n12         # 2. Clear gradients\n13         optimizer.zero_grad()\n14         \n15         # 3. Forward pass\n16         outputs = model(inputs)\n17         loss = criterion(outputs, targets)\n18         \n19         # 4. Backpropagation\n20         loss.backward()\n21         \n22         # 5. Optimization\n23         optimizer.step()\n\n\n\nWhat’s happening in each epoch of this training loop?\n\nWe iterate over the data in mini-batches (line 6-8).\nMove each batch to the GPU (line 9-10).\nZero out the gradients (line 12-13).\nRun the forward pass and compute loss (line 15-17).\nPerform backpropagation to compute gradients (line 18-19).\nUpdate the model with the optimizer (line 20-21).\n\n\n\n\nThis pattern is the core of most deep learning training loop (in fact, it is the core of any machine learning training routine)."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#bottlenecks-in-single-gpu-training",
    "href": "posts/distributed-training-from-scratch/index.html#bottlenecks-in-single-gpu-training",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Bottlenecks in Single-GPU Training",
    "text": "Bottlenecks in Single-GPU Training\nWhen training deep learning models on a single GPU, there are four main things that eat up high-bandwidth memory (HBM):\n\nModel Parameters (\\(\\Phi\\)):\nThese are the weights your model is learning.\nParameter Gradients (\\(\\nabla \\Phi\\)):\nThe gradients calculated during backpropagation, which are used to update parameters.\nOptimizer States (\\(\\Phi_{\\text{optim}}\\)):\nExtra variables required by the optimizer, such as momentum and variance in Adam.\nActivations (\\(\\mathcal{M}_{\\text{act}}\\)):\nThe intermediate outputs of each layer, which are necessary for gradient computation during backprop.\n\nThe first three (parameters, gradients, and optimizer states) are static, they make up the fixed memory footprint determined by the model’s architecture.\nThe activations are dynamic, depending on your batch and sequence length, and often become the main bottleneck for large-scale training.\n\nStatic Memory\nIf you revisit the training loop, you’ll notice that up until optimizer.step(), everything must be retained in memory. After that, you can discard activations and gradients, but the model parameters and optimizer states stick around.\nIf \\(\\Psi\\) is the total number of model parameters, the static memory required (\\(\\mathcal{M}_{static}\\)) using the Adam optimizer is a crisp: \\(16\\Psi\\) bytes.\n\n\n\n\n\n\n\n\n\nComponent\nPrecision\nSize (\\(\\Psi\\) Bytes)\nDetails\n\n\n\n\nModel Parameters\nBF32 (4 bytes)\n\\(4\\Psi\\)\nUsed for fwd/back passes\n\n\nParameter Gradients\nBF32 (4 bytes)\n\\(4\\Psi\\)\nFor backpropagation\n\n\nOptimizer States (Adam)\nFP32 (4+4 bytes)\n\\(8\\Psi\\)\n1st and 2nd moment estimates\n\n\nTotal Static Memory\n\n\\(16\\Psi\\)\nAbsolute minimum for static storage\n\n\n\n\n\n\n\n\n\nNoteWhy Adam Optimizer Uses \\(4+4\\) Bytes per Parameter\n\n\n\nAdam maintains two additional FP32 (4-byte) tensors per parameter: the first moment (mean of gradients, \\(m\\)) and the second moment (uncentered variance, \\(v\\)). Thus, for each parameter, Adam stores \\(4\\) bytes for \\(m\\) and \\(4\\) bytes for \\(v\\), totaling \\(8\\Psi\\) bytes.\n\n\nWhen optimizing large models, memory management becomes an art. Modern LLM training uses mixed precision, typically BF16 (2 bytes) for fast compute, but still maintaining an FP32 (4 bytes) copy for weights and optimizer states to preserve accuracy.\n\n\n\n\n\n\nTipMixed Precision Training\n\n\n\nMixed Precision Training accelerates deep learning and reduces memory usage by combining 16-bit (BF16/FP16) and 32-bit (FP32) floating-point operations.\n\nHow? Forward/backward passes run with low-precision types (BF16), while an FP32 master copy is kept for stability.\nWhy care? Allows larger models/batches to fit in memory and speeds up training. Mixed precision is now standard for large-scale model training.\n\n\n\nWith mixed precision, the memory usage looks like this:\n\n\n\n\n\n\n\n\n\nComponent\nPrecision\nSize (\\(\\Psi\\) Bytes)\nDetails\n\n\n\n\nModel Parameters\nBF16 (2 bytes)\n\\(2\\Psi\\)\nFor forward/backward passes\n\n\nParameter Gradients\nBF16 (2 bytes)\n\\(2\\Psi\\)\nBackpropagation\n\n\nMaster Weights\nFP32 (4 bytes)\n\\(4\\Psi\\)\nFor weight updates\n\n\nOptimizer States (Adam)\nFP32 (4+4 bytes)\n\\(8\\Psi\\)\n1st/2nd moment estimates\n\n\nTotal Static Memory\n\n\\(16\\Psi\\)\nUnchanged overall\n\n\n\nYou might notice that the total static memory remains \\(16\\Psi\\) bytes. So what is the advantage of mixed precision training?\n\nFaster Training: Using BF16 reduces computation time and memory bandwidth.\nActivations Use Half the Memory: Activations (stored in BF16) become much lighter.\n\nEven though the static footprint isn’t reduced, training can run faster, and we can fit larger dynamic activations, squeezing the most out of every GPU.\nBut here’s the hard truth: a 70B parameter model eats up around \\(70\\text{B} \\times 16\\text{ bytes} = 1120\\text{ GB}\\) of static memory, which is far beyond a single A100 GPU’s 80GB. And that’s before counting activations!\n\n\nDynamic Memory\nDynamic memory, mainly activations, is completely input-dependent and usually the cause of memory headaches.\n\nActivations: The output of each layer. They must be stored until the backward pass to compute the gradients.\n\nFor a linear layer \\(y=Wx\\), the gradient for \\(W\\) is calculated as: \\[\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot x^T\\]\nThis requires saving the layer’s input, \\(x\\) (the activation from the previous layer).\n\nActivation Memory Equation: The total memory required for activations (\\(m_{act}\\)) in mixed precision can be estimated by the following equation: \\[\\mathcal{M}_{\\text{act}} = L \\cdot \\text{seq} \\cdot \\text{bs} \\cdot h \\cdot \\left(34 + \\frac{5 \\cdot n_{heads} \\cdot \\text{seq}}{\\text{h}}\\right)\\]\nWhere:\n\n\\(L\\): Number of layers\n\\(\\text{seq}\\): Sequence length\n\\(\\text{bs}\\): Batch size (number of samples)\n\\(h\\): Hidden dimension of the model\n\\(n_{heads}\\): Number of attention heads\n\n\n\n\n\nAs we can see, activation memory usage is not static for a given model, it:\n\nGrows linearly with batch size (\\(bs\\))\nGrows quadratically with sequence length (\\(seq\\))\n\nThis quadratic growth (thanks, attention matrix!) is why activation memory swells out of control when you increase batch size or sequence length."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#batch-size-intuition",
    "href": "posts/distributed-training-from-scratch/index.html#batch-size-intuition",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Batch Size Intuition",
    "text": "Batch Size Intuition\nAs you might guess, longer sequences mean more activations, which means more memory. In fact, memory for even a single long sequence can exceed 50GB! Training large models? It’s a real constraint.\n\n\n\n\n\n\nNoteGlobal Batch Size\n\n\n\nIn LLM pre-training, batch size typically means number of tokens, not sequences. Token batch size is simply sequence length × micro-batch size.\n\n\nReal-world pre-training uses massive global batch sizes, often millions of tokens. In practice, training begins with smaller batches (for noisy, fast progress), then scales up batch size for stability and accuracy as you approach convergence.\n\nSmall batches: Early phase, high loss, helps the model learn quickly.\nBig batches: Later phase, less noise, more stable gradients."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#memory-usage-in-transformers",
    "href": "posts/distributed-training-from-scratch/index.html#memory-usage-in-transformers",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Memory Usage in Transformers",
    "text": "Memory Usage in Transformers\nTo appreciate the scale of this, let’s take a look at the memory usage in Llama 3.1 models (8B, 13B, 70B).\n\n\n\nFrom this graph, we can clearly see that for short sequences (or small batch sizes), memory usage for activations is almost negligible, but from around 4K-16K tokens they start to take up a significant amount of memory (this is because of the quadratic scaling with the sequence length, which we discussed earlier), while usage for parameters, gradients, and optimizer states is roughly independent of the sequence length and batch size.\nHow can we solve this problem of activation explosion? Can we somehow avoide storing all those activations ?\n\nSolution 1: Activation Recomputation\nWhy do we store all these activations? We need them to compute parameter gradients during backward pass. What if we could avoid keeping every activation in memory?\nThis is what gradient checkpointing does, which is also known as activation recomputation. With this technique, we keep only a few activations in memory during the forward pass and recompute the missing ones on-the-fly during backward. We save memory, but at the cost of more compute!\nNormally, we’d store every hidden state between learnable operations (like feedforward layers, layer norm, etc.) to use them during the backward pass. With activation recomputation, we only store activations at specific checkpoints and recalculate everything else during backpropagation. This helps us manage memory while training large models.\n\n\n\nBut there is no free lunch, although memory is saved, we pay with extra computation as activations are re-created during backward. There are a few ways to do activation checkpointing, and each involves different memory and compute tradeoffs.\nThe most aggressive approach is called Full Activation Checkpointing, where you only store activations at the end of each layer (instead of storing every intermediate activation). This method is great for memory since you’re keeping so little, but it’s the most compute-heavy, often increasing computation time by 30–40% because you have to recompute almost everything during backpropagation.\nBut do we really need to treat every part of the model the same? By profiling, we find that the main memory culprit is the activations from the Multi-Headed Attention (MHA) layers, since they scale quadratically with sequence length.\nThis leads to a more balanced strategy: Selective Checkpointing. Here, we only skip storing activations for the heavy MHA layers and still store them for the lighter MLP layers. The payoff is impressive: up to 70% memory savings for only about 2.7% extra computation.\n\n\n\n\n\n\nNoteActivation Checkpointing on Llama 3.1 8B model\n\n\n\nAs you can see the graph bellow, on an 8B parameter model with a batch size of 1 and sequence length 4096, activation memory without any checkpointing can hit 97 GB, which is enough to break most GPUs. With selective activation checkpointing, that drops to 17 GB. And with full checkpointing, at the extreme, memory usage can go down to just 1 GB!\n\n\n\n\n\nNow that we’ve learned about recomputation, we can tame the activation memory usage we saw in the previous graphs!\nHowever, activations still have a linear dependence on the batch size, so as we move to larger batch sizes this might become an issue again. So, what can we do to increase the batch size ?\nAnd to tackle this we have the next trick in our box, gradient accumulation, which we will discuss next.\n\n\nSolution 2: Gradient Accumulation\nGradient Accumulation is a technique that allows us to accumulate gradients over multiple micro-batches before performing a single global optimization step. This is particularly useful when we have a large batch size and we want to avoid running out of memory.\nThe general idea is to split the batch into smaller micro-batches (let’s say 3) and process them one by one. We compute the gradients for each micro-batch and accumulate them (we do not do optimizer.step() after each micro-batch). And after processing all the micro-batches, we perform a single global optimization step.\n\n\n\nLet’s take an example of a simple linear regression model, predicting the score of a student on a test based on two factors:\n\nthe number of hours studied (\\(x_1\\)) and\nthe number of hours slept the night before (\\(x_2\\)).\n\nWe assume a simple linear relationship between these inputs and the output score: \\[\n\\text{score}_{pred} = x_1 w_1 + x_2 w_2 + b\n\\] Our aim is to use stochastic gradient descent to determine the best values for \\(w_1\\), \\(w_2\\), and \\(b\\) such that the mean squared error (MSE) between the actual score (\\(\\text{score}_{target}\\)) and the predicted score (\\(\\text{score}_{pred}\\)) is minimized: \\[\n\\underset{w_1, w_2, b}{\\mathrm{argmin}} \\; (\\text{score}_{pred} - \\text{score}_{target})^2\n\\] Without gradient accumulation, we would update the parameters after each batch of student’s data.\n\nWithout Gradient Accumulation\n\n\n1  def train_no_accumulate(params: ModelParameters, \n2                         num_epochs: int = 10, \n3                         learning_rate: float = 1e-3):\n4      for epoch in range(1, num_epochs + 1):\n5          for (x1, x2), y_target in training_data:\n6  \n7              # Calculate the output of the model\n8              z1 = x1 * params.w1\n9              z2 = x2 * params.w2\n10             y_pred = z1 + z2 + params.b\n11             loss = (y_pred - y_target) ** 2\n12 \n13             # Calculate the gradients of the loss w.r.t. the parameters\n14             loss.backward()\n15 \n16             # Update the parameters (at each iteration)\n17             with torch.no_grad():\n18                 # Equivalent to calling optimizer.step()\n19                 params.w1 -= learning_rate * params.w1.grad\n20                 params.w2 -= learning_rate * params.w2.grad\n21                 params.b  -= learning_rate * params.b.grad\n22 \n23                 # Reset the gradients to zero\n24                 # Equivalent to calling optimizer.zero_grad()\n25                 params.w1.grad.zero_()\n26                 params.w2.grad.zero_()\n27                 params.b.grad.zero_()\n\n\n\nWith gradient accumulation, instead of updating the parameters after each batch of data, we accumulate gradients across several micro-batches (micro_batch_size = 3) and then update all at once.\nThis allows us to train with larger effective batch sizes even if memory is limited.\n\nWith Gradient Accumulation\n\n\n1  def train_accumulate(params: ModelParameters, \n2                       num_epochs: int = 10, \n3                       learning_rate: float = 1e-3, \n4                       micro_batch_size: int = 3):\n5  \n6      for epoch in range(1, num_epochs + 1):\n7          for index, ((x1, x2), y_target) in enumerate(training_data):\n8  \n9              # Calculate the output of the model\n10             z1 = x1 * params.w1\n11             z2 = x2 * params.w2\n12             y_pred = z1 + z2 + params.b\n13             loss = (y_pred - y_target) ** 2\n14 \n15             # Accumulate gradients\n16             loss.backward()\n17 \n18             # If we have processed 3 micro-batches OR reached the end of the dataset\n19             if (index + 1) % micro_batch_size == 0 or index == len(training_data) - 1:\n20                 with torch.no_grad():\n21                     # Equivalent to optimizer.step()\n22                     params.w1 -= learning_rate * params.w1.grad\n23                     params.w2 -= learning_rate * params.w2.grad\n24                     params.b  -= learning_rate * params.b.grad\n25 \n26                     # Reset the gradients = optimizer.zero_grad()\n27                     params.w1.grad.zero_()\n28                     params.w2.grad.zero_()\n29                     params.b.grad.zero_()\n\n\n\nGradient accumulation allows us to reduce activation memory, which grows linearly with batch size, by processing smaller micro-batches sequentially. This reduces stored activations and gradients since only one micro-batch’s worth of activations needs to be kept in memory at a time, which helps reduce the overall activation memory footprint.\nAgain, there’s a trade-off: gradient accumulation increases computation (more forward/backward passes before each optimizer step), so it can slow down training. But it enables much larger effective batch sizes on limited hardware.\nSo far, we’ve seen how techniques like gradient checkpointing and gradient accumulation help deal with the memory blowup issue caused by activations: the dynamic memory usage.\nBoth allow us to fit larger models or batches on a single GPU, but mostly by working sequentially and slowing down training. However, these don’t address the static memory required for parameters, gradients, and optimizer states, nor do they fully utilize available hardware (assume we have more than one GPU).\nTo tackle this, we can scale training across multiple GPUs using Data Parallelism. By splitting micro-batches and processing them simultaneously on several GPUs, we address both memory and compute bottlenecks."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#scaling-with-multiple-gpus-data-parallelism-dp",
    "href": "posts/distributed-training-from-scratch/index.html#scaling-with-multiple-gpus-data-parallelism-dp",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Scaling with Multiple GPUs: Data Parallelism (DP)",
    "text": "Scaling with Multiple GPUs: Data Parallelism (DP)\nRecall that in Gradient Accumulation, we were processing micro-batches (MBS) sequentially. Since these micro-batches are independent of each other, we can process them parallelly on different GPUs.\nSomething like this below: if you look carefully, now we are processing the micro-batches in parallel on different GPUs, whereas in Gradient Accumulation we were processing the micro-batches sequentially on a single GPU. This is what we do in Data Parallelism (DP).\n\n\n\n\nThe Data Parallel Setup\nIn a Data Parallel setup, we distribute the data across multiple GPUs, while maintaining a full, redundant replica of the model parameters, gradients, and optimizer states on each GPU.\n\nReplication: We maintain a full, redundant replica of the model parameters (\\(\\Phi\\)), gradients (\\(\\nabla \\Phi\\)), and optimizer states (\\(\\Phi_{\\text{optim}}\\)) on each GPU.\n\n\n\n\n\nParallel Processing: Each GPU processes a unique micro-batch simultaneously. This involves same operations, different data, which is also known as SIMD (Single Instruction Multiple Data).\n\n\n\n\n\nLocal Computation: Each GPU performs its forward pass and backward pass locally and independently, resulting in a local gradient (\\(\\nabla \\Phi_i\\)).\n\n\n\n\nIf you look carefully, we can perform the forward pass and the backward pass in parallel on different GPUs. But we cannot perform the optimizer step and update the parameters independently on different GPUs. If we do that, we will end up training N different models on N different GPUs which is not what we want.\nSo, after the backward pass, we need to synchronize the gradients across the GPUs. This is accomplished using the All-Reduce primitive.\n\n\nGradient Synchronization: The All-Reduce Primitive\nBefore we dive into the All-Reduce operation, it’s important to note that NVIDIA provides a rich set of communication primitives as part of its distributed training ecosystem such as NCCL (NVIDIA’s collective communication library). These primitives simplify and accelerate multi-GPU (and multi-node) communication, enabling efficient synchronization and sharding operations required for large-scale training.\n\n\n\n\n\n\nNoteCommunication Primitives in Distributed Training\n\n\n\nAll-Reduce is just one such primitive, used specifically for synchronizing gradients across GPUs at the end of each backward pass in standard Data Parallel training. However, there are several other primitives (like All-Gather, Reduce-Scatter, Broadcast, etc.) designed for different patterns of communication and parallelism.\nWe will discuss these additional primitives as we encounter them while exploring more advanced parallelization techniques (e.g., ZeRO, model sharding, and tensor parallelism) later in the series.\n\n\nFor now, let’s look at All-Reduce in detail, since this is exactly what we need for synchronizing the gradients during Data Parallel training.\nSince each GPU computes a gradient based only on its local micro-batch, we must add them to get the global gradient before performing the optimization step. The required communication operation is the All-Reduce primitive:\n\nInput: Different tensors (the local gradients \\(\\nabla \\Phi_1, \\nabla \\Phi_2, \\dots\\)) on each GPU.\nOperation: A reduction operation (usually summation, \\(F\\)) is applied to all tensors.\nOutput: The result of the reduction (the global gradient \\(\\sum \\nabla \\Phi_i\\)) is made available on all GPUs.\n\n\n\n\nOnce every node receives the global gradient, it performs the optimizer.step() operation independently, ensuring all model copies remain in sync. These collective operations are defined in the torch.distributed API.\nHere I’ve a machine with 4 T4 GPUs.\nray@ip-10-0-69-225:code$ nvidia-smi -L\nGPU 0: Tesla T4 (UUID: GPU-31a1b562-c769-c7f1-ede1-48847cec8d53)\nGPU 1: Tesla T4 (UUID: GPU-1beaf204-f6f7-182d-67f8-aee6c58128df)\nGPU 2: Tesla T4 (UUID: GPU-934ca246-df7e-2c7f-4bdd-b07859e46b2d)\nGPU 3: Tesla T4 (UUID: GPU-141171cb-db62-b770-97ff-955f8c7f2265)\nNow let’s create a simple example to demonstrate the All-Reduce operation by creating a tensor on each of the 4 GPUs and performing the All-Reduce operation on them.\n\ndist_all_reduce.py\n\n\nimport torch\nimport torch.distributed as dist\n\ndef init_process():\n    # Initializes the process group using the efficient nccl backend\n    dist.init_process_group(backend='nccl')\n    torch.cuda.set_device(dist.get_rank())\n\ndef example_all_reduce():\n    tensor = torch.tensor([dist.get_rank() + 1] * 3, dtype=torch.float32).cuda()\n    print(f\"Before all_reduce on rank {dist.get_rank()}: {tensor}\")\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    print(f\"After all_reduce on rank {dist.get_rank()}: {tensor}\")\n\n# Initialize the process group and set the device, create a tensor on each GPU and perform the All-Reduce operation on them.\ninit_process()\nexample_all_reduce()\n\n\n\nWe can run this code on 4 GPUs using torchrun:\ntorchrun --nproc_per_node=4 dist_all_reduce.py\nWe will get the following output:\nBefore all_reduce on rank 3: tensor([4., 4., 4.], device='cuda:3')\nBefore all_reduce on rank 0: tensor([1., 1., 1.], device='cuda:0')\nBefore all_reduce on rank 2: tensor([3., 3., 3.], device='cuda:2')\nBefore all_reduce on rank 1: tensor([2., 2., 2.], device='cuda:1')\n\nAfter all_reduce on rank 3: tensor([10., 10., 10.], device='cuda:3') \nAfter all_reduce on rank 0: tensor([10., 10., 10.], device='cuda:0') \nAfter all_reduce on rank 2: tensor([10., 10., 10.], device='cuda:2') \nAfter all_reduce on rank 1: tensor([10., 10., 10.], device='cuda:1') \n\n\nOverlapping Communication and Computation\nIn a naive/vanilla DP implementation, the GPUs sit idle during the communication phase, as the All-Reduce operation begins only after all gradients are computed in the backward pass. This is inefficient.\n\n\n\nTo eliminate this idle time, we try to overlap the communication and computation as much as possible.\n\nMethod: As soon as the gradient for a specific layer is computed during the backward pass (e.g., \\(\\nabla L_2\\)), we immediately trigger the All-Reduce for that gradient in the background.\nRationale: The computation of the next layer’s gradient (\\(\\nabla L_1\\)) is independent of the communication of the previous layer’s gradient (\\(\\nabla L_2\\)).\nImplementation: This technique is implemented via hooks in PyTorch (like post_accumulate_grad_hook()), allowing the next computation step to proceed while the communication step runs concurrently, significantly improving throughput. It attaches an all-reduce hook function to each parameter that requires gradients. With this implementation, it communicates more frequently but in smaller packets.\n\n\nOverlapping Communication and Computation\n\n\ndef register_backward_hook(self, hook):\n    \"\"\"\n    Registers a backward hook for all parameters of the model that \n    require gradients.\n    \"\"\"\n    for p in self.module.parameters():\n        if p.requires_grad is True:\n            p.register_post_accumulate_grad_hook(hook)\n\n\n\nPreviously, while communication was happening, we had to wait for all the gradients to be computed in the backward pass.\n\n\n\nBut now, we are overlapping the communication and computation. So, we are not waiting for all the gradients to be computed in the backward pass. We are computing the gradients for the next layer while the communication for the previous layer is happening.\n\n\n\nWe can, in fact, do even better and communicate more efficiently by grouping the gradients into larger buckets and performing the All-Reduce operation on these buckets.\nIt’s like packing items into boxes before shipping. (Have you noticed that, when placing an order on Amazon, they sometimes offer to pack multiple items into a single box to save on shipping costs? That’s exactly what we’re doing here, but with gradients.)\nWith this approach, we can significantly reduce communication overhead and speed up computation."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#the-limitations-of-simple-data-parallelism-dp",
    "href": "posts/distributed-training-from-scratch/index.html#the-limitations-of-simple-data-parallelism-dp",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "The Limitations of Simple Data Parallelism (DP)",
    "text": "The Limitations of Simple Data Parallelism (DP)\nNow that we’ve explored how to scale up training on multiple GPUs using Data Parallelism, a natural question arises: does this scaling result in perfect, linear performance gains? In other words, do we get proportional speedup as we add more GPUs?\nThe reality is more nuanced. Although data parallelism cleverly overlaps the all-reduce gradient synchronization with backward computation, these efficiency gains don’t hold up indefinitely. As the number of GPUs increases, into the hundreds or thousands, the cost of coordination and the demands on the networking infrastructure start to rise quickly. Eventually, with each additional GPU, the improvements in throughput diminish, and the overall system efficiency drops.\n\n\n\nThe chart above shows that as we add GPUs, throughput degrades noticeably, even though the memory usage per GPU remains unchanged regardless of the number of data parallel workers.\nData parallelism was our first (simple) strategy to scale training across more GPUs.\nHowever, our discussion so far has rested on the assumption that the entire model fits comfortably within the memory of a single GPU. But what happens when models grow so large (such as GPT-3, with 175 billion parameters) that they can no longer fit into one GPU’s memory (e.g., an NVIDIA A100 with 80GB of RAM)?\n\n\n\nAs model sizes grow, it becomes common that a single accelerator (GPU in our case) cannot contain all model parameters, optimizer states, and gradients. Therefore, we need to find additional ways to scale training beyond simple DP, which can allow us to train models that don’t fit on a single GPU.\nAnd that is what we will discuss in the next section - ZeRO (Zero Redundancy Optimizer)."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#ray-core-primitives",
    "href": "posts/distributed-training-from-scratch/index.html#ray-core-primitives",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Ray Core Primitives",
    "text": "Ray Core Primitives\nRay Core provides a minimal, yet powerful set of primitives that let you upgrade normal Python code into distributed code with almost no friction.\n\n\n\n\n\n\n\n\n\nNoteRay Compute Engine\n\n\n\nRay handles the difficult parts, like task scheduling, node failures, data transfers, and more behind the scenes so you don’t have to. As an ML engineer or researcher, you can focus on your model, algorithm, and data, while Ray takes care of the complexities of distributed systems."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#an-example-for-stateless-tasks-tasks",
    "href": "posts/distributed-training-from-scratch/index.html#an-example-for-stateless-tasks-tasks",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "An example: For Stateless Tasks (Tasks)",
    "text": "An example: For Stateless Tasks (Tasks)\nLet’s try to understand the core primitives of Ray, tasks and actors with an example. Imagine you’re building a simple app where you need to process a batch of images with a simple transformation (like inverting the colors).\nAt first, you write a for-loop to process the images sequentially. It works, but it’s sluggish, using only a single CPU core, even if your machine has lets say eight cores. What if you need to process hundreds or thousands of images? This is where Ray comes in, and a world of instant scalability.\nBelow, we walk step-by-step through the journey: from a plain, sequential Python function, which is painfully slow! to a parallel powerhouse processed by using Ray Tasks, and finally to coordinated, stateful parallelism with Ray Actors.\n\nStep 1. Sequential Processing (Slow)Step 2. Parallel Ray Task (Fast)\n\n\nEach image is processed one after another, burning a whole second per image. With 8 images, that’s 8 seconds to process all the images.\n# sequential_process.py\nimport time\nimport numpy as np\n\ndef process_image(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Simulates a slow 1-second filter.\"\"\"\n    time.sleep(1)\n    return 255 - image\n\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\n\nstart_time = time.time()\n# Sequential: 8 images × 1 sec/image = 8 seconds\nresults = [process_image(img) for img in images]\nend_time = time.time()\n\nprint(f\"Processed {len(results)} images in {end_time - start_time:.2f} seconds.\")\nOur code works, but only uses a single core, leaving the rest idle. Not a good situation. Let’s try to parallelize it using Ray Tasks.\n\n\nNow, let’s use Ray to parallelize the image processing.\nRay’s @ray.remote decorator instantly makes your function run in parallel, one copy per available CPU core.\n# parallel_process.py\nimport ray\nimport time\nimport numpy as np\n\n# 1. Initialize Ray - autodetects & uses all available CPU cores\nray.init()\n\n# 2. Decorate the function as a remote Ray task\n@ray.remote\ndef process_image(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Simulates a slow 1-second filter.\"\"\"\n    time.sleep(1)\n    return 255 - image\n\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\n\nstart_time = time.time()\n\n# 3. Launch tasks in parallel; returns list of ObjectRefs (futures)\nresult_refs = [process_image.remote(img) for img in images]\n\n# 4. Wait for and retrieve finished results via ray.get()\nresults = ray.get(result_refs)\nend_time = time.time()\n\n# On an 8-core machine: ~1 second total runtime!\nprint(f\"Processed {len(results)} images in {end_time - start_time:.2f} seconds.\")\n\nray.shutdown()\nWhat we did differently?\n\nWe decorated the function with @ray.remote decorator to make it a remote task.\nWe called the function with .remote() to launch it as a remote task.\nWe then wait for the results by calling ray.get() on the ObjectRefs returned by the remote calls.\nThe secret here is the ObjectRef, each .remote() call sends off a job in the background, while your main code keeps going. When you call ray.get(result_refs), Ray assembles all results when they are ready.\n\n\n\n\n\n\n\n\n\n\nTipRay Speed-Up\n\n\n\nBy adding just @ray.remote, .remote(), and ray.get(), we get a nearly 8x speedup with 8 CPU cores."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#going-further-for-stateful-tasks-actors",
    "href": "posts/distributed-training-from-scratch/index.html#going-further-for-stateful-tasks-actors",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Going Further: For Stateful Tasks (Actors)",
    "text": "Going Further: For Stateful Tasks (Actors)\nThe beauty of Ray is that it doesn’t just parallelize our work, it also gives us the right tool for sharing state across those jobs. Imagine that you want a running tally (say, the total number of pixels processed across all images), but you can’t use a global variable, because each parallel job runs isolated.\nWhat you really want is a service: a live, remote counter that all jobs can update in real-time. That’s what is known as an Actor in Ray: a class with its own persistent state, living somewhere on the cluster.\nLet’s see how to create and use an Actor for any stateful task.\n\nRay Actor (Stateful)\n\n\n# actor_counter.py\nimport ray\nimport numpy as np\nimport time\n\nray.init()\n\n# 1. Define the stateful service as a Python Class\n@ray.remote\nclass PixelCounter:\n    # The internal state is defined in __init__\n    def __init__(self):\n        self.total_pixels = 0\n\n    # A method to mutate (update) the internal state\n    def add(self, num_pixels: int):\n        self.total_pixels += num_pixels\n\n    # A method to retrieve the internal state\n    def get_total(self) -&gt; int:\n        return self.total_pixels\n\n# 2. Modify the Task to use the Actor Handle\n@ray.remote\ndef process_image_with_actor(image: np.ndarray, counter_actor: \"ActorHandle\"):\n    # This task calls the Actor's add method remotely\n    counter_actor.add.remote(image.size)\n    time.sleep(1)\n    # The image processing logic is here, but omitted for simplicity\n\n# --- Main Script ---\nimages = [np.random.randint(0, 255, (10, 10, 3)) for _ in range(8)]\nimage_size = images[0].size\nexpected_total = image_size * len(images) # 8 * 300 = 2400\n\n# 3. Create a single instance (the Actor Handle)\ncounter = PixelCounter.remote()\n\n# 4. Launch 8 parallel tasks, passing the Actor Handle to each\ntask_refs = [process_image_with_actor.remote(img, counter) for img in images]\n\n# Wait for all the image processing tasks to complete\nray.get(task_refs)\n\n# 5. Retrieve the final state from the Actor\nfinal_total_ref = counter.get_total.remote()\nfinal_total = ray.get(final_total_ref)\n\nprint(f\"Expected total pixels: {expected_total}\")\nprint(f\"Actual total from actor: {final_total}\")\n\nray.shutdown()\nWhat we did differently?\n\nPreviously, with Ray Tasks, each task independently processed an image in a stateless way. Now, by with the Actor (the remote PixelCounter class), we can safely accumulate the total pixel count as each task reports in.\nThe Actor’s state is persistent across many requests, enabling us to coordinate and aggregate information even in a distributed, parallel environment.\nThe key difference from before is that we now define a @ray.remote class (PixelCounter) and pass its handle to the tasks, so they can call add.remote() to update the shared state.\n\n\n\n\nThis pattern of combining Ray Tasks for stateless (think of Python functions), independent work and Ray Actors for stateful (think of Python classes) is the foundation of scalable Python pipelines for any real-world application (not just any AI applications).\nRay’s primitives empower us to build scalable, reliable, and maintainable distributed applications, without rewriting all our code or micro-managing threads and processes."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#ray-for-different-ai-workloads",
    "href": "posts/distributed-training-from-scratch/index.html#ray-for-different-ai-workloads",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Ray for Different AI Workloads",
    "text": "Ray for Different AI Workloads\nWhile Ray Core provides the low-level primitives for building distributed applications, it is not the only or always the best option, especially for specialized AI workloads.\nIt also offers higher-level abstractions tailored to specific AI tasks like data processing, training, hyperparameter search, RL and model serving.\n\n\n\n\n\n\n\n\nRay Library\nPurpose\nKey Features / Benefits\n\n\n\n\nRay Data\nScalable Data Ingest, Processing, Inference\nEffortlessly shards and preprocesses massive datasets; streams data efficiently between CPUs (ETL) and GPUs (training/inference) to maximize hardware utilization.\n\n\nRay Train\nDistributed Training & Fine-Tuning\nAbstracts away multi-node/GPU orchestration and synchronization for PyTorch, TensorFlow, etc., without the need for boilerplate or manual sync.\n\n\nRay Tune\nScalable Hyperparameter Search\nCoordinates and manages hyperparameter trials (search, early stopping, scheduling) across a cluster; includes experiment tracking and best-model picking.\n\n\nRay Serve\nFast, Programmable Model Serving\nDeploys models and logic as microservices with auto-scaling; supports model composition and features like traffic splitting and versioning.\n\n\nRay RLlib\nScalable Reinforcement Learning\nProvides a comprehensive library for training and evaluating RL algorithms.\n\n\n\nThese libraries are built on top of Ray Core and offer a more user-friendly interface for building distributed applications.\n\n\n\n\n\n\nTipWhy Use Higher-Level Ray Libraries?\n\n\n\nIf Ray Core’s Tasks and Actors are this powerful, why bother with higher-level Ray libraries like Ray Train or Ray Data?\nSimple answer is Abstraction and Specialization. While it’s technically possible to build a full distributed training pipeline with only Ray Core, that approach means you shoulder all the complexities, like manual data sharding, synchronization (e.g., for PyTorch DDP), distributed checkpointing, fault tolerance, handling resuming, and hyperparameter search. That’s a lot of boilerplate and risk!\n\n\nIt also has tight integration with popular frameworks like PyTorch, vLLM, Hugging Face, and more.\n\n\n\nThis unified ecosystem empowers us to build end-to-end distributed AI workflows.\nIn this blog, we’ll focus on distributed training with Ray Train, showing how it can scale our PyTorch training from a single GPU to a full cluster almost effortlessly.\n\n\n\n\n\n\nWarningWhy Not Use Only PyTorch Distributed?\n\n\n\nWhile PyTorch Distributed (such as DDP) is an excellent built-in solution for multi-GPU training, it’s primarily designed for single-node or homogeneous, tightly-coupled clusters. If you’re just scaling to multiple GPUs on one machine, PyTorch’s distributed tools/APIs are often enough. And you can run your training job with torchrun command.\nHowever, the challenges multiply dramatically the moment you want to scale across several machines or need to orchestrate complex workflows. Tasks like:\n\nLaunching jobs and synchronizing them across machines,\nManaging different workers, node failures, and resuming or monitoring experiments,\nEfficiently using both CPUs (for pre-processing and data loading) and GPUs (for training/inference) in one seamless workflow,\nSharding and streaming large datasets not just for one epoch, but for repeated, distributed, and fault-tolerant training, becomes painful and complex with only PyTorch’s stock tools.\n\nThis is exactly where Ray shines.\nIt abstracts away the low-level engineering required to run distributed workloads at scale. For instance, with Ray Train and Ray Data, you get seamless multi-GPU, multi-node orchestration, unified CPU-GPU pipelines, resilience and scalability. This helps you focus on your algorithms and models, rather than the underlying infrastructure."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#single-gpu-pytorch-training-on-cifar-10",
    "href": "posts/distributed-training-from-scratch/index.html#single-gpu-pytorch-training-on-cifar-10",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Single-GPU PyTorch Training on CIFAR-10",
    "text": "Single-GPU PyTorch Training on CIFAR-10\nWe’ll use a Vision Transformer model (torchvision.models.VisionTransformer) and the CIFAR-10 dataset. This code works on CPU, GPU (CUDA), or Apple MPS, but it’s strictly ordinary, non-distributed PyTorch.\nIf you have trained any model with PyTorch on a single machine/colab notebook, you might have seen a similar training loop, where we:\n\nDownload and prepare the dataset\n\nSet up data loaders\n\nDefine the model\n\nMove the model to the available device (GPU, MPS, or CPU).\n\nSet up optimizer and loss.\nRun the training loop.\n\nIterate over training data to update model weights.\nCheck accuracy on validation data.\n\nOptionally, checkpoint the model at the end.\n\nThis works perfectly for one GPU or a single machine, but doesn’t scale automatically. We’ll see soon how to migrate this to Ray Train for scaling, but for now, here’s the basic setup.\n\nDataLoader Function\nThis function sets up the DataLoaders for the CIFAR-10 training and test splits.\n\nDefine the DataLoader Function\n\n\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import Normalize, ToTensor\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport os\n\ndef get_dataloaders(batch_size):\n    \"\"\"\n    Create standard PyTorch DataLoaders.\n    No distributed code, just vanilla PyTorch.\n    \"\"\"\n\n    transform = transforms.Compose([\n        ToTensor(),\n        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_data = datasets.CIFAR10(\n            root=\"~/data\", train=True, download=True, transform=transform,\n        )\n        test_data = datasets.CIFAR10(\n            root=\"~/data\", train=False, download=True, transform=transform,\n        )\n\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_data, batch_size=batch_size)\n\n    return train_loader, test_loader\n\n\n\n\n\n\n\n\n\nTipWhy use a FileLock?\n\n\n\nWe use a FileLock to avoid concurrency issues if datasets are being downloaded.\n\n\n\n\nTraining Function\nThis is the standard PyTorch training loop, using torchvision.models.VisionTransformer.\n\nDefine the Training Function\n\n\nfrom torchvision.models import VisionTransformer\nfrom torch import nn\nimport torch\nfrom tqdm import tqdm\n\ndef train_func(lr=1e-3, epochs=10, batch_size=512):\n    \"\"\"\n    Main training function: single machine, single GPU.\n    \"\"\"\n    # Get data loaders\n    train_loader, val_loader = get_dataloaders(batch_size=batch_size)\n\n    # Create the model\n    model = VisionTransformer(\n        image_size=32,   # CIFAR-10 images are 32x32\n        patch_size=4,    # Reasonable patch size for CIFAR-10\n        num_layers=12,   # Transformer layers\n        num_heads=8,     # Attention heads\n        hidden_dim=384,  # Model width\n        mlp_dim=768,     # Transformer MLP dim\n        num_classes=10   # CIFAR-10\n    )\n\n    # Move model to correct device (GPU/MPS/CPU)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    model.to(device)\n\n    # Set up loss and optimizer\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    # Training loop\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for X, y in tqdm(train_loader, desc=f\"Train Epoch {epoch + 1}\"):\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n\n        # Validation phase\n        model.eval()\n        val_loss, num_correct, num_total = 0, 0, 0\n        with torch.no_grad():\n            for X, y in tqdm(val_loader, desc=f\"Valid Epoch {epoch + 1}\"):\n                X, y = X.to(device), y.to(device)\n                pred = model(X)\n                loss = loss_fn(pred, y)\n                val_loss += loss.item()\n                num_total += y.shape[0]\n                num_correct += (pred.argmax(1) == y).sum().item()\n        val_loss /= len(val_loader)\n        accuracy = num_correct / num_total\n\n        print(f\"  Train Loss: {train_loss:.4f} | Valid Loss: {val_loss:.4f} | Accuracy: {accuracy:.4f} ({100 * accuracy:.2f}%)\")\n\n    # Optional: Save checkpoint\n    checkpoint = {\n        'epoch': epochs,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'accuracy': accuracy,\n    }\n    torch.save(checkpoint, 'checkpoint_single_machine.pth')\n    print(f\"\\nTraining completed! Final accuracy: {100 * accuracy:.2f}%\\nCheckpoint saved to checkpoint_single_machine.pth\")\n\n\n\n\n\nTraining the Model\nLet’s now train the model on a single GPU.\ntrain_func(lr=1e-3, epochs=10, batch_size=512)\n\n\n\nAs you can see, the model is trained on a single GPU while other GPUs are idle.\nThis script represents plain vanilla PyTorch, suitable for a single GPU or single CPU. There is no distributed logic or Ray involved yet. All of the key logic, especially the get_dataloaders function and the structure of the train_func will remain mostly the same when we migrate to distributed training with Ray Train.\nLet’s now train the model at scale on multiple GPUs across multiple machines."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#distributed-training-with-ray-train",
    "href": "posts/distributed-training-from-scratch/index.html#distributed-training-with-ray-train",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Distributed Training with Ray Train",
    "text": "Distributed Training with Ray Train\nNow, let’s see how to migrate this single-machine, single-GPU training loop to distributed training using Ray Train and PyTorch on multiple machines, multiple GPUs.\n\n\n\nYour browser does not support the video tag.   Distributed Training with Ray Train Key Concepts.\n\n\nRay Train Architecture\nRay Train’s architecture is based on the following components:\n\nA Ray Train Controller/Driver that schedules the training workers, handles errors, and manages checkpoints\n\nRay Train Workers that execute the training code\n\n\n\n\nBelow are the key API concepts of Ray Train:\n\ntrain_loop_per_worker: The core function that contains your model training logic\n\nScalingConfig: Specifies the number of workers and compute resources (CPUs, GPUs, TPUs)\n\nTrainer: Manages the training process\n\nTrainer.fit(): Starts the distributed training job\n\n\n\n\n\n\nRay Data and Ray Train Integration\nHere is a diagram showing the Ray Data and Ray Train integration.\n\n\n\nWe are not going to go into the details of Ray Data and Ray Train integration in this blog post. But if you are interested in learning more about it, you can check out the Ray Data documentation."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#setup-the-environment",
    "href": "posts/distributed-training-from-scratch/index.html#setup-the-environment",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Setup the Environment",
    "text": "Setup the Environment\nBefore we start any training, let’s first check how many GPUs (and CPUs) are available in our Ray Cluster.\n\nCheck Cluster GPUs\n\n\nimport ray\nimport torch\n\ndef check_cluster_gpus():\n    \"\"\"Check GPU count in the entire Ray cluster.\"\"\"\n    # Initialize Ray if not already initialized\n    if not ray.is_initialized():\n        ray.init()\n\n    # Get cluster resources (total GPUs in cluster)\n    cluster_resources = ray.cluster_resources()\n    total_gpus = cluster_resources.get(\"GPU\", 0)\n\n    # Get available resources (currently available GPUs)\n    available_resources = ray.available_resources()\n    available_gpus = available_resources.get(\"GPU\", 0)\n\n    # Get local GPU count (GPUs on this node only)\n    local_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n\n    # Print results\n    print(\"\\n\" + \"=\"*60)\n    print(\"Ray Cluster GPU Information\")\n    print(\"=\"*60)\n    print(f\"Total GPUs in cluster:     {int(total_gpus)}\")\n    print(f\"Available GPUs in cluster: {int(available_gpus)}\")\n    print(f\"Local GPUs (head node):    {local_gpus}\")\n    print(\"=\"*60)\n\n    # Additional cluster info\n    print(\"\\nCluster Resources:\")\n    print(f\"  CPUs (total):     {int(cluster_resources.get('CPU', 0))}\")\n    print(f\"  CPUs (available): {int(available_resources.get('CPU', 0))}\")\n\n    # Show node details if available\n    try:\n        nodes = ray.nodes()\n        print(f\"\\nCluster Nodes: {len(nodes)}\")\n        for i, node in enumerate(nodes):\n            node_resources = node.get('Resources', {})\n            node_gpus = node_resources.get('GPU', 0)\n            print(f\"  Node {i+1}: {int(node_gpus)} GPU(s)\")\n    except Exception as e:\n        print(f\"\\nNote: Could not retrieve node details: {e}\")\n\n    print()\n    return {\n        'total_gpus': int(total_gpus),\n        'available_gpus': int(available_gpus),\n        'local_gpus': local_gpus\n    }\n\nif __name__ == '__main__':\n    check_cluster_gpus()\n\n\n\nAs you can see, the cluster which I have has a total of 8 GPUs. The cluster consists of a total of one Head Node and two Worker Nodes, with each worker node having 4 GPUs.\n============================================================\nRay Cluster GPU Information\n============================================================\nTotal GPUs in cluster:     8\nAvailable GPUs in cluster: 8\nLocal GPUs (head node):    0\n============================================================\n\nCluster Resources:\n  CPUs (total):     96\n  CPUs (available): 96\n\nCluster Nodes: 3\n  Node 1: 0 GPU(s)\n  Node 2: 4 GPU(s)\n  Node 3: 4 GPU(s)"
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#distributed-training-with-ray-train-and-pytorch-fsdp",
    "href": "posts/distributed-training-from-scratch/index.html#distributed-training-with-ray-train-and-pytorch-fsdp",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Distributed Training with Ray Train and PyTorch FSDP",
    "text": "Distributed Training with Ray Train and PyTorch FSDP\nNow that we have understood the basics of Ray Train, and also have a Ray cluster ready, let’s now dive into distributed training with Ray Train and PyTorch FSDP.\n\n1. Specify Cluster Scaling\nFirst, set up how many Ray workers (processes) will participate, typically one per GPU. For a Ray cluster with 8 GPUs:\n\nSpecify Cluster Scaling\n\n\nscaling_config = ScalingConfig(\n    num_workers=8,  # e.g., 8 GPUs in our cluster\n    use_gpu=True,\n    resources_per_worker={\"CPU\": 2, \"GPU\": 1},\n)\n\n\n\n\n\n2. Data Preparation: PyTorch DataLoaders\nData preparation is unchanged from typical PyTorch or DDP usage. Use our usual transforms and DataLoader logic:\n\nDefine the DataLoaders\n\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport os\n\ndef get_dataloaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_ds = datasets.CIFAR10(\"~/data\", train=True, download=True, transform=transform)\n        valid_ds = datasets.CIFAR10(\"~/data\", train=False, download=True, transform=transform)\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(valid_ds, batch_size=batch_size),\n    )\n\n\n\nNo special considerations are needed for FSDP at this stage.\n\n\n3. Define the Training Function\nNext, let’s define the training function for the Ray Train worker. This is the training function that will be executed by each worker.\nAs the model is now being prepared for FSDP, we need to use the prepare_model function to prepare the model for FSDP.\n\nDefine the Training Function\n\n\ndef train_func_per_worker(config):\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    ctx = ray.train.get_context()\n    rank = ctx.get_world_rank()\n    world_size = ctx.get_world_size()\n\n    if rank == 0:\n        print(f\"Training with FSDP across {world_size} workers...\")\n\n    # Prepare DataLoaders for distributed training\n    train_dl, valid_dl = get_dataloaders(batch_size)\n    train_dl = ray.train.torch.prepare_data_loader(train_dl)\n    valid_dl = ray.train.torch.prepare_data_loader(valid_dl)\n\n    # Define the model\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n\n    # Prepare the model for FSDP\n    model = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, sample_cnt = 0.0, 0\n        for X, y in train_dl:\n            pred = model(X)\n            loss = criterion(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * X.shape[0]\n            sample_cnt += X.shape[0]\n        train_loss = total_loss / sample_cnt\n\n        # Validation loop\n        model.eval()\n        valid_loss, correct, total = 0.0, 0, 0\n        with torch.no_grad():\n            for X, y in valid_dl:\n                pred = model(X)\n                valid_loss += criterion(pred, y).item() * X.shape[0]\n                total += y.shape[0]\n                correct += (pred.argmax(dim=1) == y).sum().item()\n        valid_loss /= total\n        acc = correct / total\n\n        if rank == 0:\n            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} Valid Loss={valid_loss:.4f} Acc={acc:.3%}\")\n\n        metrics = {\"epoch\": epoch+1, \"train_loss\": train_loss, \"valid_loss\": valid_loss, \"accuracy\": acc}\n        # Checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            with tempfile.TemporaryDirectory() as ckpt_dir:\n                torch.save(model.module.state_dict(), os.path.join(ckpt_dir, \"model.pt\"))\n                ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(ckpt_dir))\n        else:\n            ray.train.report(metrics)\n\n\n\n\n\n\n\n\n\nNoteChange the parallel strategy to DDP\n\n\n\nTo change the parallel strategy to DDP, simply change the parameter to \"ddp\":\nmodel = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n\n\n\n4. Configure Run Checkpointing and Storage\nWe now use Ray’s checkpointing utilities to save the best results and recoverable states:\n\nConfigure Run Checkpointing and Storage\n\n\nfrom ray.train import RunConfig, CheckpointConfig\n\ncheckpoint_config = CheckpointConfig(\n    num_to_keep=2,\n    checkpoint_score_attribute=\"accuracy\",\n    checkpoint_score_order=\"max\",\n)\nrun_config = RunConfig(\n    name=\"cifar10_fsdp_example\",\n    storage_path=\"/mnt/cluster_storage/training/\",  # Use a persistent/shared location\n    checkpoint_config=checkpoint_config,\n)\n\n\n\n\n\n5. Launch Training with TorchTrainer\nBring all the configs together and kick off distributed training:\n\nLaunch Training with TorchTrainer\n\n\nfrom ray.train.torch import TorchTrainer\n\nglobal_batch_size = 1024\nnum_workers = 8   \nbatch_size_per_worker = global_batch_size // num_workers\n\ntrain_loop_config = {\n    \"lr\": 1e-3,\n    \"epochs\": 20,\n    \"batch_size_per_worker\": batch_size_per_worker,\n}\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nprint(\"Starting FSDP distributed training...\")\nresult = trainer.fit()\n\n\n\n\n\n6. Load and Use Checkpoints\nOnce the training is complete, the model can be restored from the best checkpoint:\n\nLoad and Use Checkpoints\n\n\nimport torch\nfrom torchvision.models import VisionTransformer\nimport os\n\nckpt = result.checkpoint\nwith ckpt.as_directory() as ckpt_dir:\n    model_path = os.path.join(ckpt_dir, \"model.pt\")\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    model.load_state_dict(state_dict)\n\n\n\nLet’s now put all the pieces together and train the model on the cluster.\n\ntrain_fsdp.py\n\n\nimport os\nimport tempfile\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom torchvision.models import VisionTransformer\nfrom torch.utils.data import DataLoader\nfrom filelock import FileLock\nimport ray\nfrom ray.train import ScalingConfig, RunConfig, CheckpointConfig\nfrom ray.train.torch import TorchTrainer\n\ndef get_dataloaders(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    with FileLock(os.path.expanduser(\"~/data.lock\")):\n        train_data = datasets.CIFAR10(root=\"~/data\", train=True, download=True, transform=transform)\n        valid_data = datasets.CIFAR10(root=\"~/data\", train=False, download=True, transform=transform)\n    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_data, batch_size=batch_size)\n    return train_loader, valid_loader\n\ndef train_func_per_worker(config):\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n    batch_size = config[\"batch_size_per_worker\"]\n\n    ctx = ray.train.get_context()\n    world_size = ctx.get_world_size()\n    local_rank = ctx.get_world_rank()\n    if local_rank == 0:\n        print(f\"FSDP Training on {world_size} workers\")\n\n    train_loader, valid_loader = get_dataloaders(batch_size)\n    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n    valid_loader = ray.train.torch.prepare_data_loader(valid_loader)\n\n    model = VisionTransformer(\n        image_size=32, patch_size=4,\n        num_layers=12, num_heads=8, hidden_dim=384, mlp_dim=768, num_classes=10,\n    )\n    # [FSDP] Key change from DDP:\n    model = ray.train.torch.prepare_model(model, parallel_strategy=\"fsdp\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss, n = 0.0, 0\n        for X, y in train_loader:\n            pred = model(X)\n            loss = criterion(pred, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * X.shape[0]\n            n += X.shape[0]\n        train_loss /= n\n\n        model.eval()\n        correct, total, valid_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for X, y in valid_loader:\n                pred = model(X)\n                valid_loss += criterion(pred, y).item() * X.shape[0]\n                total += y.shape[0]\n                correct += (pred.argmax(dim=1) == y).sum().item()\n        valid_loss /= total\n        accuracy = correct / total\n        metrics = {\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"valid_loss\": valid_loss,\n            \"accuracy\": accuracy\n        }\n        # Save a checkpoint every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            with tempfile.TemporaryDirectory() as tmp_ckpt_dir:\n                torch.save(model.module.state_dict(),\n                           os.path.join(tmp_ckpt_dir, \"model.pt\"))\n                ray.train.report(metrics, checkpoint=ray.train.Checkpoint.from_directory(tmp_ckpt_dir))\n        else:\n            ray.train.report(metrics)\n\nscaling_config = ScalingConfig(num_workers=8, use_gpu=True, resources_per_worker={\"CPU\": 2, \"GPU\": 1})\ncheckpoint_config = CheckpointConfig(num_to_keep=2, checkpoint_score_attribute=\"accuracy\", checkpoint_score_order=\"max\")\nrun_config = RunConfig(name=\"cifar10_fsdp_example\", storage_path=\"/mnt/cluster_storage/training/\", checkpoint_config=checkpoint_config)\n\nglobal_batch_size = 1024\nbatch_size_per_worker = global_batch_size // scaling_config.num_workers\ntrain_loop_config = {\"lr\": 1e-3, \"epochs\": 20, \"batch_size_per_worker\": batch_size_per_worker}\n\ntrainer = TorchTrainer(\n    train_loop_per_worker=train_func_per_worker,\n    train_loop_config=train_loop_config,\n    scaling_config=scaling_config,\n    run_config=run_config,\n)\n\nresult = trainer.fit()\n\n\n\nWhile the training is runing, we can see the progress in the Ray dashboard. We can now see that all 8 GPUs are being used for training."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#conclusion",
    "href": "posts/distributed-training-from-scratch/index.html#conclusion",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "Conclusion",
    "text": "Conclusion\nDistributed training from scratch is often perceived as a complex and challenging endeavor, especially when it comes to configuring multiple GPUs and orchestrating communication between workers. However, modern open-source frameworks such as Ray Train, together with robust PyTorch features like FSDP, have significantly lowered the barrier to scalable, efficient distributed deep learning. In this blog, we walked through a step-by-step workflow for setting up a distributed training pipeline to fine-tune a model on multiple GPUs, leveraging Ray Train for orchestration and PyTorch FSDP for efficient memory and communication management.\nBy utilizing Ray’s high-level abstractions, we are able to streamline the engineering process, eliminate much of the boilerplate involved in custom distributed training, and gain access to valuable capabilities such as experiment tracking, fault tolerance, and automatic checkpointing. Ray’s scaling configurations allow us to harness all available compute resources, whether just a few or dozens of GPUs, while its dashboard provides clear visibility into resource utilization and training progress in real-time.\nIn this blog, we mostly talk about Data Parallelism (DP) technique for scaling training to multiple GPUs. But for tackling even more demanding tasks with extremely large models and datasets, there are additional and more advanced techniques available. Strategies such as Pipeline Parallelism, Tensor Parallelism, and Sequence Parallelism have become key components in scaling deep learning to the frontier level. These methods allow models to be split across layers, parameters, or computation steps, enabling efficient training across clusters of GPUs.\nEmerging practices in hybrid sharding, mixed precision, model offloading, and asynchronous optimization further push the boundaries of what is possible, empowering researchers and engineers to experiment at ever-increasing scales while managing resource and memory efficiency. Maybe in some future blog, we will discuss these advanced techniques in more detail (as I learn them :))."
  },
  {
    "objectID": "posts/distributed-training-from-scratch/index.html#references-further-resources",
    "href": "posts/distributed-training-from-scratch/index.html#references-further-resources",
    "title": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray",
    "section": "References & Further Resources",
    "text": "References & Further Resources\n\nAnyscale and Ray\n\nGetting Started with Ray on Anyscale\n\nDistributed Training\n\nAdvanced Large Language Models [IIT Delhi]\nTinyML and Efficient Deep Learning Computing [MIT]\nTraining LLMs on GPU Clusters\nCS231N Lecture 11, Large Scale Distributed Training\n\nLLM and Advance Deep Learning\n\nThe School of AI\nUnderstanding Deep Learning\nBuilding LLMs from scratch\n\nRay and PyTorch\n\nRay Train Documentation\nPyTorch FSDP2 Documentation\nRay Data Documentation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "From Single GPU to Clusters: A Practical Journey into Distributed Training with PyTorch and Ray\n\n\nIn this blog, we’ll explore distributed training together, breaking down the core concepts and hands-on techniques for scaling deep learning models across multiple GPUs and machines using PyTorch and Ray.\n\n\n\n\n\nNov 30, 2025\n\n\nSuman Debnath\n\n\n\n\n\nNo matching items"
  }
]